{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "\n",
    "# 00_baixar_dados — Ingestão CCCAGG (CryptoCompare/CCData) · **MVP**\n",
    "\n",
    "**Objetivo:** baixar/atualizar histórico **BTC/USD 1h** (CCCAGG), com:\n",
    "- retomada automática (append a partir do último `open_time`),\n",
    "- ou backfill de uma janela (`since`/`until`),\n",
    "- paginação com *retry/backoff*,\n",
    "- escrita **atômica** (arquivo temporário + rename),\n",
    "- **deduplicação** e **ordenação** por `open_time`,\n",
    "- **validações mínimas** (gaps/monotonicidade/valores),\n",
    "- `meta.json` com métricas da execução.\n",
    "\n",
    "> ⚠️ Este notebook cria funções e **não** dispara o download automaticamente.  \n",
    "> Execute a célula **“Run (opcional)”** somente no seu ambiente com internet e `CRYPTOCOMPARE_API_KEY` (opcional).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Imports e configuração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, time, math, json, gc\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "from typing import Optional, Any, Dict\n",
    "\n",
    "# Parâmetros padrão (MVP)\n",
    "DEFAULT_FSYM = \"BTC\"\n",
    "DEFAULT_TSYM = \"USD\"\n",
    "DEFAULT_TF   = \"1h\"  # fixo neste MVP\n",
    "DEFAULT_OUT  = \"../data/raw/BTCUSD_CCCAGG_1h.csv\"\n",
    "DEFAULT_BATCH = 2000\n",
    "DEFAULT_SLEEP = 0.12  # respeitar rate limit leve\n",
    "API_KEY = os.getenv(\"CRYPTOCOMPARE_API_KEY\")  # opcional\n",
    "BASE_URL = \"https://min-api.cryptocompare.com/data/v2/histohour\"  # 1h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Helpers (tempo, I/O, validações)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _to_epoch_seconds(x: Optional[Any]) -> Optional[int]:\n",
    "    if x is None:\n",
    "        return None\n",
    "    if isinstance(x, (int, float)):\n",
    "        return int(x)\n",
    "    if isinstance(x, str):\n",
    "        x = x.strip()\n",
    "        try:\n",
    "            if len(x) <= 10:\n",
    "                dt = datetime.strptime(x, \"%Y-%m-%d\").replace(tzinfo=timezone.utc)\n",
    "            elif len(x) <= 16:\n",
    "                dt = datetime.strptime(x, \"%Y-%m-%d %H:%M\").replace(tzinfo=timezone.utc)\n",
    "            else:\n",
    "                dt = datetime.fromisoformat(x.replace(\"Z\",\"\")).astimezone(timezone.utc)\n",
    "            return int(dt.timestamp())\n",
    "        except Exception:\n",
    "            try:\n",
    "                return int(float(x))\n",
    "            except Exception:\n",
    "                raise ValueError(f\"Não foi possível interpretar 'since/until': {x}\")\n",
    "    raise TypeError(f\"Tipo não suportado para timestamp: {type(x)}\")\n",
    "\n",
    "def _read_last_ms(csv_path: Path) -> Optional[int]:\n",
    "    if not csv_path.exists():\n",
    "        return None\n",
    "    try:\n",
    "        s = pd.read_csv(csv_path, usecols=[\"open_time\"]).open_time\n",
    "        if len(s) == 0:\n",
    "            return None\n",
    "        return int(s.max())\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _headers() -> Dict[str, str]:\n",
    "    return {\"authorization\": f\"Apikey {API_KEY}\"} if API_KEY else {}\n",
    "\n",
    "def _sleep_backoff(attempt: int, base: float = 0.5, cap: float = 8.0):\n",
    "    t = min(cap, base * (2 ** attempt))\n",
    "    time.sleep(t)\n",
    "\n",
    "def _validate_ohlcv(df: pd.DataFrame, verbose=True):\n",
    "    msgs = []\n",
    "    ok = True\n",
    "    if not df.index.is_monotonic_increasing:\n",
    "        ok = False; msgs.append(\"Index não está ordenado ASC.\")\n",
    "    for col in [\"open\",\"high\",\"low\",\"close\"]:\n",
    "        if (df[col] <= 0).any():\n",
    "            ok = False; msgs.append(f\"Valores <=0 em {col}.\")\n",
    "    if ((df[\"high\"] < df[[\"open\",\"close\"]].max(axis=1)) | (df[\"low\"] > df[[\"open\",\"close\"]].min(axis=1))).any():\n",
    "        msgs.append(\"Aviso: 'high/low' fora do envelope em algumas linhas.\")\n",
    "    diffs = (df.index[1:] - df.index[:-1]).total_seconds()\n",
    "    if len(diffs) > 0 and (diffs > 5400).any():\n",
    "        msgs.append(\"Aviso: gaps > 90min detectados.\")\n",
    "    if verbose:\n",
    "        if ok: print(\"Validações básicas OK.\")\n",
    "        for m in msgs: print(\"•\", m)\n",
    "    return ok, msgs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Cliente: requisição com retry/backoff (histohour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _fetch_histohour_page(fsym: str, tsym: str, limit: int, toTs: Optional[int]) -> list:\n",
    "    params = dict(fsym=fsym, tsym=tsym, limit=limit, e=\"CCCAGG\")\n",
    "    if toTs is not None:\n",
    "        params[\"toTs\"] = int(toTs)\n",
    "    max_attempts = 6\n",
    "    for attempt in range(max_attempts):\n",
    "        try:\n",
    "            r = requests.get(BASE_URL, params=params, headers=_headers(), timeout=20)\n",
    "            if r.status_code in (429, 500, 502, 503, 504):\n",
    "                raise requests.HTTPError(f\"HTTP {r.status_code}\")\n",
    "            r.raise_for_status()\n",
    "            data = r.json().get(\"Data\", {}).get(\"Data\", [])\n",
    "            return data or []\n",
    "        except Exception as e:\n",
    "            if attempt == max_attempts - 1:\n",
    "                raise\n",
    "            _sleep_backoff(attempt)\n",
    "    return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Função principal: download + merge/dedup/sort + meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_cccagg_ohlcv_csv(\n",
    "    fsym=DEFAULT_FSYM, tsym=DEFAULT_TSYM, out_csv=DEFAULT_OUT,\n",
    "    since=None, until=None, batch=DEFAULT_BATCH, sleep=DEFAULT_SLEEP, verbose=True,\n",
    "    checkpoint_every_page=True  # <- novo\n",
    "):\n",
    "    out_path = Path(out_csv)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    stage_path = out_path.with_suffix(\".staging.csv\")  # <- arquivo de staging\n",
    "\n",
    "    start_ts = _to_epoch_seconds(since)\n",
    "    end_ts   = _to_epoch_seconds(until)\n",
    "\n",
    "    # ler último ms do final e do staging\n",
    "    last_final_ms  = _read_last_ms(out_path) or None\n",
    "    last_stage_ms  = _read_last_ms(stage_path) or None\n",
    "\n",
    "    # Se não foi passado since/until, retoma do maior dos arquivos\n",
    "    if start_ts is None and end_ts is None:\n",
    "        last_any = max([x for x in [last_final_ms, last_stage_ms] if x is not None], default=None)\n",
    "        if last_any:\n",
    "            start_ts = int(last_any/1000) + 3600\n",
    "            if verbose:\n",
    "                print(\"Retomando de\", pd.to_datetime(start_ts, unit=\"s\", utc=True))\n",
    "\n",
    "    cursor = end_ts\n",
    "    total_pages = 0\n",
    "    total_rows_written_stage = 0\n",
    "\n",
    "    # escrever header no staging se ele não existir\n",
    "    if checkpoint_every_page and not stage_path.exists():\n",
    "        stage_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(stage_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"open_time,open,high,low,close,volume,notional_usd\\n\")\n",
    "\n",
    "    while True:\n",
    "        page = _fetch_histohour_page(fsym, tsym, batch, cursor)\n",
    "        total_pages += 1\n",
    "        if not page:\n",
    "            if verbose: print(\"Página vazia; encerrando.\")\n",
    "            break\n",
    "\n",
    "        page.sort(key=lambda d: d[\"time\"])\n",
    "        # filtra por janela e mapeia\n",
    "        rows = []\n",
    "        for d in page:\n",
    "            t = d[\"time\"]\n",
    "            if start_ts is not None and t < start_ts:\n",
    "                continue\n",
    "            if end_ts is not None and t > end_ts:\n",
    "                continue\n",
    "            rows.append([t*1000, d[\"open\"], d[\"high\"], d[\"low\"], d[\"close\"], d[\"volumefrom\"], d[\"volumeto\"]])\n",
    "\n",
    "        if rows and checkpoint_every_page:\n",
    "            # append incremental no staging (pode ficar fora de ordem; ordenaremos no merge final)\n",
    "            with open(stage_path, \"a\", encoding=\"utf-8\") as f:\n",
    "                for r2 in rows:\n",
    "                    f.write(\",\".join(str(x) for x in r2) + \"\\n\")\n",
    "                f.flush()\n",
    "                os.fsync(f.fileno())\n",
    "            total_rows_written_stage += len(rows)\n",
    "\n",
    "        if verbose:\n",
    "            ts_ult = page[-1][\"time\"]\n",
    "            print(f\"+{len(page)} (até {pd.to_datetime(ts_ult, unit='s', utc=True)})\")\n",
    "\n",
    "        cursor = page[0][\"time\"] - 1\n",
    "        if start_ts is not None and cursor < start_ts:\n",
    "            break\n",
    "        time.sleep(max(0.0, sleep))\n",
    "\n",
    "    # Se não fez checkpoint por página, ainda precisamos gravar algo (comportamento antigo)\n",
    "    if not checkpoint_every_page:\n",
    "        if verbose: print(\"Checkpoint por página desativado — nada salvo porque o processo foi interrompido ou não havia novas linhas.\")\n",
    "        return out_path\n",
    "\n",
    "    # Merge staging + final → final (dedup + sort)\n",
    "    # Obs.: mesmo se a execução for interrompida antes desta etapa, o progresso já está em stage_path.\n",
    "    if stage_path.exists():\n",
    "        df_stage = pd.read_csv(stage_path)\n",
    "        df_stage = df_stage.drop_duplicates(\"open_time\")\n",
    "\n",
    "        if out_path.exists():\n",
    "            df_final = pd.read_csv(out_path)\n",
    "            df = pd.concat([df_final, df_stage], ignore_index=True)\n",
    "        else:\n",
    "            df = df_stage\n",
    "\n",
    "        df = df.drop_duplicates(\"open_time\", keep=\"last\").sort_values(\"open_time\").reset_index(drop=True)\n",
    "\n",
    "        # escrita atômica do final\n",
    "        tmp_out = out_path.with_suffix(\".csv.tmp\")\n",
    "        df.to_csv(tmp_out, index=False)\n",
    "        tmp_out.replace(out_path)\n",
    "\n",
    "        # staging pode ser removido (ou mantido, se quiser staged commits)\n",
    "        try:\n",
    "            stage_path.unlink()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # meta\n",
    "        meta = {\n",
    "            \"run_id\": datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\") + \"_1h\",\n",
    "            \"fsym\": fsym, \"tsym\": tsym, \"tf\": \"1h\", \"exchange\": \"CCCAGG\",\n",
    "            \"rows_final\": int(len(df)),\n",
    "            \"rows_added_this_run\": int(len(df_stage)),\n",
    "            \"start_utc\": pd.to_datetime(df.open_time.iloc[0], unit=\"ms\", utc=True).isoformat(),\n",
    "            \"end_utc\":   pd.to_datetime(df.open_time.iloc[-1], unit=\"ms\", utc=True).isoformat(),\n",
    "            \"pages\": int(total_pages),\n",
    "            \"checkpointed_rows\": int(total_rows_written_stage),\n",
    "            \"batch\": batch\n",
    "        }\n",
    "        with open(out_path.with_suffix(\".meta.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(meta, f, indent=2)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Merge concluído. Linhas totais: {len(df)} | Adicionadas: {len(df_stage)}\")\n",
    "            print(\"Arquivo final:\", str(out_path))\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"Nenhum dado em staging para consolidar.\")\n",
    "\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Validações rápidas (após download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def post_validate_csv(out_csv: str):\n",
    "    path = Path(out_csv)\n",
    "    if not path.exists():\n",
    "        print(\"Arquivo não encontrado:\", path); return False\n",
    "    df = pd.read_csv(path)\n",
    "    dt = pd.to_datetime(df[\"open_time\"], unit=\"ms\", utc=True)\n",
    "    df_idx = df.set_index(dt.tz_convert(None)).drop(columns=[\"open_time\"]).sort_index()\n",
    "    ok, msgs = _validate_ohlcv(df_idx, verbose=True)\n",
    "    return ok\n",
    "\n",
    "# Exemplo (rodar só após o download):\n",
    "# post_validate_csv(DEFAULT_OUT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Run (opcional) — execute no seu ambiente com internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ⚠️ NÃO EXECUTE AQUI se seu ambiente não tiver internet.\n",
    "# Exemplo 1: retomar automaticamente\n",
    "download_cccagg_ohlcv_csv(out_csv=DEFAULT_OUT, verbose=True)\n",
    "\n",
    "# Exemplo 2: backfill exato de 2020-01-01 a 2022-12-31\n",
    "# download_cccagg_ohlcv_csv(since=\"2020-01-01\", until=\"2022-12-31\", out_csv=DEFAULT_OUT, verbose=True)\n",
    "\n",
    "# Depois, checar:\n",
    "# post_validate_csv(DEFAULT_OUT)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
