{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 🚀 Validação de Performance - CQR_LightGBM\n",
    "\n",
    "**Objetivo**: Validar a performance do modelo em dados de teste e backtest histórico\n",
    "\n",
    "**Escopo**: \n",
    "- ✅ Métricas de Cross-Validation\n",
    "- ✅ Validação Histórica (Framework 02c)\n",
    "- ✅ Comparação com Baseline\n",
    "- ✅ Aprovação para Produção\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📦 Imports essenciais\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuração visual\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"✅ Bibliotecas carregadas\")\n",
    "print(f\"📅 Validação executada em: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 📂 1. Carregamento dos Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📁 Definir caminhos\n",
    "PROJECT_ROOT = Path().absolute().parent if Path().absolute().name == 'notebooks' else Path().absolute()\n",
    "MODELS_DIR = PROJECT_ROOT / 'data' / 'processed' / 'preds'\n",
    "\n",
    "print(f\"🗂️  Diretório do projeto: {PROJECT_ROOT}\")\n",
    "print(f\"🤖 Diretório dos modelos: {MODELS_DIR}\")\n",
    "\n",
    "# Carregar resultados principais\n",
    "results_files = {\n",
    "    'cv_metrics': MODELS_DIR / 'cv_metrics.json',\n",
    "    'historical_backtest': MODELS_DIR / 'historical_backtest_results.json',\n",
    "    'training_summary': MODELS_DIR / 'training_summary.json'\n",
    "}\n",
    "\n",
    "print(\"\\n📊 Carregando resultados:\")\n",
    "data = {}\n",
    "for key, file_path in results_files.items():\n",
    "    if file_path.exists():\n",
    "        with open(file_path, 'r') as f:\n",
    "            data[key] = json.load(f)\n",
    "        print(f\"✅ {key}: {file_path.name}\")\n",
    "    else:\n",
    "        print(f\"❌ {key}: {file_path.name} não encontrado\")\n",
    "        data[key] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 📈 2. Métricas de Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 Analisar métricas de CV\n",
    "if data['cv_metrics']:\n",
    "    cv_data = data['cv_metrics']\n",
    "    \n",
    "    print(\"📈 MÉTRICAS DE CROSS-VALIDATION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Extrair métricas por horizonte\n",
    "    horizons = [42, 48, 54, 60]\n",
    "    cv_summary = {}\n",
    "    \n",
    "    for horizon in horizons:\n",
    "        cv_file = MODELS_DIR / f'cv_metrics_T{horizon}.json'\n",
    "        if cv_file.exists():\n",
    "            with open(cv_file, 'r') as f:\n",
    "                cv_horizon_data = json.load(f)\n",
    "            \n",
    "            if 'mean_metrics' in cv_horizon_data:\n",
    "                metrics = cv_horizon_data['mean_metrics']\n",
    "                cv_summary[horizon] = metrics\n",
    "                \n",
    "                print(f\"\\n🎯 Horizonte {horizon}H:\")\n",
    "                print(f\"   📉 MAE: {metrics.get('MAE', 'N/A'):.4f}\")\n",
    "                print(f\"   📊 RMSE: {metrics.get('RMSE', 'N/A'):.4f}\")\n",
    "                print(f\"   🎯 Coverage 90%: {metrics.get('Coverage_90', 'N/A'):.3f}\")\n",
    "                \n",
    "                # Verificar se coverage está no range aceitável (87% - 93%)\n",
    "                coverage = metrics.get('Coverage_90', 0)\n",
    "                if 0.87 <= coverage <= 0.93:\n",
    "                    print(f\"   ✅ Coverage: OK (target: 90% ± 3%)\")\n",
    "                else:\n",
    "                    print(f\"   ⚠️ Coverage: ATENÇÃO (fora do range 87%-93%)\")\n",
    "    \n",
    "    # Visualizar métricas CV\n",
    "    if cv_summary:\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        \n",
    "        horizons_list = list(cv_summary.keys())\n",
    "        mae_values = [cv_summary[h].get('MAE', 0) for h in horizons_list]\n",
    "        rmse_values = [cv_summary[h].get('RMSE', 0) for h in horizons_list]\n",
    "        coverage_values = [cv_summary[h].get('Coverage_90', 0) for h in horizons_list]\n",
    "        \n",
    "        # MAE por horizonte\n",
    "        axes[0].bar(horizons_list, mae_values, color='lightcoral', alpha=0.7)\n",
    "        axes[0].set_title('MAE por Horizonte', fontweight='bold')\n",
    "        axes[0].set_xlabel('Horizonte (H)')\n",
    "        axes[0].set_ylabel('MAE')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # RMSE por horizonte\n",
    "        axes[1].bar(horizons_list, rmse_values, color='lightblue', alpha=0.7)\n",
    "        axes[1].set_title('RMSE por Horizonte', fontweight='bold')\n",
    "        axes[1].set_xlabel('Horizonte (H)')\n",
    "        axes[1].set_ylabel('RMSE')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Coverage por horizonte\n",
    "        bars = axes[2].bar(horizons_list, coverage_values, color='lightgreen', alpha=0.7)\n",
    "        axes[2].axhline(y=0.90, color='red', linestyle='--', alpha=0.8, label='Target (90%)')\n",
    "        axes[2].axhline(y=0.87, color='orange', linestyle=':', alpha=0.6, label='Min (87%)')\n",
    "        axes[2].axhline(y=0.93, color='orange', linestyle=':', alpha=0.6, label='Max (93%)')\n",
    "        axes[2].set_title('Coverage 90% por Horizonte', fontweight='bold')\n",
    "        axes[2].set_xlabel('Horizonte (H)')\n",
    "        axes[2].set_ylabel('Coverage')\n",
    "        axes[2].set_ylim(0.80, 1.0)\n",
    "        axes[2].legend()\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.suptitle('📊 Métricas de Cross-Validation - CQR_LightGBM', fontsize=16, fontweight='bold', y=1.02)\n",
    "        plt.show()\n",
    "        \n",
    "        # Resumo estatístico\n",
    "        avg_mae = np.mean(mae_values)\n",
    "        avg_coverage = np.mean(coverage_values)\n",
    "        coverage_in_range = sum(1 for c in coverage_values if 0.87 <= c <= 0.93)\n",
    "        \n",
    "        print(f\"\\n📋 RESUMO CV:\")\n",
    "        print(f\"📉 MAE Médio: {avg_mae:.4f}\")\n",
    "        print(f\"🎯 Coverage Médio: {avg_coverage:.3f}\")\n",
    "        print(f\"✅ Horizontes com coverage OK: {coverage_in_range}/{len(horizons_list)}\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ Dados de CV não disponíveis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 🔬 3. Validação Histórica (Framework 02c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎯 Analisar resultados do backtest histórico\n",
    "if data['historical_backtest']:\n",
    "    backtest_data = data['historical_backtest']\n",
    "    \n",
    "    print(\"🔬 VALIDAÇÃO HISTÓRICA (FRAMEWORK 02C)\")\n",
    "    print(\"=\" * 55)\n",
    "    \n",
    "    # Informações do framework\n",
    "    config = backtest_data.get('config', {})\n",
    "    gates_summary = backtest_data.get('gates_summary', {})\n",
    "    \n",
    "    print(f\"📅 Timestamp: {backtest_data.get('timestamp', 'N/A')}\")\n",
    "    print(f\"🏗️ Framework: {backtest_data.get('framework_version', 'N/A')}\")\n",
    "    print(f\"✅ Status: {backtest_data.get('validation_status', 'N/A')}\")\n",
    "    \n",
    "    # Configuração dos gates\n",
    "    if 'gates' in config:\n",
    "        gates_config = config['gates']\n",
    "        print(f\"\\n🎛️ CONFIGURAÇÃO DOS GATES:\")\n",
    "        print(f\"   📊 Coverage: {gates_config.get('coverage_min', 'N/A')} - {gates_config.get('coverage_max', 'N/A')}\")\n",
    "        print(f\"   🔄 Crossing rate: < {gates_config.get('crossing_rate_max', 'N/A')}\")\n",
    "        print(f\"   📈 PSI: < {gates_config.get('psi_max', 'N/A')}\")\n",
    "        print(f\"   📊 KS p-value: > {gates_config.get('ks_pvalue_min', 'N/A')}\")\n",
    "    \n",
    "    # Resultados por modelo\n",
    "    print(f\"\\n🤖 RESULTADOS POR MODELO:\")\n",
    "    for model_name, results in gates_summary.items():\n",
    "        total_passed = results.get('total_passed', 0)\n",
    "        total_gates = results.get('total_gates', 0)\n",
    "        approval_rate = results.get('approval_rate', 0)\n",
    "        decision = results.get('final_decision', 'N/A')\n",
    "        \n",
    "        status_icon = \"✅\" if decision == \"GO\" else \"❌\"\n",
    "        \n",
    "        print(f\"\\n{status_icon} {model_name}:\")\n",
    "        print(f\"   🎯 Gates aprovados: {total_passed}/{total_gates}\")\n",
    "        print(f\"   📊 Taxa de aprovação: {approval_rate:.1%}\")\n",
    "        print(f\"   🏁 Decisão final: {decision}\")\n",
    "    \n",
    "    # Visualizar resultados de aprovação\n",
    "    if gates_summary:\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        \n",
    "        # Gráfico de aprovação por modelo\n",
    "        models = list(gates_summary.keys())\n",
    "        approval_rates = [gates_summary[m]['approval_rate'] * 100 for m in models]\n",
    "        colors = ['green' if gates_summary[m]['final_decision'] == 'GO' else 'red' for m in models]\n",
    "        \n",
    "        bars1 = ax1.bar(models, approval_rates, color=colors, alpha=0.7)\n",
    "        ax1.set_title('Taxa de Aprovação - Sistema 12-Gates', fontweight='bold')\n",
    "        ax1.set_ylabel('Taxa de Aprovação (%)')\n",
    "        ax1.set_ylim(0, 105)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Adicionar valores nas barras\n",
    "        for bar, rate in zip(bars1, approval_rates):\n",
    "            height = bar.get_height()\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                    f'{rate:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # Gráfico de gates aprovados\n",
    "        gates_passed = [gates_summary[m]['total_passed'] for m in models]\n",
    "        gates_total = [gates_summary[m]['total_gates'] for m in models]\n",
    "        \n",
    "        x_pos = np.arange(len(models))\n",
    "        bars2 = ax2.bar(x_pos, gates_passed, color=colors, alpha=0.7, label='Aprovados')\n",
    "        ax2.bar(x_pos, [t - p for t, p in zip(gates_total, gates_passed)], \n",
    "               bottom=gates_passed, color='lightgray', alpha=0.5, label='Reprovados')\n",
    "        \n",
    "        ax2.set_title('Gates Aprovados vs Total', fontweight='bold')\n",
    "        ax2.set_ylabel('Número de Gates')\n",
    "        ax2.set_xticks(x_pos)\n",
    "        ax2.set_xticklabels(models)\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Adicionar valores nas barras\n",
    "        for i, (passed, total) in enumerate(zip(gates_passed, gates_total)):\n",
    "            ax2.text(i, passed/2, str(passed), ha='center', va='center', fontweight='bold')\n",
    "            if total - passed > 0:\n",
    "                ax2.text(i, passed + (total-passed)/2, str(total-passed), ha='center', va='center')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.suptitle('🎯 Resultados da Validação Histórica', fontsize=16, fontweight='bold', y=1.02)\n",
    "        plt.show()\n",
    "        \n",
    "        # Verificar se CQR_LightGBM foi aprovado\n",
    "        cqr_approved = gates_summary.get('CQR_LightGBM', {}).get('final_decision') == 'GO'\n",
    "        print(f\"\\n🎯 RESULTADO FINAL:\")\n",
    "        if cqr_approved:\n",
    "            print(f\"✅ CQR_LightGBM: APROVADO para produção\")\n",
    "            print(f\"🚀 Modelo passou em {gates_summary.get('CQR_LightGBM', {}).get('approval_rate', 0):.1%} dos testes\")\n",
    "        else:\n",
    "            print(f\"❌ CQR_LightGBM: NÃO APROVADO\")\n",
    "            print(f\"⚠️ Necessária revisão antes da produção\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ Dados de backtest histórico não disponíveis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## ⚖️ 4. Comparação com Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 Comparar com baseline (se disponível)\n",
    "if data['historical_backtest'] and 'fold_results' in data['historical_backtest']:\n",
    "    fold_results = data['historical_backtest']['fold_results']\n",
    "    \n",
    "    print(\"⚖️ COMPARAÇÃO COM BASELINE\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Extrair métricas dos modelos\n",
    "    model_performance = {}\n",
    "    \n",
    "    for fold in fold_results:\n",
    "        for model_name, model_data in fold['models'].items():\n",
    "            if model_name not in model_performance:\n",
    "                model_performance[model_name] = {'MAE': [], 'Coverage': []}\n",
    "            \n",
    "            # Calcular métricas médias por fold\n",
    "            mae_values = []\n",
    "            coverage_values = []\n",
    "            \n",
    "            for horizon, metrics in model_data['metrics'].items():\n",
    "                mae_values.append(metrics.get('MAE', 0))\n",
    "                coverage_values.append(metrics.get('Coverage_90', 0))\n",
    "            \n",
    "            if mae_values:\n",
    "                model_performance[model_name]['MAE'].append(np.mean(mae_values))\n",
    "            if coverage_values:\n",
    "                model_performance[model_name]['Coverage'].append(np.mean(coverage_values))\n",
    "    \n",
    "    # Calcular estatísticas finais\n",
    "    comparison_results = {}\n",
    "    for model_name, perf in model_performance.items():\n",
    "        comparison_results[model_name] = {\n",
    "            'avg_mae': np.mean(perf['MAE']) if perf['MAE'] else 0,\n",
    "            'avg_coverage': np.mean(perf['Coverage']) if perf['Coverage'] else 0,\n",
    "            'std_mae': np.std(perf['MAE']) if perf['MAE'] else 0\n",
    "        }\n",
    "    \n",
    "    # Display comparação\n",
    "    print(f\"\\n📊 MÉTRICAS COMPARATIVAS:\")\n",
    "    for model_name, results in comparison_results.items():\n",
    "        print(f\"\\n🤖 {model_name}:\")\n",
    "        print(f\"   📉 MAE Médio: {results['avg_mae']:.4f} (±{results['std_mae']:.4f})\")\n",
    "        print(f\"   🎯 Coverage Médio: {results['avg_coverage']:.3f}\")\n",
    "    \n",
    "    # Calcular melhoria se tivermos CQR_LightGBM e baseline\n",
    "    if 'CQR_LightGBM' in comparison_results and 'HAR-RV_Baseline' in comparison_results:\n",
    "        cqr_mae = comparison_results['CQR_LightGBM']['avg_mae']\n",
    "        baseline_mae = comparison_results['HAR-RV_Baseline']['avg_mae']\n",
    "        \n",
    "        if baseline_mae > 0:\n",
    "            improvement = ((baseline_mae - cqr_mae) / baseline_mae) * 100\n",
    "            \n",
    "            print(f\"\\n🚀 MELHORIA DO CQR_LightGBM:\")\n",
    "            print(f\"📈 Redução no MAE: {improvement:.1f}%\")\n",
    "            print(f\"📊 MAE Baseline: {baseline_mae:.4f}\")\n",
    "            print(f\"📊 MAE CQR: {cqr_mae:.4f}\")\n",
    "            \n",
    "            if improvement > 20:\n",
    "                print(f\"✅ Melhoria significativa (>{20}%)\")\n",
    "            elif improvement > 10:\n",
    "                print(f\"✅ Melhoria moderada ({10}-{20}%)\")\n",
    "            else:\n",
    "                print(f\"⚠️ Melhoria marginal (<{10}%)\")\n",
    "    \n",
    "    # Visualizar comparação\n",
    "    if len(comparison_results) > 1:\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        \n",
    "        models = list(comparison_results.keys())\n",
    "        mae_values = [comparison_results[m]['avg_mae'] for m in models]\n",
    "        coverage_values = [comparison_results[m]['avg_coverage'] for m in models]\n",
    "        \n",
    "        # Comparação MAE\n",
    "        colors = ['green' if 'CQR' in m else 'blue' for m in models]\n",
    "        bars1 = ax1.bar(models, mae_values, color=colors, alpha=0.7)\n",
    "        ax1.set_title('Comparação MAE', fontweight='bold')\n",
    "        ax1.set_ylabel('MAE')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Adicionar valores\n",
    "        for bar, mae in zip(bars1, mae_values):\n",
    "            height = bar.get_height()\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                    f'{mae:.4f}', ha='center', va='bottom', fontsize=10)\n",
    "        \n",
    "        # Comparação Coverage\n",
    "        bars2 = ax2.bar(models, coverage_values, color=colors, alpha=0.7)\n",
    "        ax2.axhline(y=0.90, color='red', linestyle='--', alpha=0.8, label='Target (90%)')\n",
    "        ax2.set_title('Comparação Coverage', fontweight='bold')\n",
    "        ax2.set_ylabel('Coverage 90%')\n",
    "        ax2.set_ylim(0.80, 1.0)\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Adicionar valores\n",
    "        for bar, cov in zip(bars2, coverage_values):\n",
    "            height = bar.get_height()\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "                    f'{cov:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.suptitle('⚖️ Comparação: CQR_LightGBM vs Baseline', fontsize=16, fontweight='bold', y=1.02)\n",
    "        plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"❌ Dados de comparação não disponíveis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 🎯 5. Decisão Final para Produção"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🏁 Decisão final baseada em todos os critérios\n",
    "print(\"🎯 DECISÃO FINAL PARA PRODUÇÃO\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Critérios de aprovação\n",
    "criteria = {\n",
    "    'cv_quality': False,\n",
    "    'backtest_approval': False,\n",
    "    'performance_improvement': False,\n",
    "    'technical_validation': False\n",
    "}\n",
    "\n",
    "reasons = []\n",
    "\n",
    "# 1. Verificar qualidade do CV\n",
    "if cv_summary:\n",
    "    cv_coverage_ok = sum(1 for h in cv_summary.values() if 0.87 <= h.get('Coverage_90', 0) <= 0.93)\n",
    "    if cv_coverage_ok >= len(cv_summary) * 0.75:  # 75% dos horizontes OK\n",
    "        criteria['cv_quality'] = True\n",
    "        reasons.append(\"✅ Cross-validation: Coverage adequado na maioria dos horizontes\")\n",
    "    else:\n",
    "        reasons.append(\"❌ Cross-validation: Coverage fora do range em muitos horizontes\")\n",
    "else:\n",
    "    reasons.append(\"⚠️ Cross-validation: Dados não disponíveis\")\n",
    "\n",
    "# 2. Verificar aprovação no backtest\n",
    "if data['historical_backtest'] and 'gates_summary' in data['historical_backtest']:\n",
    "    cqr_result = data['historical_backtest']['gates_summary'].get('CQR_LightGBM', {})\n",
    "    if cqr_result.get('final_decision') == 'GO':\n",
    "        criteria['backtest_approval'] = True\n",
    "        approval_rate = cqr_result.get('approval_rate', 0)\n",
    "        reasons.append(f\"✅ Backtest histórico: Aprovado (taxa: {approval_rate:.1%})\")\n",
    "    else:\n",
    "        reasons.append(\"❌ Backtest histórico: Não aprovado no sistema de gates\")\n",
    "else:\n",
    "    reasons.append(\"⚠️ Backtest histórico: Dados não disponíveis\")\n",
    "\n",
    "# 3. Verificar melhoria de performance\n",
    "if 'CQR_LightGBM' in comparison_results and 'HAR-RV_Baseline' in comparison_results:\n",
    "    cqr_mae = comparison_results['CQR_LightGBM']['avg_mae']\n",
    "    baseline_mae = comparison_results['HAR-RV_Baseline']['avg_mae']\n",
    "    \n",
    "    if baseline_mae > 0:\n",
    "        improvement = ((baseline_mae - cqr_mae) / baseline_mae) * 100\n",
    "        if improvement > 10:  # Mínimo 10% de melhoria\n",
    "            criteria['performance_improvement'] = True\n",
    "            reasons.append(f\"✅ Performance: Melhoria de {improvement:.1f}% no MAE\")\n",
    "        else:\n",
    "            reasons.append(f\"❌ Performance: Melhoria insuficiente ({improvement:.1f}%)\")\n",
    "    else:\n",
    "        reasons.append(\"⚠️ Performance: Não foi possível calcular melhoria\")\n",
    "else:\n",
    "    reasons.append(\"⚠️ Performance: Dados de comparação não disponíveis\")\n",
    "\n",
    "# 4. Verificar validação técnica\n",
    "quality_report_file = MODELS_DIR / 'quality_check_report.json'\n",
    "if quality_report_file.exists():\n",
    "    with open(quality_report_file, 'r') as f:\n",
    "        quality_data = json.load(f)\n",
    "    \n",
    "    overall_quality = quality_data.get('overall_quality', 0)\n",
    "    if overall_quality >= 80:\n",
    "        criteria['technical_validation'] = True\n",
    "        reasons.append(f\"✅ Validação técnica: Score de qualidade {overall_quality:.1f}%\")\n",
    "    else:\n",
    "        reasons.append(f\"❌ Validação técnica: Score baixo ({overall_quality:.1f}%)\")\n",
    "else:\n",
    "    reasons.append(\"⚠️ Validação técnica: Execute o notebook de qualidade primeiro\")\n",
    "\n",
    "# Decisão final\n",
    "approved_criteria = sum(criteria.values())\n",
    "total_criteria = len(criteria)\n",
    "\n",
    "print(f\"\\n📊 CRITÉRIOS DE AVALIAÇÃO ({approved_criteria}/{total_criteria}):\")\n",
    "for reason in reasons:\n",
    "    print(f\"   {reason}\")\n",
    "\n",
    "# Decisão\n",
    "if approved_criteria >= 3:  # Pelo menos 3 dos 4 critérios\n",
    "    final_decision = \"🟢 APROVADO PARA PRODUÇÃO\"\n",
    "    recommendation = \"✅ O modelo CQR_LightGBM está pronto para deploy em produção\"\n",
    "    confidence = \"Alta\" if approved_criteria == 4 else \"Média\"\n",
    "elif approved_criteria >= 2:\n",
    "    final_decision = \"🟡 APROVAÇÃO CONDICIONAL\"\n",
    "    recommendation = \"⚠️ Modelo pode ir para produção com monitoramento reforçado\"\n",
    "    confidence = \"Média\"\n",
    "else:\n",
    "    final_decision = \"🔴 NÃO APROVADO\"\n",
    "    recommendation = \"❌ Modelo precisa de melhorias antes da produção\"\n",
    "    confidence = \"Baixa\"\n",
    "\n",
    "print(f\"\\n🏁 DECISÃO FINAL: {final_decision}\")\n",
    "print(f\"💡 RECOMENDAÇÃO: {recommendation}\")\n",
    "print(f\"🎯 CONFIANÇA: {confidence}\")\n",
    "\n",
    "# Salvar decisão final\n",
    "final_assessment = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'criteria': criteria,\n",
    "    'approved_criteria': approved_criteria,\n",
    "    'total_criteria': total_criteria,\n",
    "    'final_decision': final_decision,\n",
    "    'recommendation': recommendation,\n",
    "    'confidence': confidence,\n",
    "    'reasons': reasons\n",
    "}\n",
    "\n",
    "assessment_file = MODELS_DIR / 'production_assessment.json'\n",
    "with open(assessment_file, 'w') as f:\n",
    "    json.dump(final_assessment, f, indent=2)\n",
    "\n",
    "print(f\"\\n💾 Avaliação salva em: {assessment_file}\")\n",
    "print(\"\\n🎉 Validação de performance concluída!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
