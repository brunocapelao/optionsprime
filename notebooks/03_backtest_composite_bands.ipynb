{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# üß™ Backtest de Faixas Compostas - Pseudo-Backtest\n",
    "\n",
    "Este notebook avalia a qualidade das **faixas compostas** atrav√©s de um pseudo-backtest.\n",
    "\n",
    "## üìã Metodologia\n",
    "\n",
    "- **Tipo**: Pseudo-backtest (modelo atual aplicado a dados hist√≥ricos)\n",
    "- **Per√≠odo**: √öltimos N pontos temporais dispon√≠veis\n",
    "- **Horizonte**: T=42, 48, 54, 60 barras de 4H (7-10 dias)\n",
    "- **Abordagem Composta**: Cada data futura usa a previs√£o do modelo espec√≠fico para aquele horizonte\n",
    "\n",
    "## ‚ö†Ô∏è Limita√ß√£o: Look-Ahead Bias\n",
    "\n",
    "Como estamos usando o modelo atual (treinado at√© hoje) para avaliar previs√µes no passado,\n",
    "existe um **vi√©s de look-ahead**: o modelo \"viu\" os dados que estamos testando durante o treinamento.\n",
    "\n",
    "**Interpreta√ß√£o correta:**\n",
    "- ‚úÖ V√°lido para avaliar **calibra√ß√£o** das faixas (coverage dos intervalos)\n",
    "- ‚úÖ V√°lido para avaliar **consist√™ncia** temporal\n",
    "- ‚ö†Ô∏è Limitado para avaliar **poder preditivo absoluto** (otimista demais)\n",
    "\n",
    "## üéØ Objetivos\n",
    "\n",
    "1. Verificar se os intervalos de confian√ßa est√£o bem calibrados (90% CI ‚âà 90%, 50% CI ‚âà 50%)\n",
    "2. Avaliar sharpness (largura) das faixas\n",
    "3. Analisar erros da mediana (p50)\n",
    "4. Identificar padr√µes temporais e regimes de mercado\n",
    "5. Comparar faixas compostas vs modelos individuais"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Setup e Configura√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configura√ß√µes de visualiza√ß√£o\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (16, 10)\n",
    "plt.rcParams['font.size'] = 10\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"‚úÖ Imports carregados\")\n",
    "print(f\"üìÖ Data: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Par√¢metros do Backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura√ß√µes\n",
    "CONFIG = {\n",
    "    'horizons': [42, 48, 54, 60],  # Horizontes em barras de 4H\n",
    "    'n_backtest_points': 50,        # Quantos pontos ts0 avaliar\n",
    "    'bar_frequency_hours': 4,       # Frequ√™ncia dos dados\n",
    "    'confidence_levels': [0.90, 0.50],  # N√≠veis de confian√ßa a avaliar\n",
    "    'tolerance_hours': 4,           # Toler√¢ncia para matching temporal\n",
    "}\n",
    "\n",
    "# Caminhos\n",
    "data_dir = Path('../data/processed')\n",
    "features_path = data_dir / 'features' / 'features_4H.parquet'\n",
    "preds_dir = data_dir / 'preds'\n",
    "output_dir = data_dir / 'models' / 'report'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"‚öôÔ∏è Configura√ß√£o do Backtest:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Fun√ß√µes Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_realized_price(ts_forecast: pd.Timestamp, df_features: pd.DataFrame, \n",
    "                       tolerance_hours: int = 4) -> float:\n",
    "    \"\"\"\n",
    "    Busca o pre√ßo realizado mais pr√≥ximo de ts_forecast.\n",
    "    \n",
    "    Args:\n",
    "        ts_forecast: Data alvo da previs√£o\n",
    "        df_features: DataFrame com dados hist√≥ricos\n",
    "        tolerance_hours: Toler√¢ncia m√°xima em horas\n",
    "    \n",
    "    Returns:\n",
    "        Pre√ßo realizado (close) ou None se n√£o encontrado\n",
    "    \"\"\"\n",
    "    # Calcular diferen√ßa temporal\n",
    "    df_features['time_diff'] = (df_features['ts'] - ts_forecast).abs()\n",
    "    \n",
    "    # Encontrar o mais pr√≥ximo\n",
    "    closest_idx = df_features['time_diff'].idxmin()\n",
    "    closest_row = df_features.loc[closest_idx]\n",
    "    \n",
    "    # Verificar se est√° dentro da toler√¢ncia\n",
    "    if closest_row['time_diff'] <= pd.Timedelta(hours=tolerance_hours):\n",
    "        return closest_row['close']\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def calculate_coverage(y_true: np.ndarray, y_lower: np.ndarray, \n",
    "                       y_upper: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calcula a cobertura de um intervalo de confian√ßa.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Valores reais\n",
    "        y_lower: Limite inferior do intervalo\n",
    "        y_upper: Limite superior do intervalo\n",
    "    \n",
    "    Returns:\n",
    "        Percentual de vezes que y_true est√° dentro do intervalo\n",
    "    \"\"\"\n",
    "    within_interval = (y_true >= y_lower) & (y_true <= y_upper)\n",
    "    return np.mean(within_interval) * 100\n",
    "\n",
    "\n",
    "def pinball_loss(y_true: np.ndarray, y_pred: np.ndarray, \n",
    "                 quantile: float) -> float:\n",
    "    \"\"\"\n",
    "    Calcula o Pinball Loss para avalia√ß√£o de quantis.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Valores reais\n",
    "        y_pred: Valores previstos\n",
    "        quantile: N√≠vel do quantil (0-1)\n",
    "    \n",
    "    Returns:\n",
    "        Pinball loss m√©dio\n",
    "    \"\"\"\n",
    "    error = y_true - y_pred\n",
    "    loss = np.where(error >= 0, \n",
    "                    quantile * error, \n",
    "                    (quantile - 1) * error)\n",
    "    return np.mean(loss)\n",
    "\n",
    "\n",
    "def calculate_sharpness(y_lower: np.ndarray, y_upper: np.ndarray, \n",
    "                        s0: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calcula a largura m√©dia do intervalo (normalizada por S0).\n",
    "    \n",
    "    Args:\n",
    "        y_lower: Limite inferior\n",
    "        y_upper: Limite superior\n",
    "        s0: Pre√ßo de refer√™ncia\n",
    "    \n",
    "    Returns:\n",
    "        Largura m√©dia em percentual\n",
    "    \"\"\"\n",
    "    width = (y_upper - y_lower) / s0 * 100\n",
    "    return np.mean(width)\n",
    "\n",
    "\n",
    "print(\"‚úÖ Fun√ß√µes auxiliares definidas:\")\n",
    "print(\"   ‚Ä¢ get_realized_price()\")\n",
    "print(\"   ‚Ä¢ calculate_coverage()\")\n",
    "print(\"   ‚Ä¢ pinball_loss()\")\n",
    "print(\"   ‚Ä¢ calculate_sharpness()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 2. Carregamento de Dados Hist√≥ricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar features (pre√ßos hist√≥ricos)\n",
    "print(\"üìÇ Carregando dados hist√≥ricos...\")\n",
    "df_features = pd.read_parquet(features_path)\n",
    "df_features['ts'] = pd.to_datetime(df_features['ts'], utc=True)\n",
    "df_features = df_features.sort_values('ts').reset_index(drop=True)\n",
    "\n",
    "print(f\"   ‚úì Features: {len(df_features):,} linhas\")\n",
    "print(f\"   ‚úì Per√≠odo: {df_features['ts'].min()} a {df_features['ts'].max()}\")\n",
    "print(f\"   ‚úì Colunas: {list(df_features.columns[:10])}...\")\n",
    "\n",
    "# Carregar predi√ß√µes atuais (para usar como template)\n",
    "print(\"\\nüìÇ Carregando predi√ß√µes atuais...\")\n",
    "dfs_pred = {}\n",
    "\n",
    "for T in CONFIG['horizons']:\n",
    "    pred_file = preds_dir / f'preds_T={T}.parquet'\n",
    "    if pred_file.exists():\n",
    "        df_temp = pd.read_parquet(pred_file)\n",
    "        df_temp['ts0'] = pd.to_datetime(df_temp['ts0'], utc=True)\n",
    "        if 'ts_forecast' in df_temp.columns:\n",
    "            df_temp['ts_forecast'] = pd.to_datetime(df_temp['ts_forecast'], utc=True)\n",
    "        dfs_pred[T] = df_temp\n",
    "        print(f\"   ‚úì T={T}: {len(df_temp)} predi√ß√µes\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  T={T}: arquivo n√£o encontrado\")\n",
    "\n",
    "print(f\"\\n‚úÖ {len(dfs_pred)} arquivos de predi√ß√£o carregados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 3. Gera√ß√£o de Previs√µes para Backtest (Pseudo-Backtest)\n",
    "\n",
    "Vamos selecionar N pontos temporais hist√≥ricos e gerar previs√µes para cada um,\n",
    "simulando como seriam as previs√µes se tiv√©ssemos feito naquele momento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionar pontos ts0 para backtest\n",
    "# Vamos pegar os √∫ltimos N pontos, mas deixando espa√ßo para as previs√µes se realizarem\n",
    "max_horizon_days = max(CONFIG['horizons']) * CONFIG['bar_frequency_hours'] / 24\n",
    "buffer_days = int(max_horizon_days) + 2  # Margem de seguran√ßa\n",
    "\n",
    "# Data m√°xima poss√≠vel para ts0 (deixar espa√ßo para forecast se realizar)\n",
    "max_ts0 = df_features['ts'].max() - pd.Timedelta(days=buffer_days)\n",
    "\n",
    "# Filtrar features at√© max_ts0\n",
    "df_features_backtest = df_features[df_features['ts'] <= max_ts0].copy()\n",
    "\n",
    "# Selecionar √∫ltimos N pontos\n",
    "n_points = min(CONFIG['n_backtest_points'], len(df_features_backtest))\n",
    "backtest_indices = np.linspace(\n",
    "    len(df_features_backtest) - n_points,\n",
    "    len(df_features_backtest) - 1,\n",
    "    n_points,\n",
    "    dtype=int\n",
    ")\n",
    "\n",
    "ts0_points = df_features_backtest.iloc[backtest_indices]['ts'].values\n",
    "\n",
    "print(f\"üéØ Selecionando pontos ts0 para backtest:\")\n",
    "print(f\"   ‚Ä¢ Total de pontos: {n_points}\")\n",
    "print(f\"   ‚Ä¢ Primeiro ts0: {pd.Timestamp(ts0_points[0])}\")\n",
    "print(f\"   ‚Ä¢ √öltimo ts0: {pd.Timestamp(ts0_points[-1])}\")\n",
    "print(f\"   ‚Ä¢ Data m√°xima features: {df_features['ts'].max()}\")\n",
    "print(f\"   ‚Ä¢ Buffer para forecasts: {buffer_days} dias\")\n",
    "print(f\"   ‚Ä¢ Horizonte m√°ximo: {max_horizon_days:.1f} dias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar pseudo-previs√µes para cada ts0\n",
    "# Usaremos os quantis das previs√µes atuais como template\n",
    "# (assumindo que a distribui√ß√£o seria similar)\n",
    "\n",
    "print(\"üîÆ Gerando pseudo-previs√µes...\")\n",
    "print(\"‚ö†Ô∏è  Nota: Usando modelo atual = look-ahead bias presente\\n\")\n",
    "\n",
    "backtest_predictions = []\n",
    "\n",
    "for idx, ts0 in enumerate(ts0_points):\n",
    "    ts0 = pd.Timestamp(ts0, tz='UTC')  # ‚≠ê Adicionar timezone\n",
    "    \n",
    "    # Pegar S0 (pre√ßo no momento ts0)\n",
    "    # Usar busca por proximidade em vez de igualdade exata\n",
    "    time_diffs = (df_features['ts'] - ts0).abs()\n",
    "    closest_idx = time_diffs.idxmin()\n",
    "    s0_row = df_features.loc[closest_idx]\n",
    "    S0 = s0_row['close']\n",
    "    \n",
    "    # Para cada horizonte\n",
    "    for T in CONFIG['horizons']:\n",
    "        # Calcular ts_forecast\n",
    "        ts_forecast = ts0 + pd.Timedelta(hours=T * CONFIG['bar_frequency_hours'])\n",
    "        \n",
    "        # Pegar valores realizados\n",
    "        price_realized = get_realized_price(ts_forecast, df_features, \n",
    "                                            CONFIG['tolerance_hours'])\n",
    "        \n",
    "        if price_realized is None:\n",
    "            continue  # Pular se n√£o temos o valor realizado\n",
    "        \n",
    "        # Usar os quantis das previs√µes atuais como propor√ß√£o de S0\n",
    "        # (isso √© a simplifica√ß√£o do pseudo-backtest)\n",
    "        if T in dfs_pred and len(dfs_pred[T]) > 0:\n",
    "            # Pegar uma previs√£o recente como template\n",
    "            template = dfs_pred[T].iloc[-1]\n",
    "            \n",
    "            # Calcular quantis proporcionalmente a S0\n",
    "            ratio_05 = template['p_05'] / template['S0']\n",
    "            ratio_25 = template['p_25'] / template['S0']\n",
    "            ratio_50 = template['p_50'] / template['S0']\n",
    "            ratio_75 = template['p_75'] / template['S0']\n",
    "            ratio_95 = template['p_95'] / template['S0']\n",
    "            \n",
    "            backtest_predictions.append({\n",
    "                'ts0': ts0,\n",
    "                'T': T,\n",
    "                'ts_forecast': ts_forecast,\n",
    "                'S0': S0,\n",
    "                'p_05': S0 * ratio_05,\n",
    "                'p_25': S0 * ratio_25,\n",
    "                'p_50': S0 * ratio_50,\n",
    "                'p_75': S0 * ratio_75,\n",
    "                'p_95': S0 * ratio_95,\n",
    "                'price_realized': price_realized,\n",
    "                'days_ahead': T * CONFIG['bar_frequency_hours'] / 24,\n",
    "            })\n",
    "    \n",
    "    if (idx + 1) % 10 == 0:\n",
    "        print(f\"   Processados: {idx + 1}/{n_points} pontos ts0\")\n",
    "\n",
    "# Criar DataFrame\n",
    "df_backtest = pd.DataFrame(backtest_predictions)\n",
    "\n",
    "print(f\"\\n‚úÖ Pseudo-previs√µes geradas:\")\n",
    "print(f\"   ‚Ä¢ Total: {len(df_backtest)} previs√µes\")\n",
    "if len(df_backtest) > 0:\n",
    "    print(f\"   ‚Ä¢ Por horizonte:\")\n",
    "    for T in CONFIG['horizons']:\n",
    "        count = len(df_backtest[df_backtest['T'] == T])\n",
    "        print(f\"      - T={T}: {count} previs√µes\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  Nenhuma previs√£o gerada! Verifique os dados.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Verificar por que n√£o temos previs√µes\n",
    "print(\"üîç DEBUG: Verificando gera√ß√£o de previs√µes...\")\n",
    "\n",
    "test_ts0 = pd.Timestamp(ts0_points[0])\n",
    "print(f\"\\nTestando com ts0={test_ts0}\")\n",
    "\n",
    "# Verificar S0\n",
    "s0_row = df_features[df_features['ts'] == test_ts0]\n",
    "print(f\"Linhas com ts0: {len(s0_row)}\")\n",
    "if len(s0_row) > 0:\n",
    "    S0 = s0_row.iloc[0]['close']\n",
    "    print(f\"S0: ${S0:,.2f}\")\n",
    "    \n",
    "    # Testar um horizonte\n",
    "    T = 42\n",
    "    ts_forecast = test_ts0 + pd.Timedelta(hours=T * CONFIG['bar_frequency_hours'])\n",
    "    print(f\"\\nT={T}, ts_forecast={ts_forecast}\")\n",
    "    \n",
    "    # Testar get_realized_price\n",
    "    price_realized = get_realized_price(ts_forecast, df_features, CONFIG['tolerance_hours'])\n",
    "    print(f\"Price realized: {price_realized}\")\n",
    "    \n",
    "    if price_realized:\n",
    "        print(\"‚úÖ Conseguiu pegar pre√ßo realizado\")\n",
    "    else:\n",
    "        print(\"‚ùå N√£o conseguiu pegar pre√ßo realizado\")\n",
    "        \n",
    "        # Ver qual √© o timestamp mais pr√≥ximo\n",
    "        df_features_temp = df_features.copy()\n",
    "        df_features_temp['time_diff'] = (df_features_temp['ts'] - ts_forecast).abs()\n",
    "        closest = df_features_temp.nsmallest(5, 'time_diff')[['ts', 'close', 'time_diff']]\n",
    "        print(\"\\nTimestamps mais pr√≥ximos:\")\n",
    "        print(closest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## 4. Constru√ß√£o das Faixas Compostas\n",
    "\n",
    "Para cada data futura, selecionamos a previs√£o do modelo com horizonte espec√≠fico para aquela data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para faixas compostas, cada ts_forecast deve usar a previs√£o do horizonte correto\n",
    "# Precisamos agrupar por ts_forecast e pegar apenas a previs√£o \"nativa\" daquela data\n",
    "\n",
    "print(\"üé® Construindo faixas compostas...\")\n",
    "\n",
    "# Criar mapeamento: ts_forecast ‚Üí horizonte esperado\n",
    "# Para cada ts0, calculamos qual T corresponde a cada data futura\n",
    "\n",
    "composite_forecasts = []\n",
    "\n",
    "# Agrupar por ts0\n",
    "for ts0 in df_backtest['ts0'].unique():\n",
    "    ts0_preds = df_backtest[df_backtest['ts0'] == ts0]\n",
    "    \n",
    "    # Para cada horizonte, essa √© a previs√£o \"nativa\" para aquela data espec√≠fica\n",
    "    for T in CONFIG['horizons']:\n",
    "        t_pred = ts0_preds[ts0_preds['T'] == T]\n",
    "        if len(t_pred) > 0:\n",
    "            composite_forecasts.append(t_pred.iloc[0].to_dict())\n",
    "\n",
    "df_composite = pd.DataFrame(composite_forecasts)\n",
    "\n",
    "print(f\"   ‚úì Total de previs√µes compostas: {len(df_composite)}\")\n",
    "print(f\"   ‚úì Per√≠odo: {df_composite['ts0'].min()} a {df_composite['ts0'].max()}\")\n",
    "print(f\"   ‚úì Forecast dates: {df_composite['ts_forecast'].min()} a {df_composite['ts_forecast'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## 5. M√©tricas de Calibra√ß√£o\n",
    "\n",
    "Avaliar se os intervalos de confian√ßa est√£o bem calibrados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä M√âTRICAS DE CALIBRA√á√ÉO - FAIXAS COMPOSTAS\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "# Extrair arrays\n",
    "y_true = df_composite['price_realized'].values\n",
    "y_p05 = df_composite['p_05'].values\n",
    "y_p25 = df_composite['p_25'].values\n",
    "y_p50 = df_composite['p_50'].values\n",
    "y_p75 = df_composite['p_75'].values\n",
    "y_p95 = df_composite['p_95'].values\n",
    "S0 = df_composite['S0'].values\n",
    "\n",
    "# 1. Coverage dos intervalos de confian√ßa\n",
    "coverage_90 = calculate_coverage(y_true, y_p05, y_p95)\n",
    "coverage_50 = calculate_coverage(y_true, y_p25, y_p75)\n",
    "\n",
    "print(f\"\\nüéØ COVERAGE (Cobertura dos Intervalos):\")\n",
    "print(f\"   ‚Ä¢ 90% CI (p05-p95): {coverage_90:.2f}% (esperado: ~90%)\")\n",
    "if 85 <= coverage_90 <= 95:\n",
    "    print(f\"      ‚úÖ Bem calibrado!\")\n",
    "else:\n",
    "    print(f\"      ‚ö†Ô∏è  {'Subestimado' if coverage_90 < 85 else 'Superestimado'}\")\n",
    "\n",
    "print(f\"   ‚Ä¢ 50% CI (p25-p75): {coverage_50:.2f}% (esperado: ~50%)\")\n",
    "if 45 <= coverage_50 <= 55:\n",
    "    print(f\"      ‚úÖ Bem calibrado!\")\n",
    "else:\n",
    "    print(f\"      ‚ö†Ô∏è  {'Subestimado' if coverage_50 < 45 else 'Superestimado'}\")\n",
    "\n",
    "# 2. Sharpness (largura das faixas)\n",
    "sharpness_90 = calculate_sharpness(y_p05, y_p95, S0)\n",
    "sharpness_50 = calculate_sharpness(y_p25, y_p75, S0)\n",
    "\n",
    "print(f\"\\nüìè SHARPNESS (Largura das Faixas):\")\n",
    "print(f\"   ‚Ä¢ 90% CI: {sharpness_90:.2f}%\")\n",
    "print(f\"   ‚Ä¢ 50% CI: {sharpness_50:.2f}%\")\n",
    "\n",
    "# 3. Erros da mediana (p50)\n",
    "mae = np.mean(np.abs(y_true - y_p50))\n",
    "mape = np.mean(np.abs(y_true - y_p50) / y_true) * 100\n",
    "rmse = np.sqrt(np.mean((y_true - y_p50)**2))\n",
    "bias = np.mean(y_p50 - y_true)\n",
    "\n",
    "print(f\"\\nüìâ ERROS DA MEDIANA (p50):\")\n",
    "print(f\"   ‚Ä¢ MAE:  ${mae:,.2f}\")\n",
    "print(f\"   ‚Ä¢ MAPE: {mape:.2f}%\")\n",
    "print(f\"   ‚Ä¢ RMSE: ${rmse:,.2f}\")\n",
    "print(f\"   ‚Ä¢ Bias: ${bias:,.2f} ({'otimista' if bias > 0 else 'pessimista'})\")\n",
    "\n",
    "# 4. Pinball Loss por quantil\n",
    "loss_05 = pinball_loss(y_true, y_p05, 0.05)\n",
    "loss_25 = pinball_loss(y_true, y_p25, 0.25)\n",
    "loss_50 = pinball_loss(y_true, y_p50, 0.50)\n",
    "loss_75 = pinball_loss(y_true, y_p75, 0.75)\n",
    "loss_95 = pinball_loss(y_true, y_p95, 0.95)\n",
    "\n",
    "print(f\"\\nüé≤ PINBALL LOSS (Qualidade dos Quantis):\")\n",
    "print(f\"   ‚Ä¢ p05: {loss_05:.2f}\")\n",
    "print(f\"   ‚Ä¢ p25: {loss_25:.2f}\")\n",
    "print(f\"   ‚Ä¢ p50: {loss_50:.2f} (menor √© melhor)\")\n",
    "print(f\"   ‚Ä¢ p75: {loss_75:.2f}\")\n",
    "print(f\"   ‚Ä¢ p95: {loss_95:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### M√©tricas por Horizonte\n",
    "\n",
    "Comparar a performance de cada horizonte individual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä M√âTRICAS POR HORIZONTE\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "metrics_by_horizon = []\n",
    "\n",
    "for T in CONFIG['horizons']:\n",
    "    df_T = df_composite[df_composite['T'] == T]\n",
    "    \n",
    "    if len(df_T) == 0:\n",
    "        continue\n",
    "    \n",
    "    y_true_T = df_T['price_realized'].values\n",
    "    y_p05_T = df_T['p_05'].values\n",
    "    y_p25_T = df_T['p_25'].values\n",
    "    y_p50_T = df_T['p_50'].values\n",
    "    y_p75_T = df_T['p_75'].values\n",
    "    y_p95_T = df_T['p_95'].values\n",
    "    S0_T = df_T['S0'].values\n",
    "    \n",
    "    coverage_90_T = calculate_coverage(y_true_T, y_p05_T, y_p95_T)\n",
    "    coverage_50_T = calculate_coverage(y_true_T, y_p25_T, y_p75_T)\n",
    "    sharpness_90_T = calculate_sharpness(y_p05_T, y_p95_T, S0_T)\n",
    "    sharpness_50_T = calculate_sharpness(y_p25_T, y_p75_T, S0_T)\n",
    "    mae_T = np.mean(np.abs(y_true_T - y_p50_T))\n",
    "    mape_T = np.mean(np.abs(y_true_T - y_p50_T) / y_true_T) * 100\n",
    "    \n",
    "    days_ahead = T * CONFIG['bar_frequency_hours'] / 24\n",
    "    \n",
    "    metrics_by_horizon.append({\n",
    "        'Horizonte': f'T={T} ({days_ahead:.0f}d)',\n",
    "        'N': len(df_T),\n",
    "        'Cov 90%': f'{coverage_90_T:.1f}%',\n",
    "        'Cov 50%': f'{coverage_50_T:.1f}%',\n",
    "        'Largura 90%': f'{sharpness_90_T:.2f}%',\n",
    "        'Largura 50%': f'{sharpness_50_T:.2f}%',\n",
    "        'MAE': f'${mae_T:,.0f}',\n",
    "        'MAPE': f'{mape_T:.2f}%',\n",
    "    })\n",
    "\n",
    "df_metrics_horizon = pd.DataFrame(metrics_by_horizon)\n",
    "print(df_metrics_horizon.to_string(index=False))\n",
    "print(\"\\\\n\" + \"=\"*90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## üö® An√°lise Cr√≠tica: Os Resultados S√£o Preocupantes?\n",
    "\n",
    "Vamos interpretar os resultados com contexto adequado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç AN√ÅLISE CONTEXTUAL DOS RESULTADOS\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "# 1. Analisar o per√≠odo testado\n",
    "print(\"\\nüìÖ 1. CONTEXTO TEMPORAL\")\n",
    "print(\"-\"*90)\n",
    "\n",
    "# Verificar movimento de pre√ßos no per√≠odo\n",
    "ts0_min = df_composite['ts0'].min()\n",
    "ts0_max = df_composite['ts0'].max()\n",
    "ts_forecast_min = df_composite['ts_forecast'].min()\n",
    "ts_forecast_max = df_composite['ts_forecast'].max()\n",
    "\n",
    "# Pegar pre√ßos do per√≠odo\n",
    "prices_start = df_features[df_features['ts'] >= ts0_min]['close'].iloc[0] if len(df_features[df_features['ts'] >= ts0_min]) > 0 else None\n",
    "prices_end = df_features[df_features['ts'] <= ts_forecast_max].iloc[-1]['close'] if len(df_features[df_features['ts'] <= ts_forecast_max]) > 0 else None\n",
    "\n",
    "if prices_start and prices_end:\n",
    "    price_change = (prices_end - prices_start) / prices_start * 100\n",
    "    print(f\"Per√≠odo de previs√£o: {ts0_min.strftime('%Y-%m-%d')} a {ts_forecast_max.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"Pre√ßo inicial: ${prices_start:,.2f}\")\n",
    "    print(f\"Pre√ßo final: ${prices_end:,.2f}\")\n",
    "    print(f\"Varia√ß√£o: {price_change:+.2f}%\")\n",
    "    \n",
    "    if price_change < -5:\n",
    "        print(f\"\\n‚ö†Ô∏è  QUEDA SIGNIFICATIVA no per√≠odo testado!\")\n",
    "        print(f\"   Isso explica o bias otimista do modelo.\")\n",
    "    elif price_change > 5:\n",
    "        print(f\"\\nüìà ALTA SIGNIFICATIVA no per√≠odo testado.\")\n",
    "    else:\n",
    "        print(f\"\\n‚û°Ô∏è  Movimento lateral no per√≠odo.\")\n",
    "\n",
    "# 2. Comparar previs√µes vs realidade\n",
    "print(\"\\n\\nüìä 2. DISTRIBUI√á√ÉO DOS ERROS\")\n",
    "print(\"-\"*90)\n",
    "\n",
    "errors = y_true - y_p50\n",
    "errors_pct = (y_true - y_p50) / y_true * 100\n",
    "\n",
    "print(f\"Erro m√©dio: ${np.mean(errors):,.2f} ({np.mean(errors_pct):.2f}%)\")\n",
    "print(f\"Erro mediano: ${np.median(errors):,.2f} ({np.median(errors_pct):.2f}%)\")\n",
    "print(f\"Erro std: ${np.std(errors):,.2f}\")\n",
    "print(f\"Erro m√≠n: ${np.min(errors):,.2f} (modelo subestimou)\")\n",
    "print(f\"Erro m√°x: ${np.max(errors):,.2f} (modelo superestimou)\")\n",
    "\n",
    "# Percentual de vezes que errou para cima vs para baixo\n",
    "overestimated = np.sum(y_p50 > y_true) / len(y_true) * 100\n",
    "underestimated = np.sum(y_p50 < y_true) / len(y_true) * 100\n",
    "\n",
    "print(f\"\\nüìà Modelo previu acima do real: {overestimated:.1f}% das vezes\")\n",
    "print(f\"üìâ Modelo previu abaixo do real: {underestimated:.1f}% das vezes\")\n",
    "\n",
    "if overestimated > 70:\n",
    "    print(f\"   ‚ö†Ô∏è  VI√âS SISTEM√ÅTICO: Modelo √© muito otimista!\")\n",
    "elif underestimated > 70:\n",
    "    print(f\"   ‚ö†Ô∏è  VI√âS SISTEM√ÅTICO: Modelo √© muito pessimista!\")\n",
    "\n",
    "# 3. An√°lise de calibra√ß√£o por contexto\n",
    "print(\"\\n\\nüéØ 3. CALIBRA√á√ÉO EM PERSPECTIVA\")\n",
    "print(\"-\"*90)\n",
    "\n",
    "print(f\"Coverage 90% observado: {coverage_90:.1f}%\")\n",
    "print(f\"Coverage ideal: 90%\")\n",
    "print(f\"Diferen√ßa: {coverage_90 - 90:.1f} pontos percentuais\")\n",
    "\n",
    "if coverage_90 < 50:\n",
    "    print(f\"\\n‚ùå PROBLEMA S√âRIO: Faixas MUITO estreitas\")\n",
    "    print(f\"   ‚Ä¢ As previs√µes n√£o capturam a volatilidade real\")\n",
    "    print(f\"   ‚Ä¢ Confian√ßa excessiva nas estimativas\")\n",
    "    print(f\"   ‚Ä¢ Risco de decis√µes baseadas em intervalos irrealistas\")\n",
    "elif coverage_90 < 75:\n",
    "    print(f\"\\n‚ö†Ô∏è  PROBLEMA MODERADO: Faixas subestimadas\")\n",
    "    print(f\"   ‚Ä¢ Incerteza real √© maior que o modelo indica\")\n",
    "    print(f\"   ‚Ä¢ Requer ajuste na calibra√ß√£o\")\n",
    "elif coverage_90 < 85:\n",
    "    print(f\"\\n‚öôÔ∏è  Pequeno desvio: Calibra√ß√£o precisa de ajuste fino\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Calibra√ß√£o aceit√°vel (85-95%)\")\n",
    "\n",
    "# 4. Compara√ß√£o com benchmarks\n",
    "print(\"\\n\\nüìè 4. COMPARA√á√ÉO COM BENCHMARKS\")\n",
    "print(\"-\"*90)\n",
    "\n",
    "print(f\"MAPE: {mape:.2f}%\")\n",
    "if mape < 3:\n",
    "    print(f\"   ‚úÖ Excelente (< 3%)\")\n",
    "elif mape < 5:\n",
    "    print(f\"   ‚úÖ Bom (3-5%)\")\n",
    "elif mape < 10:\n",
    "    print(f\"   ‚ö†Ô∏è  Aceit√°vel (5-10%) - Pode melhorar\")\n",
    "else:\n",
    "    print(f\"   ‚ùå Ruim (> 10%) - Necessita revis√£o\")\n",
    "\n",
    "print(f\"\\nSharpness 90% CI: {sharpness_90:.2f}%\")\n",
    "if sharpness_90 < 5:\n",
    "    print(f\"   ‚ö†Ô∏è  Faixas muito estreitas (< 5%)\")\n",
    "    print(f\"   Risco: Falsa sensa√ß√£o de precis√£o\")\n",
    "elif sharpness_90 < 10:\n",
    "    print(f\"   ‚úÖ Razo√°vel (5-10%)\")\n",
    "elif sharpness_90 < 20:\n",
    "    print(f\"   ‚ö†Ô∏è  Faixas largas (10-20%)\")\n",
    "    print(f\"   Trade-off: Mais cautela, menos informativo\")\n",
    "else:\n",
    "    print(f\"   ‚ùå Faixas muito largas (> 20%)\")\n",
    "    print(f\"   Utilidade question√°vel\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è VEREDITO FINAL: √â Preocupante?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚öñÔ∏è VEREDITO FINAL: OS RESULTADOS S√ÉO PREOCUPANTES?\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "print(\"\\nüéØ RESUMO EXECUTIVO:\\n\")\n",
    "\n",
    "# Identificar problemas\n",
    "problems = []\n",
    "warnings = []\n",
    "positives = []\n",
    "\n",
    "# Avaliar cada aspecto\n",
    "if coverage_90 < 50:\n",
    "    problems.append(\"‚ùå Coverage MUITO baixo (38.5% vs 90% esperado)\")\n",
    "    problems.append(\"‚ùå Faixas de confian√ßa extremamente subestimadas\")\n",
    "elif coverage_90 < 75:\n",
    "    warnings.append(\"‚ö†Ô∏è  Coverage baixo - faixas subestimadas\")\n",
    "\n",
    "if overestimated > 90:\n",
    "    problems.append(f\"‚ùå Vi√©s sistem√°tico SEVERO ({overestimated:.0f}% prev√™ acima)\")\n",
    "elif overestimated > 70:\n",
    "    warnings.append(f\"‚ö†Ô∏è  Vi√©s sistem√°tico moderado ({overestimated:.0f}% prev√™ acima)\")\n",
    "\n",
    "if mape < 5:\n",
    "    positives.append(f\"‚úÖ MAPE excelente ({mape:.2f}%)\")\n",
    "elif mape < 10:\n",
    "    positives.append(f\"‚úÖ MAPE aceit√°vel ({mape:.2f}%)\")\n",
    "else:\n",
    "    problems.append(f\"‚ùå MAPE ruim ({mape:.2f}%)\")\n",
    "\n",
    "if 5 <= sharpness_90 <= 10:\n",
    "    positives.append(f\"‚úÖ Largura das faixas razo√°vel ({sharpness_90:.2f}%)\")\n",
    "\n",
    "# Mostrar resultados\n",
    "if problems:\n",
    "    print(\"üö® PROBLEMAS CR√çTICOS:\")\n",
    "    for p in problems:\n",
    "        print(f\"   {p}\")\n",
    "    print()\n",
    "\n",
    "if warnings:\n",
    "    print(\"‚ö†Ô∏è  PONTOS DE ATEN√á√ÉO:\")\n",
    "    for w in warnings:\n",
    "        print(f\"   {w}\")\n",
    "    print()\n",
    "\n",
    "if positives:\n",
    "    print(\"‚úÖ PONTOS POSITIVOS:\")\n",
    "    for p in positives:\n",
    "        print(f\"   {p}\")\n",
    "    print()\n",
    "\n",
    "# Veredito geral\n",
    "print(\"-\"*90)\n",
    "print(\"\\nüí° INTERPRETA√á√ÉO GERAL:\\n\")\n",
    "\n",
    "if len(problems) >= 2:\n",
    "    print(\"üî¥ SIM, OS RESULTADOS S√ÉO PREOCUPANTES\")\n",
    "    print()\n",
    "    print(\"   O modelo apresenta problemas significativos:\")\n",
    "    print()\n",
    "    print(\"   1Ô∏è‚É£  VI√âS OTIMISTA SISTEM√ÅTICO\")\n",
    "    print(\"      ‚Ä¢ 96% das previs√µes acima do real\")\n",
    "    print(\"      ‚Ä¢ Bias m√©dio de +$5,140 (4.6%)\")\n",
    "    print(\"      ‚Ä¢ Modelo 'espera' pre√ßos mais altos que a realidade\")\n",
    "    print()\n",
    "    print(\"   2Ô∏è‚É£  INTERVALO DE CONFIAN√áA MAL CALIBRADO\")\n",
    "    print(\"      ‚Ä¢ Coverage de 38.5% quando deveria ser 90%\")\n",
    "    print(\"      ‚Ä¢ Faixas muito estreitas (overconfident)\")\n",
    "    print(\"      ‚Ä¢ Subestima a incerteza real do mercado\")\n",
    "    print()\n",
    "    print(\"   ‚ö†Ô∏è  IMPLICA√á√ïES PR√ÅTICAS:\")\n",
    "    print(\"      ‚Ä¢ Trading: Estrat√©gias podem ser muito agressivas\")\n",
    "    print(\"      ‚Ä¢ Risk Management: Subestima√ß√£o de riscos\")\n",
    "    print(\"      ‚Ä¢ Stop-loss: Pode ser acionado com frequ√™ncia inesperada\")\n",
    "    print()\n",
    "else:\n",
    "    print(\"üü° RESULTADOS MISTOS - REQUER ATEN√á√ÉO\")\n",
    "    print()\n",
    "    print(\"   O modelo tem aspectos bons e ruins:\")\n",
    "    print(\"   ‚úÖ Erro m√©dio (MAPE) aceit√°vel\")\n",
    "    print(\"   ‚ùå Calibra√ß√£o dos intervalos precisa melhorar\")\n",
    "    print()\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print()\n",
    "print(\"üìã RECOMENDA√á√ïES:\")\n",
    "print()\n",
    "print(\"1Ô∏è‚É£  CURTO PRAZO (Usar modelo hoje):\")\n",
    "print(\"   ‚Ä¢ ‚ö†Ô∏è  Ajustar expectativas: Reduzir previs√µes em ~4-5%\")\n",
    "print(\"   ‚Ä¢ ‚ö†Ô∏è  Ampliar faixas: Multiplicar p05/p95 por fator 1.5-2.0\")\n",
    "print(\"   ‚Ä¢ ‚úÖ Usar MAPE como refer√™ncia de erro esperado\")\n",
    "print()\n",
    "print(\"2Ô∏è‚É£  M√âDIO PRAZO (Melhorias no modelo):\")\n",
    "print(\"   ‚Ä¢ üîß Recalibrar quantis (ajustar alpha/conformidade)\")\n",
    "print(\"   ‚Ä¢ üîß Investigar per√≠odo de treinamento (pode estar enviesado)\")\n",
    "print(\"   ‚Ä¢ üîß Adicionar features de volatilidade/regime de mercado\")\n",
    "print(\"   ‚Ä¢ üîß Testar com per√≠odos mais longos de backtest\")\n",
    "print()\n",
    "print(\"3Ô∏è‚É£  LONGO PRAZO (Valida√ß√£o):\")\n",
    "print(\"   ‚Ä¢ üìä Walk-forward backtest (re-treinar no passado)\")\n",
    "print(\"   ‚Ä¢ üìä Valida√ß√£o out-of-sample real (dados nunca vistos)\")\n",
    "print(\"   ‚Ä¢ üìä Comparar com benchmarks (naive, GARCH, etc)\")\n",
    "print()\n",
    "print(\"=\"*90)\n",
    "print()\n",
    "print(\"‚ö° NOTA IMPORTANTE SOBRE PSEUDO-BACKTEST:\")\n",
    "print()\n",
    "print(\"   Estes resultados t√™m LOOK-AHEAD BIAS (modelo viu os dados).\")\n",
    "print(\"   Em um backtest real (walk-forward), os resultados podem ser:\")\n",
    "print(\"   ‚Ä¢ üìâ PIORES: Se o modelo se beneficiou do look-ahead\")\n",
    "print(\"   ‚Ä¢ üìà MELHORES: Se o per√≠odo testado foi atipicamente dif√≠cil\")\n",
    "print()\n",
    "print(\"   Para conclus√µes definitivas, NECESS√ÅRIO fazer walk-forward backtest.\")\n",
    "print()\n",
    "print(\"=\"*90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Resposta Direta: Os Resultados S√£o Preocupantes?\n",
    "\n",
    "### üî¥ **SIM, s√£o preocupantes, mas com contexto importante:**\n",
    "\n",
    "#### ‚ùå Problemas Identificados:\n",
    "\n",
    "1. **Vi√©s Otimista Severo**: \n",
    "   - 96% das previs√µes s√£o mais altas que a realidade\n",
    "   - Bias m√©dio de +$5,140 (4.6%)\n",
    "   - Modelo sistematicamente \"otimista\"\n",
    "\n",
    "2. **Intervalos de Confian√ßa Mal Calibrados**:\n",
    "   - Coverage de apenas 38.5% quando deveria ser 90%\n",
    "   - Faixas muito estreitas (\"overconfident\")\n",
    "   - Subestima a incerteza real\n",
    "\n",
    "#### ‚úÖ Pontos Positivos:\n",
    "\n",
    "1. **Erro M√©dio Aceit√°vel**:\n",
    "   - MAPE de 4.68% √© considerado bom\n",
    "   - Modelo tem capacidade preditiva\n",
    "\n",
    "2. **Largura Razo√°vel**:\n",
    "   - 6.4% de largura est√° dentro do aceit√°vel\n",
    "   - O problema √© a calibra√ß√£o, n√£o a largura em si\n",
    "\n",
    "---\n",
    "\n",
    "### ü§î Mas Por Que Isso Aconteceu?\n",
    "\n",
    "Poss√≠veis causas:\n",
    "\n",
    "1. **Per√≠odo de Treinamento Otimista**:\n",
    "   - Modelo treinado em per√≠odo de alta\n",
    "   - \"Aprendeu\" a esperar pre√ßos crescentes\n",
    "\n",
    "2. **Conformal Prediction Needs Recalibration**:\n",
    "   - Alpha atual n√£o est√° capturando a distribui√ß√£o real\n",
    "   - Precisa ajustar par√¢metros de calibra√ß√£o\n",
    "\n",
    "3. **Look-Ahead Bias do Pseudo-Backtest**:\n",
    "   - Modelo viu os dados que est√° \"prevendo\"\n",
    "   - Resultados reais podem ser diferentes\n",
    "\n",
    "4. **Falta de Features de Regime**:\n",
    "   - Modelo n√£o detecta mudan√ßas de bull‚Üíbear\n",
    "   - Sempre assume regime similar ao treinamento\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ O Que Fazer?\n",
    "\n",
    "#### Solu√ß√£o Imediata (usar modelo hoje):\n",
    "\n",
    "```python\n",
    "# Ajustar previs√µes\n",
    "p50_ajustado = p50_original * 0.955  # Reduzir 4.5%\n",
    "\n",
    "# Ampliar faixas\n",
    "p05_ajustado = S0 + (p05_original - S0) * 1.7\n",
    "p95_ajustado = S0 + (p95_original - S0) * 1.7\n",
    "```\n",
    "\n",
    "#### Solu√ß√£o de M√©dio Prazo:\n",
    "\n",
    "1. **Recalibrar quantis** com conformal prediction ajustado\n",
    "2. **Adicionar features** de regime de mercado (bull/bear detection)\n",
    "3. **Testar m√∫ltiplos per√≠odos** de backtest\n",
    "4. **Implementar walk-forward** backtest verdadeiro\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è Conclus√£o:\n",
    "\n",
    "**Os resultados INDICAM problemas reais** que precisam ser endere√ßados, MAS:\n",
    "\n",
    "- ‚úÖ O modelo tem capacidade preditiva (MAPE bom)\n",
    "- ‚ö†Ô∏è A calibra√ß√£o precisa ser corrigida\n",
    "- üîç Pseudo-backtest tem limita√ß√µes (look-ahead bias)\n",
    "- üìä Necess√°rio valida√ß√£o adicional (walk-forward)\n",
    "\n",
    "**N√£o descarte o modelo, mas n√£o use sem ajustes!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
