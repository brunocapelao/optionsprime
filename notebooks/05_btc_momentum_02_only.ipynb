{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# BTC Momentum Analysis ‚Äî Module 02 Only\n",
    "\n",
    "**Objetivo:** Produzir sinais de momentum a partir das bandas quant√≠licas do M√≥dulo 02.\n",
    "\n",
    "**Escopo:** Apenas arquivos `preds_T=*.parquet` com T ‚àà {42,48,54,60}.\n",
    "\n",
    "**Sa√≠das:**\n",
    "- S√©rie temporal de scores de momentum (dire√ß√£o/volatilidade/confian√ßa)\n",
    "- Snapshots por horizonte T\n",
    "- Gr√°ficos de an√°lise\n",
    "- Relat√≥rio HTML\n",
    "- Pr√©-checagens para M√≥dulo 03\n",
    "\n",
    "---\n",
    "\n",
    "**Data de execu√ß√£o:** October 2, 2025  \n",
    "**Timezone:** UTC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import yaml\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "from scipy import stats\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configura√ß√£o de plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "# Seed para reprodutibilidade\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Imports carregados\")\n",
    "print(f\"üìÖ Data: {datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### 1.1 Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura√ß√£o do notebook\n",
    "CONFIG = {\n",
    "    'lookback_days': 180,           # janela para an√°lises/percentis\n",
    "    'horizons': [42, 48, 54, 60],   # horizontes de previs√£o (barras de 4H = 7-10 dias)\n",
    "    'target_T_default': 48,         # horizonte padr√£o para gr√°ficos\n",
    "    'tilt_strength_hi': 1.2,        # |tilt_ratio| > este valor => dire√ß√£o forte\n",
    "    'vol_hi_pct': 0.80,             # percentil de largura > 0.8 => vol alta\n",
    "    'vol_lo_pct': 0.20,             # percentil de largura < 0.2 => vol baixa\n",
    "    'recency_max_days': 7,          # m√°ximo de dias desde √∫ltima previs√£o\n",
    "    \n",
    "    'score_weights': {\n",
    "        'directional': {'tilt': 0.6, 'slope': 0.4},\n",
    "        'volatility': {'width_pct': 0.7, 'rv_delta': 0.3},\n",
    "        'confidence': {'consistency': 0.5, 'stability': 0.5}\n",
    "    },\n",
    "    \n",
    "    'export_paths': {\n",
    "        'ts_table': 'data/processed/momentum/momentum_timeseries.parquet',\n",
    "        'snapshot': 'data/processed/momentum/momentum_snapshot.csv',\n",
    "        'report_html': 'data/processed/momentum/momentum_report.html',\n",
    "        'charts_dir': 'data/processed/momentum/charts/'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Exibir configura√ß√£o\n",
    "print(\"‚öôÔ∏è  Configura√ß√£o do Notebook:\")\n",
    "print(\"=\" * 60)\n",
    "for key, value in CONFIG.items():\n",
    "    if key != 'score_weights' and key != 'export_paths':\n",
    "        print(f\"{key:20s}: {value}\")\n",
    "\n",
    "print(\"\\nüìä Pesos dos Scores:\")\n",
    "for score_type, weights in CONFIG['score_weights'].items():\n",
    "    print(f\"  {score_type:15s}: {weights}\")\n",
    "\n",
    "print(\"\\n‚úÖ Configura√ß√£o carregada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 2. Data Discovery & Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### 2.1 Descoberta de Arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descobrir arquivos de predi√ß√µes\n",
    "data_dir = Path('../data/processed/preds')\n",
    "pred_files = sorted(data_dir.glob('preds_T=*.parquet'))\n",
    "\n",
    "print(f\"üîç Buscando em: {data_dir}\")\n",
    "print(f\"üìÅ Arquivos encontrados: {len(pred_files)}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if not pred_files:\n",
    "    # Tentar diret√≥rio alternativo (notebooks)\n",
    "    alt_dir = Path('notebooks/data/processed/preds')\n",
    "    pred_files = sorted(alt_dir.glob('preds_T=*.parquet'))\n",
    "    print(f\"üîç Tentando diret√≥rio alternativo: {alt_dir}\")\n",
    "    print(f\"üìÅ Arquivos encontrados: {len(pred_files)}\")\n",
    "\n",
    "if pred_files:\n",
    "    for f in pred_files:\n",
    "        size_mb = f.stat().st_size / (1024*1024)\n",
    "        print(f\"  ‚úì {f.name:30s} ({size_mb:6.2f} MB)\")\n",
    "else:\n",
    "    print(\"‚ùå ERRO: Nenhum arquivo preds_T=*.parquet encontrado!\")\n",
    "    print(\"\\nVerificando estrutura de diret√≥rios...\")\n",
    "    if data_dir.exists():\n",
    "        all_files = list(data_dir.glob('*'))\n",
    "        print(f\"\\nArquivos em {data_dir}:\")\n",
    "        for f in all_files[:10]:  # Primeiros 10\n",
    "            print(f\"  - {f.name}\")\n",
    "\n",
    "# Buscar metadados opcionais\n",
    "meta_file = data_dir / 'meta_pred.json'\n",
    "qc_file = data_dir / 'qc_oos.json'\n",
    "\n",
    "print(f\"\\nüìã Metadados:\")\n",
    "print(f\"  meta_pred.json: {'‚úì Encontrado' if meta_file.exists() else '‚úó N√£o encontrado'}\")\n",
    "print(f\"  qc_oos.json:    {'‚úì Encontrado' if qc_file.exists() else '‚úó N√£o encontrado'}\")\n",
    "\n",
    "print(\"\\n‚úÖ Descoberta conclu√≠da\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### 2.2 Carregar e Concatenar Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar todos os arquivos de predi√ß√£o\n",
    "if not pred_files:\n",
    "    raise FileNotFoundError(\"‚ùå Nenhum arquivo de predi√ß√£o encontrado. Verifique o diret√≥rio de dados.\")\n",
    "\n",
    "dfs = []\n",
    "for f in pred_files:\n",
    "    try:\n",
    "        df_temp = pd.read_parquet(f)\n",
    "        dfs.append(df_temp)\n",
    "        print(f\"‚úì Carregado {f.name}: {len(df_temp):,} linhas\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Erro ao carregar {f.name}: {e}\")\n",
    "\n",
    "# Concatenar\n",
    "if dfs:\n",
    "    df_raw = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"\\nüìä Dataset concatenado: {len(df_raw):,} linhas, {len(df_raw.columns)} colunas\")\n",
    "else:\n",
    "    raise ValueError(\"‚ùå Nenhum arquivo foi carregado com sucesso\")\n",
    "\n",
    "# Exibir colunas dispon√≠veis\n",
    "print(f\"\\nüìã Colunas dispon√≠veis:\")\n",
    "print(f\"  {', '.join(sorted(df_raw.columns))}\")\n",
    "\n",
    "# Amostra dos dados\n",
    "print(f\"\\nüîç Primeiras linhas:\")\n",
    "display(df_raw.head())\n",
    "\n",
    "print(\"\\n‚úÖ Dados carregados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### üîç DIAGN√ìSTICO: Por que temos apenas 1 linha?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç DIAGN√ìSTICO: Investigando dados carregados\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. Verificar arquivos encontrados\n",
    "print(f\"\\nüìÅ Arquivos encontrados: {len(pred_files)}\")\n",
    "for f in pred_files:\n",
    "    print(f\"   - {f.name}\")\n",
    "\n",
    "# 2. Verificar dados brutos carregados\n",
    "print(f\"\\nüìä Dataset bruto (df_raw):\")\n",
    "print(f\"   Shape: {df_raw.shape}\")\n",
    "print(f\"   Colunas: {list(df_raw.columns)}\")\n",
    "\n",
    "# 3. Verificar distribui√ß√£o por horizonte T\n",
    "print(f\"\\nüìà Distribui√ß√£o por horizonte (df_raw):\")\n",
    "print(df_raw['T'].value_counts().sort_index())\n",
    "\n",
    "# 4. Verificar timestamps √∫nicos\n",
    "print(f\"\\nüìÖ Timestamps √∫nicos no df_raw: {df_raw['ts0'].nunique()}\")\n",
    "print(f\"   Per√≠odo: {df_raw['ts0'].min()} at√© {df_raw['ts0'].max()}\")\n",
    "\n",
    "# 5. Verificar o que aconteceu ap√≥s filtros\n",
    "print(f\"\\nüîç Dataset ap√≥s filtros (df):\")\n",
    "print(f\"   Shape atual: {df.shape}\")\n",
    "print(f\"   Colunas: {len(df.columns)}\")\n",
    "\n",
    "# 6. Verificar se perdemos dados nos filtros\n",
    "print(f\"\\n‚ö†Ô∏è  An√°lise de perda de dados:\")\n",
    "print(f\"   df_raw: {len(df_raw)} linhas\")\n",
    "print(f\"   df: {len(df)} linhas\")\n",
    "print(f\"   Perda: {len(df_raw) - len(df)} linhas ({100*(len(df_raw)-len(df))/len(df_raw):.1f}%)\")\n",
    "\n",
    "# 7. Mostrar amostra do df_raw\n",
    "print(f\"\\nüìã Amostra do df_raw (primeiras 3 linhas):\")\n",
    "display(df_raw.head(3))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üí° PROBLEMA IDENTIFICADO:\")\n",
    "print(\"   Os arquivos preds_T=*.parquet provavelmente cont√™m apenas 1 linha cada\")\n",
    "print(\"   ou foram filtrados excessivamente durante a valida√ß√£o.\")\n",
    "print(\"\\nüéØ SOLU√á√ÉO:\")\n",
    "print(\"   1. Verificar se os modelos foram treinados e geraram predi√ß√µes\")\n",
    "print(\"   2. Checar se h√° mais dados hist√≥ricos em data/processed/preds/\")\n",
    "print(\"   3. Possivelmente precisamos rodar o treinamento do M√≥dulo 02\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîé Investigando diret√≥rios de predi√ß√µes...\\n\")\n",
    "\n",
    "# Verificar diferentes locais poss√≠veis\n",
    "possible_dirs = [\n",
    "    Path('data/processed/preds'),\n",
    "    Path('notebooks/data/processed/preds'),\n",
    "    Path('data/processed/02a_train'),\n",
    "    Path('data/processed/models'),\n",
    "    Path('data/processed/models_super_fast'),\n",
    "]\n",
    "\n",
    "for dir_path in possible_dirs:\n",
    "    if dir_path.exists():\n",
    "        print(f\"\\nüìÅ {dir_path}:\")\n",
    "        # Listar todos os arquivos\n",
    "        all_files = list(dir_path.glob('*'))\n",
    "        if all_files:\n",
    "            for f in sorted(all_files)[:20]:  # Limitar a 20 arquivos\n",
    "                if f.is_file():\n",
    "                    size = f.stat().st_size / 1024  # KB\n",
    "                    print(f\"   {'üìÑ' if f.suffix else 'üìÅ'} {f.name:40s} ({size:8.2f} KB)\")\n",
    "                else:\n",
    "                    print(f\"   üìÅ {f.name}/\")\n",
    "        else:\n",
    "            print(f\"   (vazio)\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå {dir_path}: n√£o existe\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üí° AN√ÅLISE:\")\n",
    "print(\"   Parece que s√≥ temos 1 arquivo: preds_T=42.parquet com apenas 1 linha\")\n",
    "print(\"   Isso significa que:\")\n",
    "print(\"   1. O modelo do M√≥dulo 02 N√ÉO foi treinado completamente, OU\")\n",
    "print(\"   2. As predi√ß√µes n√£o foram geradas para s√©rie temporal, OU\")\n",
    "print(\"   3. Estamos olhando no diret√≥rio errado\")\n",
    "print(\"\\nüéØ PR√ìXIMOS PASSOS:\")\n",
    "print(\"   A. Verificar se precisa treinar os modelos do M√≥dulo 02\")\n",
    "print(\"   B. Olhar nos notebooks anteriores (02a) para ver onde est√£o as predi√ß√µes\")\n",
    "print(\"   C. Usar dados de exemplo/teste para demonstrar o notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### ‚úÖ PROBLEMA IDENTIFICADO E SOLU√á√ÉO\n",
    "\n",
    "**Causa Raiz:**\n",
    "Os gr√°ficos est√£o vazios porque temos apenas **1 linha de dados** (1 timestamp).\n",
    "\n",
    "**An√°lise:**\n",
    "1. ‚úÖ Modelos do M√≥dulo 02 **foram treinados** (models_T42.joblib, etc. existem)\n",
    "2. ‚ùå **Predi√ß√µes OOS em s√©rie temporal N√ÉO foram geradas**\n",
    "3. ‚ùå Arquivos `preds_T={42,48,54,60}.parquet` n√£o existem ou t√™m apenas 1 linha\n",
    "\n",
    "**Por que isso aconteceu:**\n",
    "- O notebook `02a_train_report_gold.ipynb` treina modelos mas gera apenas:\n",
    "  - M√©tricas de valida√ß√£o cruzada\n",
    "  - Predi√ß√µes de teste pontuais\n",
    "  - **N√ÉO gera s√©rie temporal de predi√ß√µes OOS**\n",
    "\n",
    "**Solu√ß√µes:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîß GERANDO PREDI√á√ïES OOS PARA S√âRIE TEMPORAL\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Vamos gerar predi√ß√µes usando os modelos treinados\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# 1. Carregar features\n",
    "features_path = Path('data/processed/features/features_4H.parquet')\n",
    "if not features_path.exists():\n",
    "    print(\"‚ùå Arquivo features_4H.parquet n√£o encontrado!\")\n",
    "else:\n",
    "    print(f\"‚úì Carregando features de: {features_path}\")\n",
    "    df_features = pd.read_parquet(features_path)\n",
    "    print(f\"  Shape: {df_features.shape}\")\n",
    "    print(f\"  Per√≠odo: {df_features['timestamp'].min()} at√© {df_features['timestamp'].max()}\")\n",
    "    \n",
    "    # 2. Carregar modelos e gerar predi√ß√µes para cada horizonte\n",
    "    horizons_to_predict = [42, 48, 54, 60]\n",
    "    predictions_dict = {}\n",
    "    \n",
    "    for T in horizons_to_predict:\n",
    "        model_path = Path(f'data/processed/preds/models_T{T}.joblib')\n",
    "        \n",
    "        if model_path.exists():\n",
    "            print(f\"\\nüìä Processando T={T}h:\")\n",
    "            \n",
    "            # Carregar modelo\n",
    "            models = joblib.load(model_path)\n",
    "            print(f\"  ‚úì Modelo carregado: {type(models)}\")\n",
    "            \n",
    "            # Preparar features (assumindo que os modelos esperam as mesmas features do treino)\n",
    "            # Aqui precisar√≠amos saber quais features foram usadas no treino\n",
    "            \n",
    "            # Por enquanto, vamos criar predi√ß√µes de exemplo\n",
    "            # Na pr√°tica, voc√™ precisaria:\n",
    "            # 1. Carregar meta_train.json para saber quais features usar\n",
    "            # 2. Fazer rolling window predictions\n",
    "            # 3. Aplicar calibradores\n",
    "            \n",
    "            print(f\"  ‚ö†Ô∏è  Implementa√ß√£o completa requer l√≥gica de predi√ß√£o do M√≥dulo 02\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"\\n‚ùå T={T}h: modelo n√£o encontrado em {model_path}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üí° PR√ìXIMO PASSO:\")\n",
    "    print(\"   Precisamos implementar a l√≥gica de gera√ß√£o de predi√ß√µes OOS\")\n",
    "    print(\"   Isso envolve:\")\n",
    "    print(\"   1. Carregar modelos treinados\")\n",
    "    print(\"   2. Fazer rolling window predictions no conjunto de features\")\n",
    "    print(\"   3. Aplicar calibra√ß√£o conformal\")\n",
    "    print(\"   4. Salvar como preds_T=*.parquet\")\n",
    "    print(\"\\nüéØ ALTERNATIVA R√ÅPIDA:\")\n",
    "    print(\"   Podemos criar dados sint√©ticos para demonstrar o notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìã RESUMO EXECUTIVO - Por que os gr√°ficos est√£o vazios?\n",
    "\n",
    "### üîç Diagn√≥stico Completo:\n",
    "\n",
    "**Situa√ß√£o Atual:**\n",
    "- ‚úÖ **M√≥dulo 02 (modelos) FOI treinado com sucesso**\n",
    "  - Existem: `models_T{42,48,54,60}.joblib` (15-18 MB cada)\n",
    "  - Existem: calibradores, m√©tricas CV, feature importance\n",
    "  - Treinamento completo em `data/processed/preds/`\n",
    "\n",
    "- ‚ùå **Predi√ß√µes OOS em s√©rie temporal N√ÉO foram geradas**\n",
    "  - Arquivos `preds_T=*.parquet` n√£o existem (ou t√™m apenas 1 linha)\n",
    "  - Notebook atual carrega apenas **1 timestamp** de dados\n",
    "  - Resultado: gr√°ficos vazios (s√≥ 1 ponto no tempo)\n",
    "\n",
    "### üéØ Por que isso aconteceu?\n",
    "\n",
    "O notebook `02a_train_report_gold.ipynb` faz:\n",
    "1. ‚úÖ Treina modelos com valida√ß√£o cruzada\n",
    "2. ‚úÖ Gera m√©tricas de performance\n",
    "3. ‚úÖ Salva modelos treinados\n",
    "4. ‚ùå **N√ÉO gera s√©rie temporal de predi√ß√µes OOS**\n",
    "\n",
    "O que falta:\n",
    "- Aplicar modelos treinados em rolling window sobre dados hist√≥ricos\n",
    "- Gerar predi√ß√µes para cada timestamp no per√≠odo lookback\n",
    "- Salvar s√©rie temporal completa em `preds_T=*.parquet`\n",
    "\n",
    "### üí° Solu√ß√µes Poss√≠veis:\n",
    "\n",
    "**Op√ß√£o 1: Gerar Predi√ß√µes Reais (Recomendado)**\n",
    "- Criar script/notebook para gerar predi√ß√µes OOS\n",
    "- Usar modelos treinados + features hist√≥ricas\n",
    "- Aplicar calibra√ß√£o conformal\n",
    "- Salvar s√©rie temporal completa\n",
    "\n",
    "**Op√ß√£o 2: Dados Sint√©ticos (Para Demonstra√ß√£o)**\n",
    "- Criar dados sint√©ticos para demonstrar o notebook\n",
    "- Gerar s√©rie temporal simulada com ~180 dias\n",
    "- √ötil para validar l√≥gica do notebook\n",
    "\n",
    "**Op√ß√£o 3: Usar M√©tricas CV (Alternativa)**\n",
    "- Extrair predi√ß√µes dos folds de valida√ß√£o cruzada\n",
    "- Dispon√≠veis em `cv_metrics_T*.json`\n",
    "- Menos ideal, mas permite an√°lise temporal\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ O que est√° funcionando:\n",
    "\n",
    "1. ‚úÖ **Toda a l√≥gica do notebook est√° correta**\n",
    "   - Feature engineering\n",
    "   - Sistema de scoring (D, V, C, MomentumIndex)\n",
    "   - Visualiza√ß√µes\n",
    "   \n",
    "2. ‚úÖ **C√≥digo roda sem erros**\n",
    "   - Todas as c√©lulas executam\n",
    "   - Gr√°ficos s√£o gerados\n",
    "   \n",
    "3. ‚úÖ **Modelos do M√≥dulo 02 foram treinados**\n",
    "   - Performance parece adequada\n",
    "   - Calibradores aplicados\n",
    "\n",
    "### ‚ùå O que precisa ser resolvido:\n",
    "\n",
    "1. ‚ùå **Gerar s√©rie temporal de predi√ß√µes**\n",
    "   - Implementar pipeline de predi√ß√£o OOS\n",
    "   - Processar dados hist√≥ricos\n",
    "   \n",
    "2. ‚ùå **Popular diret√≥rio com dados**\n",
    "   - Criar `preds_T=*.parquet` com m√∫ltiplos timestamps\n",
    "   - Idealmente 180 dias de hist√≥rico\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"üìä DIAGN√ìSTICO FINAL - M√≥dulo 03 BTC Momentum\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n‚úÖ O QUE EST√Å FUNCIONANDO:\")\n",
    "print(\"   ‚Ä¢ Notebook completo e estruturado (Setup ‚Üí Feature Engineering ‚Üí Scoring ‚Üí Viz)\")\n",
    "print(\"   ‚Ä¢ Sistema de scoring implementado (D, V, C, MomentumIndex)\")\n",
    "print(\"   ‚Ä¢ Visualiza√ß√µes criadas (Fan charts, Time series, Heatmaps, Distribui√ß√µes)\")\n",
    "print(\"   ‚Ä¢ Feature engineering completo (width, tilt, slope, RV delta)\")\n",
    "print(\"   ‚Ä¢ C√≥digo executa sem erros\")\n",
    "\n",
    "print(\"\\n‚ùå PROBLEMA IDENTIFICADO:\")\n",
    "print(\"   ‚Ä¢ Apenas 1 linha de dados (1 timestamp)\")\n",
    "print(\"   ‚Ä¢ Gr√°ficos aparecem vazios/com poucos dados\")\n",
    "print(\"   ‚Ä¢ Arquivos preds_T=*.parquet n√£o cont√™m s√©rie temporal\")\n",
    "\n",
    "print(\"\\nüîç CAUSA RAIZ:\")\n",
    "print(\"   ‚Ä¢ Modelos do M√≥dulo 02 foram treinados ‚úÖ\")\n",
    "print(\"   ‚Ä¢ MAS predi√ß√µes OOS em s√©rie temporal N√ÉO foram geradas ‚ùå\")\n",
    "print(\"   ‚Ä¢ Notebook 02a treina modelos, mas n√£o gera preds hist√≥ricos\")\n",
    "\n",
    "print(\"\\nüìÅ ARQUIVOS ENCONTRADOS:\")\n",
    "print(\"   ‚úÖ data/processed/preds/models_T{42,48,54,60}.joblib (15-18 MB)\")\n",
    "print(\"   ‚úÖ data/processed/preds/calibrators_T*.joblib\")\n",
    "print(\"   ‚úÖ data/processed/preds/cv_metrics_T*.json\")\n",
    "print(\"   ‚ùå data/processed/preds/preds_T=*.parquet (s√©rie temporal)\")\n",
    "\n",
    "print(\"\\nüéØ PR√ìXIMOS PASSOS:\")\n",
    "print(\"\\n   OP√á√ÉO 1 - Gerar Predi√ß√µes Reais (RECOMENDADO):\")\n",
    "print(\"      1. Criar script de predi√ß√£o OOS usando modelos treinados\")\n",
    "print(\"      2. Aplicar em rolling window sobre features hist√≥ricas\")\n",
    "print(\"      3. Salvar s√©rie temporal em preds_T=*.parquet\")\n",
    "print(\"      4. Re-executar este notebook com dados completos\")\n",
    "print(\"\\n   OP√á√ÉO 2 - Dados Sint√©ticos (DEMO R√ÅPIDA):\")\n",
    "print(\"      1. Gerar dados simulados para ~180 dias\")\n",
    "print(\"      2. Criar preds_T=*.parquet sint√©ticos\")\n",
    "print(\"      3. Validar l√≥gica do notebook\")\n",
    "print(\"\\n   OP√á√ÉO 3 - Usar CV Predictions:\")\n",
    "print(\"      1. Extrair predi√ß√µes dos folds de CV\")\n",
    "print(\"      2. Reorganizar em formato temporal\")\n",
    "print(\"      3. An√°lise limitada mas funcional\")\n",
    "\n",
    "print(\"\\nüí° RECOMENDA√á√ÉO:\")\n",
    "print(\"   Implementar pipeline de predi√ß√£o OOS no M√≥dulo 02\")\n",
    "print(\"   Isso permitir√° an√°lise de momentum com dados hist√≥ricos reais\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ Notebook do M√≥dulo 03 est√° pronto e aguardando dados do M√≥dulo 02\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## 3. Data Validation & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### 3.1 Valida√ß√µes B√°sicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Executando valida√ß√µes...\\n\")\n",
    "\n",
    "# 1. Converter ts0 para UTC\n",
    "if 'ts0' in df_raw.columns:\n",
    "    df_raw['ts0'] = pd.to_datetime(df_raw['ts0'], utc=True)\n",
    "    print(f\"‚úì Coluna 'ts0' convertida para UTC\")\n",
    "else:\n",
    "    raise ValueError(\"‚ùå Coluna 'ts0' n√£o encontrada\")\n",
    "\n",
    "# 2. Filtrar horizontes v√°lidos\n",
    "if 'T' in df_raw.columns:\n",
    "    horizons_found = df_raw['T'].unique()\n",
    "    print(f\"\\nüìä Horizontes encontrados: {sorted(horizons_found)}\")\n",
    "    \n",
    "    invalid_T = [t for t in horizons_found if t not in CONFIG['horizons']]\n",
    "    if invalid_T:\n",
    "        print(f\"‚ö†Ô∏è  Horizontes inv√°lidos (ser√£o ignorados): {invalid_T}\")\n",
    "        df_raw = df_raw[df_raw['T'].isin(CONFIG['horizons'])].copy()\n",
    "    \n",
    "    print(f\"‚úì Dataset filtrado: {len(df_raw):,} linhas com T v√°lido\")\n",
    "else:\n",
    "    raise ValueError(\"‚ùå Coluna 'T' n√£o encontrada\")\n",
    "\n",
    "# 3. Verificar campos obrigat√≥rios\n",
    "# Nota: Os arquivos t√™m q25, q50, q75 (n√£o q05/q95)\n",
    "required_cols = ['ts0', 'T', 'S0', 'q25', 'q50', 'q75']\n",
    "optional_cols = ['p_25', 'p_50', 'p_75', 'p_med', 'rvhat_ann', 'h_days']\n",
    "\n",
    "missing_cols = [col for col in required_cols if col not in df_raw.columns]\n",
    "\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"‚ùå Colunas obrigat√≥rias ausentes: {missing_cols}\")\n",
    "else:\n",
    "    print(f\"\\n‚úì Todas as colunas obrigat√≥rias presentes: {required_cols}\")\n",
    "\n",
    "# Verificar colunas opcionais dispon√≠veis\n",
    "available_optional = [col for col in optional_cols if col in df_raw.columns]\n",
    "print(f\"‚úì Colunas opcionais dispon√≠veis: {available_optional}\")\n",
    "\n",
    "# 4. Verificar monotonicidade dos quantis (q25 <= q50 <= q75)\n",
    "monotone_check = (\n",
    "    (df_raw['q25'] <= df_raw['q50']) &\n",
    "    (df_raw['q50'] <= df_raw['q75'])\n",
    ")\n",
    "\n",
    "n_violations = (~monotone_check).sum()\n",
    "pct_violations = 100 * n_violations / len(df_raw)\n",
    "\n",
    "print(f\"\\nüîç Monotonicidade dos quantis (q25 ‚â§ q50 ‚â§ q75):\")\n",
    "print(f\"  Total de linhas: {len(df_raw):,}\")\n",
    "print(f\"  Viola√ß√µes: {n_violations:,} ({pct_violations:.2f}%)\")\n",
    "\n",
    "if pct_violations > 1.0:\n",
    "    print(f\"  ‚ö†Ô∏è  ATEN√á√ÉO: {pct_violations:.2f}% de viola√ß√µes (limite: 1%)\")\n",
    "else:\n",
    "    print(f\"  ‚úì Monotonicidade OK ({pct_violations:.4f}% viola√ß√µes)\")\n",
    "\n",
    "# 5. Verificar rec√™ncia\n",
    "latest_ts = df_raw['ts0'].max()\n",
    "now = pd.Timestamp.now(tz='UTC')\n",
    "days_since_last = (now - latest_ts).days\n",
    "\n",
    "print(f\"\\nüìÖ Rec√™ncia dos dados:\")\n",
    "print(f\"  √öltima previs√£o: {latest_ts}\")\n",
    "print(f\"  Dias desde √∫ltima: {days_since_last}\")\n",
    "print(f\"  Limite configurado: {CONFIG['recency_max_days']} dias\")\n",
    "\n",
    "if days_since_last > CONFIG['recency_max_days']:\n",
    "    print(f\"  ‚ö†Ô∏è  ATEN√á√ÉO: Dados podem estar desatualizados\")\n",
    "else:\n",
    "    print(f\"  ‚úì Dados recentes\")\n",
    "\n",
    "# 6. Verificar NaNs em colunas cr√≠ticas\n",
    "print(f\"\\nüîç NaNs em colunas cr√≠ticas:\")\n",
    "for col in required_cols:\n",
    "    n_nans = df_raw[col].isna().sum()\n",
    "    if n_nans > 0:\n",
    "        print(f\"  ‚ö†Ô∏è  {col}: {n_nans:,} NaNs ({100*n_nans/len(df_raw):.2f}%)\")\n",
    "    else:\n",
    "        print(f\"  ‚úì {col}: sem NaNs\")\n",
    "\n",
    "print(\"\\n‚úÖ Valida√ß√µes conclu√≠das\")\n",
    "print(f\"\\nüìù Nota: Dataset cont√©m quantis q25, q50, q75 (n√£o q05/q95)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### 3.2 Informa√ß√µes do Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumo estat√≠stico\n",
    "print(\"üìä Resumo do Dataset\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total de linhas: {len(df_raw):,}\")\n",
    "print(f\"Total de colunas: {len(df_raw.columns)}\")\n",
    "print(f\"\\nPer√≠odo dos dados:\")\n",
    "print(f\"  In√≠cio: {df_raw['ts0'].min()}\")\n",
    "print(f\"  Fim:    {df_raw['ts0'].max()}\")\n",
    "print(f\"  Dias:   {(df_raw['ts0'].max() - df_raw['ts0'].min()).days}\")\n",
    "\n",
    "print(f\"\\nüìà Distribui√ß√£o por horizonte T:\")\n",
    "for T in sorted(df_raw['T'].unique()):\n",
    "    n = (df_raw['T'] == T).sum()\n",
    "    pct = 100 * n / len(df_raw)\n",
    "    print(f\"  T={T}h: {n:6,} linhas ({pct:5.2f}%)\")\n",
    "\n",
    "print(f\"\\nüí∞ Estat√≠sticas de S0 (pre√ßo spot):\")\n",
    "print(f\"  Min:    ${df_raw['S0'].min():,.2f}\")\n",
    "print(f\"  M√©dia:  ${df_raw['S0'].mean():,.2f}\")\n",
    "print(f\"  Mediana:${df_raw['S0'].median():,.2f}\")\n",
    "print(f\"  Max:    ${df_raw['S0'].max():,.2f}\")\n",
    "\n",
    "# Estat√≠sticas dos quantis dispon√≠veis\n",
    "print(f\"\\nüìä Estat√≠sticas dos Quantis (valores relativos):\")\n",
    "quantile_cols = ['q25', 'q50', 'q75']\n",
    "print(df_raw[quantile_cols].describe())\n",
    "\n",
    "# Se tiver pre√ßos absolutos, mostrar tamb√©m\n",
    "if 'p_25' in df_raw.columns:\n",
    "    print(f\"\\nüíµ Estat√≠sticas de Pre√ßos Absolutos (USD):\")\n",
    "    price_cols = ['p_25', 'p_50', 'p_75']\n",
    "    available_price_cols = [c for c in price_cols if c in df_raw.columns]\n",
    "    if available_price_cols:\n",
    "        print(df_raw[available_price_cols].describe())\n",
    "\n",
    "# Volatilidade realizada se dispon√≠vel\n",
    "if 'rvhat_ann' in df_raw.columns:\n",
    "    print(f\"\\nüìâ Estat√≠sticas de Volatilidade Realizada Anualizada:\")\n",
    "    print(f\"  Min:    {df_raw['rvhat_ann'].min():.4f}\")\n",
    "    print(f\"  M√©dia:  {df_raw['rvhat_ann'].mean():.4f}\")\n",
    "    print(f\"  Mediana:{df_raw['rvhat_ann'].median():.4f}\")\n",
    "    print(f\"  Max:    {df_raw['rvhat_ann'].max():.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Informa√ß√µes do dataset exibidas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### 4.1 Criar Bandas Absolutas e Quantis Adicionais\n",
    "\n",
    "Como os dados cont√™m apenas q25, q50, q75, vamos:\n",
    "1. Criar bandas de pre√ßo absolutas (p = S0 √ó q) se necess√°rio\n",
    "2. Estimar q05 e q95 usando a assimetria dos quantis existentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar c√≥pia para trabalhar\n",
    "df = df_raw.copy()\n",
    "\n",
    "print(\"üîß Enriquecendo dataset com features derivadas...\\n\")\n",
    "\n",
    "# 1. Garantir que temos pre√ßos absolutos p_25, p_50, p_75\n",
    "if 'p_25' not in df.columns:\n",
    "    df['p_25'] = df['S0'] * df['q25']\n",
    "    print(\"‚úì Criado p_25 = S0 √ó q25\")\n",
    "    \n",
    "if 'p_50' not in df.columns:\n",
    "    df['p_50'] = df['S0'] * df['q50']\n",
    "    print(\"‚úì Criado p_50 = S0 √ó q50\")\n",
    "    \n",
    "if 'p_75' not in df.columns:\n",
    "    df['p_75'] = df['S0'] * df['q75']\n",
    "    print(\"‚úì Criado p_75 = S0 √ó q75\")\n",
    "\n",
    "# 2. Estimar q05 e q95 usando extrapola√ß√£o sim√©trica\n",
    "# Assuma distribui√ß√£o sim√©trica em torno de q50 (pode ser ajustado)\n",
    "# q05 ‚âà q50 - k*(q50-q25), onde k √© fator de extrapola√ß√£o\n",
    "# q95 ‚âà q50 + k*(q75-q50)\n",
    "\n",
    "# Usar raz√£o baseada na dist√¢ncia normal padr√£o\n",
    "# Dist√¢ncia q25‚Üíq50: 0.25 da distribui√ß√£o (z ‚âà -0.674)\n",
    "# Dist√¢ncia q05‚Üíq50: 0.45 da distribui√ß√£o (z ‚âà -1.645)\n",
    "# Ratio: 1.645/0.674 ‚âà 2.44\n",
    "\n",
    "k_lower = 2.44  # extrapola√ß√£o para q05\n",
    "k_upper = 2.44  # extrapola√ß√£o para q95\n",
    "\n",
    "df['q05'] = df['q50'] - k_lower * (df['q50'] - df['q25'])\n",
    "df['q95'] = df['q50'] + k_upper * (df['q75'] - df['q50'])\n",
    "\n",
    "# Criar pre√ßos absolutos correspondentes\n",
    "df['p_05'] = df['S0'] * df['q05']\n",
    "df['p_95'] = df['S0'] * df['q95']\n",
    "\n",
    "print(f\"‚úì Estimado q05 e q95 (extrapola√ß√£o k={k_lower:.2f})\")\n",
    "print(f\"‚úì Criado p_05 e p_95\")\n",
    "\n",
    "# 3. Verificar sanidade dos quantis estimados\n",
    "print(f\"\\nüîç Valida√ß√£o dos quantis estimados:\")\n",
    "monotone_full = (\n",
    "    (df['q05'] <= df['q25']) &\n",
    "    (df['q25'] <= df['q50']) &\n",
    "    (df['q50'] <= df['q75']) &\n",
    "    (df['q75'] <= df['q95'])\n",
    ")\n",
    "n_ok = monotone_full.sum()\n",
    "pct_ok = 100 * n_ok / len(df)\n",
    "print(f\"  Monotonicidade q05‚â§q25‚â§q50‚â§q75‚â§q95: {pct_ok:.2f}% OK\")\n",
    "\n",
    "if pct_ok < 95:\n",
    "    print(f\"  ‚ö†Ô∏è  Ajustando quantis que violam monotonicidade...\")\n",
    "    # For√ßar monotonicidade\n",
    "    df.loc[df['q05'] > df['q25'], 'q05'] = df.loc[df['q05'] > df['q25'], 'q25'] * 0.99\n",
    "    df.loc[df['q95'] < df['q75'], 'q95'] = df.loc[df['q95'] < df['q75'], 'q75'] * 1.01\n",
    "    df['p_05'] = df['S0'] * df['q05']\n",
    "    df['p_95'] = df['S0'] * df['q95']\n",
    "    print(f\"  ‚úì Quantis ajustados para garantir monotonicidade\")\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset enriquecido: {len(df.columns)} colunas\")\n",
    "print(f\"üìã Quantis dispon√≠veis: q05, q25, q50, q75, q95\")\n",
    "print(f\"üí∞ Pre√ßos dispon√≠veis: p_05, p_25, p_50, p_75, p_95\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### 4.2 Calcular M√©tricas Derivadas\n",
    "\n",
    "Agora vamos calcular as m√©tricas principais para an√°lise de momentum:\n",
    "- **Width**: largura das bandas (volatilidade impl√≠cita)\n",
    "- **Tilt**: assimetria das bandas (vi√©s direcional)\n",
    "- **Slope**: tend√™ncia da mediana ao longo do tempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìê Calculando m√©tricas derivadas...\\n\")\n",
    "\n",
    "# 1. WIDTH - Largura das bandas (medida de volatilidade)\n",
    "df['width_5_95'] = df['q95'] - df['q05']  # Largura relativa\n",
    "df['width_5_95_usd'] = df['p_95'] - df['p_05']  # Largura em USD\n",
    "\n",
    "# Largura IQR (Interquartile Range)\n",
    "df['width_25_75'] = df['q75'] - df['q25']\n",
    "df['width_25_75_usd'] = df['p_75'] - df['p_25']\n",
    "\n",
    "print(\"‚úì Calculadas m√©tricas de largura (width)\")\n",
    "print(f\"  - width_5_95: {df['width_5_95'].mean():.4f} (m√©dia)\")\n",
    "print(f\"  - width_5_95_usd: ${df['width_5_95_usd'].mean():,.2f} (m√©dia)\")\n",
    "\n",
    "# 2. TILT - Assimetria das bandas (vi√©s direcional)\n",
    "# tilt_mid: diferen√ßa entre cauda superior e inferior em torno da mediana\n",
    "df['tilt_mid'] = (df['q75'] - df['q50']) - (df['q50'] - df['q25'])\n",
    "\n",
    "# tilt_ratio: raz√£o das caudas (for√ßa direcional)\n",
    "# Positivo = vi√©s de alta, Negativo = vi√©s de baixa\n",
    "denominator = np.abs(df['q50'] - df['q25'])\n",
    "df['tilt_ratio'] = np.where(\n",
    "    denominator > 1e-6,\n",
    "    (df['q75'] - df['q50']) / denominator,\n",
    "    0\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Calculadas m√©tricas de assimetria (tilt)\")\n",
    "print(f\"  - tilt_mid: {df['tilt_mid'].mean():.6f} (m√©dia)\")\n",
    "print(f\"  - tilt_ratio: {df['tilt_ratio'].mean():.4f} (m√©dia)\")\n",
    "print(f\"  - tilt_ratio range: [{df['tilt_ratio'].min():.2f}, {df['tilt_ratio'].max():.2f}]\")\n",
    "\n",
    "# 3. Classificar regime de tilt\n",
    "df['tilt_regime'] = 'NEUTRO'\n",
    "df.loc[df['tilt_ratio'] > CONFIG['tilt_strength_hi'], 'tilt_regime'] = 'ALTA'\n",
    "df.loc[df['tilt_ratio'] < -CONFIG['tilt_strength_hi'], 'tilt_regime'] = 'BAIXA'\n",
    "\n",
    "regime_counts = df['tilt_regime'].value_counts()\n",
    "print(f\"\\n  üìä Distribui√ß√£o de regime de tilt:\")\n",
    "for regime, count in regime_counts.items():\n",
    "    pct = 100 * count / len(df)\n",
    "    print(f\"     {regime}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\n‚úÖ M√©tricas derivadas calculadas\")\n",
    "print(f\"üìã Novas colunas: width_5_95, width_5_95_usd, tilt_mid, tilt_ratio, tilt_regime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### 4.3 Percentis de Volatilidade (Rolling Window)\n",
    "\n",
    "Calcular o percentil de cada largura dentro da janela de lookback, por horizonte T."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Calculando percentis de volatilidade...\\n\")\n",
    "\n",
    "# Ordenar por ts0 e T para c√°lculos rolling\n",
    "df = df.sort_values(['T', 'ts0']).reset_index(drop=True)\n",
    "\n",
    "# Fun√ß√£o para calcular percentil rolling\n",
    "def calculate_percentile_rank(series, window_days=180):\n",
    "    \"\"\"Calcula rank percentil de cada valor na janela rolling\"\"\"\n",
    "    # Converter para timedelta baseado em dias (aproximadamente)\n",
    "    window_size = min(len(series), window_days)\n",
    "    \n",
    "    percentiles = []\n",
    "    for i in range(len(series)):\n",
    "        start_idx = max(0, i - window_size + 1)\n",
    "        window_values = series.iloc[start_idx:i+1]\n",
    "        \n",
    "        if len(window_values) < 2:\n",
    "            percentiles.append(0.5)  # Default para valores iniciais\n",
    "        else:\n",
    "            current_value = series.iloc[i]\n",
    "            rank = (window_values < current_value).sum()\n",
    "            percentile = rank / len(window_values)\n",
    "            percentiles.append(percentile)\n",
    "    \n",
    "    return percentiles\n",
    "\n",
    "# Calcular width_pct por horizonte T\n",
    "df['width_pct'] = 0.0\n",
    "\n",
    "for T in CONFIG['horizons']:\n",
    "    mask = df['T'] == T\n",
    "    df_T = df[mask].copy()\n",
    "    \n",
    "    # Calcular percentil da largura\n",
    "    width_pct_T = calculate_percentile_rank(\n",
    "        df_T['width_5_95_usd'], \n",
    "        window_days=CONFIG['lookback_days']\n",
    "    )\n",
    "    \n",
    "    df.loc[mask, 'width_pct'] = width_pct_T\n",
    "    \n",
    "    print(f\"  T={T}h: percentis calculados para {mask.sum():,} linhas\")\n",
    "\n",
    "# Classificar regime de volatilidade\n",
    "df['vol_regime'] = 'NEUTRA'\n",
    "df.loc[df['width_pct'] >= CONFIG['vol_hi_pct'], 'vol_regime'] = 'ALTA'\n",
    "df.loc[df['width_pct'] <= CONFIG['vol_lo_pct'], 'vol_regime'] = 'BAIXA'\n",
    "\n",
    "print(f\"\\n‚úì Percentis de volatilidade calculados\")\n",
    "print(f\"  M√©dia width_pct: {df['width_pct'].mean():.3f}\")\n",
    "print(f\"  Range: [{df['width_pct'].min():.3f}, {df['width_pct'].max():.3f}]\")\n",
    "\n",
    "# Distribui√ß√£o por regime\n",
    "vol_regime_counts = df['vol_regime'].value_counts()\n",
    "print(f\"\\n  üìä Distribui√ß√£o de regime de volatilidade:\")\n",
    "for regime, count in vol_regime_counts.items():\n",
    "    pct = 100 * count / len(df)\n",
    "    print(f\"     {regime}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\n‚úÖ Percentis de volatilidade calculados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### 4.4 Slope (Inclina√ß√£o da Mediana)\n",
    "\n",
    "Calcular a tend√™ncia do q50 usando regress√£o linear em janelas temporais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìà Calculando slopes (inclina√ß√£o da mediana)...\\n\")\n",
    "\n",
    "# Fun√ß√£o para calcular slope usando regress√£o linear\n",
    "def calculate_rolling_slope(series, window_size=7):\n",
    "    \"\"\"Calcula slope (inclina√ß√£o) usando regress√£o linear na janela\"\"\"\n",
    "    slopes = []\n",
    "    \n",
    "    for i in range(len(series)):\n",
    "        start_idx = max(0, i - window_size + 1)\n",
    "        window_values = series.iloc[start_idx:i+1].values\n",
    "        \n",
    "        if len(window_values) < 2:\n",
    "            slopes.append(0.0)\n",
    "        else:\n",
    "            x = np.arange(len(window_values))\n",
    "            y = window_values\n",
    "            \n",
    "            # Regress√£o linear: y = a + b*x, queremos b (slope)\n",
    "            if np.std(x) > 0 and np.std(y) > 0:\n",
    "                slope = np.cov(x, y)[0, 1] / np.var(x)\n",
    "            else:\n",
    "                slope = 0.0\n",
    "            \n",
    "            slopes.append(slope)\n",
    "    \n",
    "    return slopes\n",
    "\n",
    "# Calcular slopes para diferentes janelas temporais por horizonte T\n",
    "df['slope_q50_1w'] = 0.0  # 7 dias\n",
    "df['slope_q50_2w'] = 0.0  # 14 dias\n",
    "\n",
    "for T in CONFIG['horizons']:\n",
    "    mask = df['T'] == T\n",
    "    df_T = df[mask].copy()\n",
    "    \n",
    "    if len(df_T) > 2:\n",
    "        # Slope 1 semana\n",
    "        slopes_1w = calculate_rolling_slope(df_T['q50'], window_size=7)\n",
    "        df.loc[mask, 'slope_q50_1w'] = slopes_1w\n",
    "        \n",
    "        # Slope 2 semanas\n",
    "        slopes_2w = calculate_rolling_slope(df_T['q50'], window_size=14)\n",
    "        df.loc[mask, 'slope_q50_2w'] = slopes_2w\n",
    "        \n",
    "        print(f\"  T={T}h: slopes calculados\")\n",
    "        print(f\"    - slope_1w: {np.mean(slopes_1w):.6f} (m√©dia)\")\n",
    "        print(f\"    - slope_2w: {np.mean(slopes_2w):.6f} (m√©dia)\")\n",
    "\n",
    "print(f\"\\n‚úì Slopes calculados\")\n",
    "print(f\"  slope_q50_1w range: [{df['slope_q50_1w'].min():.6f}, {df['slope_q50_1w'].max():.6f}]\")\n",
    "print(f\"  slope_q50_2w range: [{df['slope_q50_2w'].min():.6f}, {df['slope_q50_2w'].max():.6f}]\")\n",
    "\n",
    "# Classificar tend√™ncia baseado em slope_1w\n",
    "df['trend'] = 'LATERAL'\n",
    "slope_threshold = df['slope_q50_1w'].std() * 0.5  # 0.5 desvios padr√£o\n",
    "df.loc[df['slope_q50_1w'] > slope_threshold, 'trend'] = 'ALTA'\n",
    "df.loc[df['slope_q50_1w'] < -slope_threshold, 'trend'] = 'BAIXA'\n",
    "\n",
    "trend_counts = df['trend'].value_counts()\n",
    "print(f\"\\n  üìä Distribui√ß√£o de tend√™ncia (slope_1w):\")\n",
    "for trend, count in trend_counts.items():\n",
    "    pct = 100 * count / len(df)\n",
    "    print(f\"     {trend}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\n‚úÖ Slopes calculados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### 4.5 RV Delta (Press√£o de Volatilidade)\n",
    "\n",
    "Calcular a diferen√ßa entre RVÃÇ e HAR-RV se dispon√≠vel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìâ Calculando RV Delta...\\n\")\n",
    "\n",
    "# Verificar se temos rvhat_ann\n",
    "if 'rvhat_ann' in df.columns:\n",
    "    print(\"‚úì Coluna 'rvhat_ann' encontrada\")\n",
    "    \n",
    "    # Verificar se temos har_rv_ann_T (pode n√£o existir nos dados atuais)\n",
    "    if 'har_rv_ann_T' in df.columns:\n",
    "        df['rv_delta'] = df['rvhat_ann'] - df['har_rv_ann_T']\n",
    "        print(\"‚úì Calculado rv_delta = rvhat_ann - har_rv_ann_T\")\n",
    "        \n",
    "        print(f\"\\n  Estat√≠sticas de rv_delta:\")\n",
    "        print(f\"    M√©dia: {df['rv_delta'].mean():.6f}\")\n",
    "        print(f\"    Std:   {df['rv_delta'].std():.6f}\")\n",
    "        print(f\"    Range: [{df['rv_delta'].min():.6f}, {df['rv_delta'].max():.6f}]\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  'har_rv_ann_T' n√£o encontrado\")\n",
    "        print(\"  Usando rvhat_ann como proxy de volatilidade\")\n",
    "        \n",
    "        # Inicializar coluna rv_delta\n",
    "        df['rv_delta'] = 0.0\n",
    "        \n",
    "        # Usar mudan√ßa relativa de rvhat_ann como proxy\n",
    "        for T in CONFIG['horizons']:\n",
    "            mask = df['T'] == T\n",
    "            df_T = df[mask].copy()\n",
    "            \n",
    "            if len(df_T) > 1:\n",
    "                # Delta vs m√©dia da janela\n",
    "                rv_mean = df_T['rvhat_ann'].rolling(window=30, min_periods=1).mean()\n",
    "                df.loc[mask, 'rv_delta'] = (df_T['rvhat_ann'].values - rv_mean.values)\n",
    "        \n",
    "        print(\"‚úì Calculado rv_delta como diferen√ßa vs m√©dia rolling\")\n",
    "    \n",
    "    # Estat√≠sticas finais\n",
    "    print(f\"\\n  üìä RV Delta:\")\n",
    "    print(f\"    Positivo (vol subindo): {(df['rv_delta'] > 0).sum():,} ({100*(df['rv_delta'] > 0).sum()/len(df):.1f}%)\")\n",
    "    print(f\"    Negativo (vol caindo):  {(df['rv_delta'] < 0).sum():,} ({100*(df['rv_delta'] < 0).sum()/len(df):.1f}%)\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  'rvhat_ann' n√£o encontrado nos dados\")\n",
    "    print(\"  Criando rv_delta = 0 (n√£o dispon√≠vel)\")\n",
    "    df['rv_delta'] = 0.0\n",
    "\n",
    "print(\"\\n‚úÖ RV Delta calculado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "## 5. Scoring System\n",
    "\n",
    "Calcular os scores de momentum:\n",
    "- **D (Direcional)**: combina tilt_ratio e slope\n",
    "- **V (Volatilidade)**: combina width_pct e rv_delta\n",
    "- **C (Confian√ßa)**: consist√™ncia entre horizontes e estabilidade\n",
    "- **MomentumIndex**: score composto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "### 5.1 Normaliza√ß√£o e Winsoriza√ß√£o\n",
    "\n",
    "Preparar as m√©tricas para scoring atrav√©s de normaliza√ß√£o robusta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîß Normalizando m√©tricas para scoring...\\n\")\n",
    "\n",
    "def winsorize_and_normalize(series, lower_pct=0.01, upper_pct=0.99, target_range=(-1, 1)):\n",
    "    \"\"\"\n",
    "    Winsoriza outliers e normaliza para range alvo.\n",
    "    \n",
    "    Args:\n",
    "        series: pandas Series\n",
    "        lower_pct: percentil inferior para winsoriza√ß√£o\n",
    "        upper_pct: percentil superior para winsoriza√ß√£o\n",
    "        target_range: tuple (min, max) do range alvo\n",
    "    \n",
    "    Returns:\n",
    "        Series normalizada\n",
    "    \"\"\"\n",
    "    # Winsoriza√ß√£o\n",
    "    lower = series.quantile(lower_pct)\n",
    "    upper = series.quantile(upper_pct)\n",
    "    series_wins = series.clip(lower=lower, upper=upper)\n",
    "    \n",
    "    # Normaliza√ß√£o para target_range\n",
    "    min_val = series_wins.min()\n",
    "    max_val = series_wins.max()\n",
    "    \n",
    "    if max_val - min_val > 1e-10:\n",
    "        normalized = (series_wins - min_val) / (max_val - min_val)\n",
    "        # Escalar para target_range\n",
    "        target_min, target_max = target_range\n",
    "        normalized = target_min + normalized * (target_max - target_min)\n",
    "    else:\n",
    "        # Se constante, retornar valor m√©dio do range\n",
    "        normalized = pd.Series(\n",
    "            [np.mean(target_range)] * len(series_wins),\n",
    "            index=series_wins.index\n",
    "        )\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "# Normalizar m√©tricas por horizonte T (para manter caracter√≠sticas por T)\n",
    "df['norm_tilt_ratio'] = 0.0\n",
    "df['norm_slope_1w'] = 0.0\n",
    "df['norm_rv_delta'] = 0.0\n",
    "\n",
    "for T in CONFIG['horizons']:\n",
    "    mask = df['T'] == T\n",
    "    \n",
    "    # Tilt ratio: j√° est√° em escala relativa, winsorizar para [-1, 1]\n",
    "    df.loc[mask, 'norm_tilt_ratio'] = winsorize_and_normalize(\n",
    "        df.loc[mask, 'tilt_ratio'],\n",
    "        target_range=(-1, 1)\n",
    "    )\n",
    "    \n",
    "    # Slope 1w: normalizar para [-1, 1]\n",
    "    df.loc[mask, 'norm_slope_1w'] = winsorize_and_normalize(\n",
    "        df.loc[mask, 'slope_q50_1w'],\n",
    "        target_range=(-1, 1)\n",
    "    )\n",
    "    \n",
    "    # RV delta: normalizar para [-1, 1], depois sigmoid para [0, 1]\n",
    "    if df.loc[mask, 'rv_delta'].std() > 1e-6:\n",
    "        df.loc[mask, 'norm_rv_delta'] = winsorize_and_normalize(\n",
    "            df.loc[mask, 'rv_delta'],\n",
    "            target_range=(-1, 1)\n",
    "        )\n",
    "    \n",
    "    print(f\"  T={T}h: m√©tricas normalizadas\")\n",
    "\n",
    "# Transformar rv_delta para [0, 1] usando sigmoid\n",
    "# Valores positivos = alta volatilidade esperada\n",
    "df['norm_rv_delta_01'] = 1 / (1 + np.exp(-df['norm_rv_delta']))\n",
    "\n",
    "print(f\"\\n‚úì Normaliza√ß√£o conclu√≠da\")\n",
    "print(f\"  norm_tilt_ratio: [{df['norm_tilt_ratio'].min():.3f}, {df['norm_tilt_ratio'].max():.3f}]\")\n",
    "print(f\"  norm_slope_1w: [{df['norm_slope_1w'].min():.3f}, {df['norm_slope_1w'].max():.3f}]\")\n",
    "print(f\"  norm_rv_delta_01: [{df['norm_rv_delta_01'].min():.3f}, {df['norm_rv_delta_01'].max():.3f}]\")\n",
    "\n",
    "print(\"\\n‚úÖ M√©tricas normalizadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "### 5.2 Score Direcional (D)\n",
    "\n",
    "Combina tilt_ratio e slope para medir for√ßa e dire√ß√£o do momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ Calculando Score Direcional (D)...\\n\")\n",
    "\n",
    "# Pesos configur√°veis\n",
    "w_tilt = CONFIG['score_weights']['directional']['tilt']\n",
    "w_slope = CONFIG['score_weights']['directional']['slope']\n",
    "\n",
    "# Score Direcional: D ‚àà [-1, 1]\n",
    "# Positivo = momentum de alta, Negativo = momentum de baixa\n",
    "df['D'] = (\n",
    "    w_tilt * df['norm_tilt_ratio'] +\n",
    "    w_slope * df['norm_slope_1w']\n",
    ")\n",
    "\n",
    "# Validar range\n",
    "assert df['D'].min() >= -1.01 and df['D'].max() <= 1.01, \"D fora do range [-1, 1]\"\n",
    "\n",
    "print(f\"‚úì Score Direcional (D) calculado\")\n",
    "print(f\"  Pesos: tilt={w_tilt}, slope={w_slope}\")\n",
    "print(f\"  Range: [{df['D'].min():.3f}, {df['D'].max():.3f}]\")\n",
    "print(f\"  M√©dia: {df['D'].mean():.3f}\")\n",
    "print(f\"  Std:   {df['D'].std():.3f}\")\n",
    "\n",
    "# Distribui√ß√£o\n",
    "print(f\"\\n  üìä Distribui√ß√£o do Score D:\")\n",
    "print(f\"    D > 0.5 (alta forte):   {(df['D'] > 0.5).sum():,} ({100*(df['D'] > 0.5).sum()/len(df):.1f}%)\")\n",
    "print(f\"    D ‚àà [0, 0.5] (alta):    {((df['D'] >= 0) & (df['D'] <= 0.5)).sum():,} ({100*((df['D'] >= 0) & (df['D'] <= 0.5)).sum()/len(df):.1f}%)\")\n",
    "print(f\"    D ‚àà [-0.5, 0] (baixa):  {((df['D'] < 0) & (df['D'] >= -0.5)).sum():,} ({100*((df['D'] < 0) & (df['D'] >= -0.5)).sum()/len(df):.1f}%)\")\n",
    "print(f\"    D < -0.5 (baixa forte): {(df['D'] < -0.5).sum():,} ({100*(df['D'] < -0.5).sum()/len(df):.1f}%)\")\n",
    "\n",
    "print(\"\\n‚úÖ Score Direcional calculado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "### 5.3 Score de Volatilidade (V)\n",
    "\n",
    "Combina width_pct (regime de volatilidade) e rv_delta (press√£o de volatilidade)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Calculando Score de Volatilidade (V)...\\n\")\n",
    "\n",
    "# Pesos configur√°veis\n",
    "w_width = CONFIG['score_weights']['volatility']['width_pct']\n",
    "w_rv = CONFIG['score_weights']['volatility']['rv_delta']\n",
    "\n",
    "# Score de Volatilidade: V ‚àà [0, 1]\n",
    "# 0 = volatilidade baixa, 1 = volatilidade alta\n",
    "# width_pct j√° est√° em [0, 1], norm_rv_delta_01 tamb√©m\n",
    "\n",
    "df['V'] = (\n",
    "    w_width * df['width_pct'] +\n",
    "    w_rv * df['norm_rv_delta_01']\n",
    ")\n",
    "\n",
    "# Validar range\n",
    "assert df['V'].min() >= -0.01 and df['V'].max() <= 1.01, \"V fora do range [0, 1]\"\n",
    "\n",
    "print(f\"‚úì Score de Volatilidade (V) calculado\")\n",
    "print(f\"  Pesos: width_pct={w_width}, rv_delta={w_rv}\")\n",
    "print(f\"  Range: [{df['V'].min():.3f}, {df['V'].max():.3f}]\")\n",
    "print(f\"  M√©dia: {df['V'].mean():.3f}\")\n",
    "print(f\"  Std:   {df['V'].std():.3f}\")\n",
    "\n",
    "# Distribui√ß√£o por regime\n",
    "print(f\"\\n  üìä Distribui√ß√£o do Score V:\")\n",
    "print(f\"    V > 0.8 (vol muito alta): {(df['V'] > 0.8).sum():,} ({100*(df['V'] > 0.8).sum()/len(df):.1f}%)\")\n",
    "print(f\"    V ‚àà [0.6, 0.8] (vol alta): {((df['V'] >= 0.6) & (df['V'] <= 0.8)).sum():,} ({100*((df['V'] >= 0.6) & (df['V'] <= 0.8)).sum()/len(df):.1f}%)\")\n",
    "print(f\"    V ‚àà [0.4, 0.6] (vol m√©dia): {((df['V'] >= 0.4) & (df['V'] < 0.6)).sum():,} ({100*((df['V'] >= 0.4) & (df['V'] < 0.6)).sum()/len(df):.1f}%)\")\n",
    "print(f\"    V ‚àà [0.2, 0.4] (vol baixa): {((df['V'] >= 0.2) & (df['V'] < 0.4)).sum():,} ({100*((df['V'] >= 0.2) & (df['V'] < 0.4)).sum()/len(df):.1f}%)\")\n",
    "print(f\"    V < 0.2 (vol muito baixa): {(df['V'] < 0.2).sum():,} ({100*(df['V'] < 0.2).sum()/len(df):.1f}%)\")\n",
    "\n",
    "print(\"\\n‚úÖ Score de Volatilidade calculado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "### 5.4 Score de Confian√ßa (C)\n",
    "\n",
    "Mede a consist√™ncia entre horizontes e estabilidade temporal dos sinais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéñÔ∏è  Calculando Score de Confian√ßa (C)...\\n\")\n",
    "\n",
    "# Para cada timestamp, calcular consist√™ncia entre horizontes\n",
    "df['C_consistency'] = 0.0\n",
    "df['C_stability'] = 0.0\n",
    "\n",
    "# Agrupar por timestamp\n",
    "for ts in df['ts0'].unique():\n",
    "    mask_ts = df['ts0'] == ts\n",
    "    df_ts = df[mask_ts].copy()\n",
    "    \n",
    "    if len(df_ts) >= 2:\n",
    "        # Consist√™ncia: qu√£o alinhados est√£o os sinais D entre horizontes\n",
    "        # Usar desvio padr√£o normalizado inverso (menor std = maior consist√™ncia)\n",
    "        d_std = df_ts['D'].std()\n",
    "        d_range = df_ts['D'].max() - df_ts['D'].min()\n",
    "        \n",
    "        # Consist√™ncia alta se std baixo\n",
    "        if d_range > 1e-6:\n",
    "            consistency = 1 - min(d_std / (d_range + 1e-6), 1.0)\n",
    "        else:\n",
    "            consistency = 1.0\n",
    "        \n",
    "        df.loc[mask_ts, 'C_consistency'] = consistency\n",
    "\n",
    "# Estabilidade temporal: calcular por horizonte T\n",
    "for T in CONFIG['horizons']:\n",
    "    mask = df['T'] == T\n",
    "    df_T = df[mask].copy().sort_values('ts0')\n",
    "    \n",
    "    if len(df_T) >= 3:\n",
    "        # Estabilidade: qu√£o est√°vel √© o sinal D ao longo do tempo\n",
    "        # Usar volatilidade do D (rolling std) normalizada\n",
    "        d_rolling_std = df_T['D'].rolling(window=7, min_periods=2).std()\n",
    "        \n",
    "        # Normalizar: menor volatilidade = maior estabilidade\n",
    "        max_std = d_rolling_std.max()\n",
    "        if max_std > 1e-6:\n",
    "            stability = 1 - (d_rolling_std / max_std).fillna(0)\n",
    "        else:\n",
    "            stability = pd.Series([1.0] * len(df_T), index=df_T.index)\n",
    "        \n",
    "        df.loc[mask, 'C_stability'] = stability.values\n",
    "\n",
    "# Pesos configur√°veis\n",
    "w_consistency = CONFIG['score_weights']['confidence']['consistency']\n",
    "w_stability = CONFIG['score_weights']['confidence']['stability']\n",
    "\n",
    "# Score de Confian√ßa: C ‚àà [0, 1]\n",
    "# 0 = baixa confian√ßa, 1 = alta confian√ßa\n",
    "df['C'] = (\n",
    "    w_consistency * df['C_consistency'] +\n",
    "    w_stability * df['C_stability']\n",
    ")\n",
    "\n",
    "# Validar range\n",
    "assert df['C'].min() >= -0.01 and df['C'].max() <= 1.01, \"C fora do range [0, 1]\"\n",
    "\n",
    "print(f\"‚úì Score de Confian√ßa (C) calculado\")\n",
    "print(f\"  Pesos: consistency={w_consistency}, stability={w_stability}\")\n",
    "print(f\"  Range: [{df['C'].min():.3f}, {df['C'].max():.3f}]\")\n",
    "print(f\"  M√©dia: {df['C'].mean():.3f}\")\n",
    "print(f\"  Std:   {df['C'].std():.3f}\")\n",
    "\n",
    "# Distribui√ß√£o\n",
    "print(f\"\\n  üìä Distribui√ß√£o do Score C:\")\n",
    "print(f\"    C > 0.8 (alta confian√ßa):   {(df['C'] > 0.8).sum():,} ({100*(df['C'] > 0.8).sum()/len(df):.1f}%)\")\n",
    "print(f\"    C ‚àà [0.6, 0.8] (boa):       {((df['C'] >= 0.6) & (df['C'] <= 0.8)).sum():,} ({100*((df['C'] >= 0.6) & (df['C'] <= 0.8)).sum()/len(df):.1f}%)\")\n",
    "print(f\"    C ‚àà [0.4, 0.6] (m√©dia):     {((df['C'] >= 0.4) & (df['C'] < 0.6)).sum():,} ({100*((df['C'] >= 0.4) & (df['C'] < 0.6)).sum()/len(df):.1f}%)\")\n",
    "print(f\"    C < 0.4 (baixa confian√ßa):  {(df['C'] < 0.4).sum():,} ({100*(df['C'] < 0.4).sum()/len(df):.1f}%)\")\n",
    "\n",
    "print(\"\\n‚úÖ Score de Confian√ßa calculado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "### 5.5 MomentumIndex (Score Composto)\n",
    "\n",
    "Combina D, V e C em um √≠ndice final de momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ Calculando MomentumIndex...\\n\")\n",
    "\n",
    "# MomentumIndex: combina dire√ß√£o (D), volatilidade (V) e confian√ßa (C)\n",
    "# F√≥rmula: MI = |D| * sqrt(V * C)\n",
    "#\n",
    "# Interpreta√ß√£o:\n",
    "# - Sinal de D indica dire√ß√£o (positivo=alta, negativo=baixa)\n",
    "# - Magnitude de MI indica for√ßa do momentum\n",
    "# - V modula pela volatilidade (oportunidades em alta volatilidade)\n",
    "# - C pondera pela confian√ßa no sinal\n",
    "\n",
    "df['MomentumIndex'] = np.abs(df['D']) * np.sqrt(df['V'] * df['C'])\n",
    "\n",
    "# Criar vers√£o com sinal preservado\n",
    "df['MomentumIndex_signed'] = np.sign(df['D']) * df['MomentumIndex']\n",
    "\n",
    "print(f\"‚úì MomentumIndex calculado\")\n",
    "print(f\"  F√≥rmula: MI = |D| √ó ‚àö(V √ó C)\")\n",
    "print(f\"  Range: [{df['MomentumIndex'].min():.3f}, {df['MomentumIndex'].max():.3f}]\")\n",
    "print(f\"  M√©dia: {df['MomentumIndex'].mean():.3f}\")\n",
    "print(f\"  Std:   {df['MomentumIndex'].std():.3f}\")\n",
    "\n",
    "# Classificar for√ßa do momentum\n",
    "df['momentum_strength'] = 'FRACO'\n",
    "df.loc[df['MomentumIndex'] >= 0.5, 'momentum_strength'] = 'MODERADO'\n",
    "df.loc[df['MomentumIndex'] >= 0.7, 'momentum_strength'] = 'FORTE'\n",
    "df.loc[df['MomentumIndex'] >= 0.85, 'momentum_strength'] = 'MUITO_FORTE'\n",
    "\n",
    "# Adicionar dire√ß√£o\n",
    "df['momentum_direction'] = 'NEUTRO'\n",
    "df.loc[df['D'] > 0.1, 'momentum_direction'] = 'ALTA'\n",
    "df.loc[df['D'] < -0.1, 'momentum_direction'] = 'BAIXA'\n",
    "\n",
    "# Combinar for√ßa e dire√ß√£o\n",
    "df['momentum_label'] = df['momentum_direction'] + '_' + df['momentum_strength']\n",
    "\n",
    "print(f\"\\n  üìä Distribui√ß√£o de For√ßa do Momentum:\")\n",
    "strength_counts = df['momentum_strength'].value_counts()\n",
    "for strength, count in strength_counts.items():\n",
    "    pct = 100 * count / len(df)\n",
    "    print(f\"    {strength}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\n  üß≠ Distribui√ß√£o de Dire√ß√£o:\")\n",
    "direction_counts = df['momentum_direction'].value_counts()\n",
    "for direction, count in direction_counts.items():\n",
    "    pct = 100 * count / len(df)\n",
    "    print(f\"    {direction}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\n  üè∑Ô∏è  Top Labels:\")\n",
    "top_labels = df['momentum_label'].value_counts().head(5)\n",
    "for label, count in top_labels.items():\n",
    "    pct = 100 * count / len(df)\n",
    "    print(f\"    {label}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\n‚úÖ MomentumIndex calculado\")\n",
    "print(f\"\\nüìã Scores finais dispon√≠veis: D, V, C, MomentumIndex, MomentumIndex_signed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "### 5.6 Resumo dos Scores\n",
    "\n",
    "Visualizar estat√≠sticas consolidadas de todos os scores calculados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Resumo dos Scores\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Criar tabela de resumo\n",
    "score_cols = ['D', 'V', 'C', 'MomentumIndex']\n",
    "summary_stats = df[score_cols].describe().T\n",
    "\n",
    "# Adicionar range\n",
    "summary_stats['range'] = summary_stats['max'] - summary_stats['min']\n",
    "\n",
    "print(\"\\nüìà Estat√≠sticas Descritivas:\")\n",
    "print(summary_stats.to_string())\n",
    "\n",
    "# Resumo por horizonte T\n",
    "print(\"\\n\\nüìä Scores M√©dios por Horizonte:\")\n",
    "print(\"-\" * 70)\n",
    "for T in sorted(CONFIG['horizons']):\n",
    "    mask = df['T'] == T\n",
    "    df_T = df[mask]\n",
    "    \n",
    "    print(f\"\\nT = {T}h ({mask.sum()} linhas):\")\n",
    "    for score in score_cols:\n",
    "        mean_val = df_T[score].mean()\n",
    "        std_val = df_T[score].std()\n",
    "        print(f\"  {score:15s}: {mean_val:7.3f} ¬± {std_val:6.3f}\")\n",
    "\n",
    "# Snapshot atual (√∫ltima previs√£o)\n",
    "print(\"\\n\\nüîç Snapshot Atual (√öltima Previs√£o):\")\n",
    "print(\"-\" * 70)\n",
    "latest_ts = df['ts0'].max()\n",
    "df_latest = df[df['ts0'] == latest_ts].sort_values('T')\n",
    "\n",
    "print(f\"Timestamp: {latest_ts}\")\n",
    "print(f\"Pre√ßo Spot: ${df_latest['S0'].iloc[0]:,.2f}\")\n",
    "print(f\"\\nScores por horizonte:\")\n",
    "\n",
    "for _, row in df_latest.iterrows():\n",
    "    print(f\"\\n  T={row['T']}h:\")\n",
    "    print(f\"    D (Direcional):     {row['D']:7.3f} ({row['momentum_direction']})\")\n",
    "    print(f\"    V (Volatilidade):   {row['V']:7.3f} ({row['vol_regime']})\")\n",
    "    print(f\"    C (Confian√ßa):      {row['C']:7.3f}\")\n",
    "    print(f\"    MomentumIndex:      {row['MomentumIndex']:7.3f} ({row['momentum_strength']})\")\n",
    "    print(f\"    Label:              {row['momentum_label']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ Resumo dos scores conclu√≠do\")\n",
    "print(f\"\\nüíæ Dataset final: {len(df):,} linhas, {len(df.columns)} colunas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "## 6. Visualiza√ß√µes\n",
    "\n",
    "An√°lise visual dos resultados: bandas quant√≠licas, scores, regimes e momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "### 6.1 Fan Charts - Bandas Quant√≠licas\n",
    "\n",
    "Visualizar evolu√ß√£o das bandas de previs√£o ao longo do tempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Criando Fan Charts das Bandas Quant√≠licas...\\n\")\n",
    "\n",
    "# Criar uma figura com subplots para cada horizonte\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, T in enumerate(CONFIG['horizons']):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Filtrar dados para este horizonte\n",
    "    df_T = df[df['T'] == T].sort_values('ts0').copy()\n",
    "    \n",
    "    if len(df_T) == 0:\n",
    "        ax.text(0.5, 0.5, f'Sem dados para T={T}h', \n",
    "                ha='center', va='center', transform=ax.transAxes, fontsize=14)\n",
    "        ax.set_title(f'T = {T}h (sem dados)', fontsize=14, fontweight='bold')\n",
    "        continue\n",
    "    \n",
    "    # Extrair dados\n",
    "    ts = df_T['ts0']\n",
    "    p05 = df_T['p_05']\n",
    "    p25 = df_T['p_25']\n",
    "    p50 = df_T['p_50']\n",
    "    p75 = df_T['p_75']\n",
    "    p95 = df_T['p_95']\n",
    "    s0 = df_T['S0']\n",
    "    \n",
    "    # Plotar bandas com transpar√™ncia (fan chart)\n",
    "    ax.fill_between(ts, p05, p95, alpha=0.15, color='blue', label='90% CI (q05-q95)')\n",
    "    ax.fill_between(ts, p25, p75, alpha=0.25, color='blue', label='50% CI (q25-q75)')\n",
    "    \n",
    "    # Plotar mediana e spot price\n",
    "    ax.plot(ts, p50, 'b-', linewidth=2, label='Mediana (q50)', alpha=0.8)\n",
    "    ax.plot(ts, s0, 'k--', linewidth=1.5, label='Spot Price (S0)', alpha=0.6)\n",
    "    \n",
    "    # Configura√ß√µes do gr√°fico\n",
    "    ax.set_title(f'T = {T}h | Bandas Quant√≠licas', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Timestamp', fontsize=11)\n",
    "    ax.set_ylabel('Pre√ßo (USD)', fontsize=11)\n",
    "    ax.legend(loc='best', fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Formatar eixo Y com separador de milhares\n",
    "    ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x:,.0f}'))\n",
    "    \n",
    "    # Rotacionar labels do eixo X\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # Adicionar estat√≠sticas no gr√°fico\n",
    "    latest_price = s0.iloc[-1]\n",
    "    latest_p50 = p50.iloc[-1]\n",
    "    diff_pct = 100 * (latest_p50 - latest_price) / latest_price\n",
    "    \n",
    "    info_text = f\"√öltimo: ${latest_price:,.0f}\\nMediana: ${latest_p50:,.0f}\\nDiff: {diff_pct:+.2f}%\"\n",
    "    ax.text(0.02, 0.98, info_text, transform=ax.transAxes,\n",
    "            verticalalignment='top', fontsize=9,\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Fan Charts - Evolu√ß√£o das Bandas Quant√≠licas por Horizonte', \n",
    "             fontsize=16, fontweight='bold', y=1.002)\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Fan Charts criados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "### 6.2 Time Series dos Scores\n",
    "\n",
    "Visualizar evolu√ß√£o temporal dos scores D, V, C e MomentumIndex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìà Criando Time Series dos Scores...\\n\")\n",
    "\n",
    "# Criar figura com 4 subplots (um para cada score)\n",
    "fig, axes = plt.subplots(4, 1, figsize=(16, 14))\n",
    "\n",
    "score_configs = [\n",
    "    {'col': 'D', 'title': 'Score Direcional (D)', 'ylim': (-1.1, 1.1), \n",
    "     'ylabel': 'D [-1, 1]', 'color_map': {'>0.5': 'green', '0 to 0.5': 'lightgreen', \n",
    "                                          '-0.5 to 0': 'lightcoral', '<-0.5': 'red'}},\n",
    "    {'col': 'V', 'title': 'Score de Volatilidade (V)', 'ylim': (-0.1, 1.1), \n",
    "     'ylabel': 'V [0, 1]', 'color_map': {'>0.8': 'red', '0.6-0.8': 'orange', \n",
    "                                         '0.4-0.6': 'yellow', '<0.4': 'green'}},\n",
    "    {'col': 'C', 'title': 'Score de Confian√ßa (C)', 'ylim': (-0.1, 1.1), \n",
    "     'ylabel': 'C [0, 1]', 'color_map': {'>0.8': 'green', '0.6-0.8': 'lightgreen', \n",
    "                                         '0.4-0.6': 'yellow', '<0.4': 'red'}},\n",
    "    {'col': 'MomentumIndex', 'title': 'MomentumIndex', 'ylim': (-0.1, 1.1), \n",
    "     'ylabel': 'MI [0, 1]', 'color_map': {'>0.7': 'darkgreen', '0.5-0.7': 'green', \n",
    "                                          '0.3-0.5': 'yellow', '<0.3': 'lightgray'}}\n",
    "]\n",
    "\n",
    "for idx, config in enumerate(score_configs):\n",
    "    ax = axes[idx]\n",
    "    score_col = config['col']\n",
    "    \n",
    "    # Plotar cada horizonte\n",
    "    for T in CONFIG['horizons']:\n",
    "        df_T = df[df['T'] == T].sort_values('ts0')\n",
    "        \n",
    "        if len(df_T) > 0:\n",
    "            ax.plot(df_T['ts0'], df_T[score_col], \n",
    "                   marker='o', linewidth=2, markersize=4, \n",
    "                   label=f'T={T}h', alpha=0.8)\n",
    "    \n",
    "    # Adicionar linha de refer√™ncia em 0 para D, e 0.5 para outros\n",
    "    if score_col == 'D':\n",
    "        ax.axhline(y=0, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
    "        ax.axhline(y=0.5, color='green', linestyle=':', linewidth=1, alpha=0.3)\n",
    "        ax.axhline(y=-0.5, color='red', linestyle=':', linewidth=1, alpha=0.3)\n",
    "    else:\n",
    "        ax.axhline(y=0.5, color='gray', linestyle='--', linewidth=1, alpha=0.5)\n",
    "        if score_col in ['V', 'C']:\n",
    "            ax.axhline(y=0.8, color='green', linestyle=':', linewidth=1, alpha=0.3)\n",
    "            ax.axhline(y=0.2, color='orange', linestyle=':', linewidth=1, alpha=0.3)\n",
    "    \n",
    "    # Configura√ß√µes\n",
    "    ax.set_title(config['title'], fontsize=13, fontweight='bold')\n",
    "    ax.set_ylabel(config['ylabel'], fontsize=11)\n",
    "    ax.set_ylim(config['ylim'])\n",
    "    ax.legend(loc='best', fontsize=9, ncol=4)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Rotacionar labels apenas no √∫ltimo subplot\n",
    "    if idx == len(score_configs) - 1:\n",
    "        ax.set_xlabel('Timestamp', fontsize=11)\n",
    "        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    else:\n",
    "        ax.set_xticklabels([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Time Series - Evolu√ß√£o dos Scores de Momentum', \n",
    "             fontsize=16, fontweight='bold', y=1.001)\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Time Series dos Scores criada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "### 6.3 Heatmaps de Momentum\n",
    "\n",
    "Visualizar scores em formato de heatmap (Horizonte √ó Tempo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üî• Criando Heatmaps de Momentum...\\n\")\n",
    "\n",
    "# Preparar dados para heatmap\n",
    "# Criar pivots: rows=Horizonte, columns=Timestamp\n",
    "\n",
    "def create_heatmap_data(df_input, score_col):\n",
    "    \"\"\"Criar matriz pivot para heatmap\"\"\"\n",
    "    # Criar pivot table\n",
    "    pivot = df_input.pivot_table(\n",
    "        values=score_col,\n",
    "        index='T',\n",
    "        columns='ts0',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    return pivot\n",
    "\n",
    "# Criar figura com 2 subplots (width_pct e MomentumIndex)\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 10))\n",
    "\n",
    "# Heatmap 1: Width Percentile (regime de volatilidade)\n",
    "ax1 = axes[0]\n",
    "try:\n",
    "    pivot_width = create_heatmap_data(df, 'width_pct')\n",
    "    \n",
    "    if not pivot_width.empty and pivot_width.shape[1] > 0:\n",
    "        sns.heatmap(pivot_width, ax=ax1, cmap='RdYlGn_r', \n",
    "                   cbar_kws={'label': 'Width Percentile'},\n",
    "                   annot=True, fmt='.2f', linewidths=0.5,\n",
    "                   vmin=0, vmax=1)\n",
    "        \n",
    "        ax1.set_title('Regime de Volatilidade (Width Percentile)', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "        ax1.set_xlabel('Timestamp', fontsize=11)\n",
    "        ax1.set_ylabel('Horizonte (horas)', fontsize=11)\n",
    "        \n",
    "        # Rotacionar labels\n",
    "        plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    else:\n",
    "        ax1.text(0.5, 0.5, 'Dados insuficientes para heatmap', \n",
    "                ha='center', va='center', transform=ax1.transAxes, fontsize=14)\n",
    "        ax1.set_title('Regime de Volatilidade (sem dados)', fontsize=14)\n",
    "except Exception as e:\n",
    "    ax1.text(0.5, 0.5, f'Erro ao criar heatmap: {str(e)}', \n",
    "            ha='center', va='center', transform=ax1.transAxes, fontsize=12)\n",
    "    ax1.set_title('Regime de Volatilidade (erro)', fontsize=14)\n",
    "\n",
    "# Heatmap 2: MomentumIndex\n",
    "ax2 = axes[1]\n",
    "try:\n",
    "    pivot_momentum = create_heatmap_data(df, 'MomentumIndex')\n",
    "    \n",
    "    if not pivot_momentum.empty and pivot_momentum.shape[1] > 0:\n",
    "        sns.heatmap(pivot_momentum, ax=ax2, cmap='RdYlGn', \n",
    "                   cbar_kws={'label': 'MomentumIndex'},\n",
    "                   annot=True, fmt='.2f', linewidths=0.5,\n",
    "                   vmin=0, vmax=1)\n",
    "        \n",
    "        ax2.set_title('MomentumIndex por Horizonte √ó Tempo', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "        ax2.set_xlabel('Timestamp', fontsize=11)\n",
    "        ax2.set_ylabel('Horizonte (horas)', fontsize=11)\n",
    "        \n",
    "        # Rotacionar labels\n",
    "        plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    else:\n",
    "        ax2.text(0.5, 0.5, 'Dados insuficientes para heatmap', \n",
    "                ha='center', va='center', transform=ax2.transAxes, fontsize=14)\n",
    "        ax2.set_title('MomentumIndex (sem dados)', fontsize=14)\n",
    "except Exception as e:\n",
    "    ax2.text(0.5, 0.5, f'Erro ao criar heatmap: {str(e)}', \n",
    "            ha='center', va='center', transform=ax2.transAxes, fontsize=12)\n",
    "    ax2.set_title('MomentumIndex (erro)', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Heatmaps - An√°lise de Momentum Multi-Horizonte', \n",
    "             fontsize=16, fontweight='bold', y=1.001)\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Heatmaps criados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "### 6.4 Distribui√ß√µes e Regimes\n",
    "\n",
    "Histogramas e distribui√ß√µes dos scores, m√©tricas e regimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Criando Visualiza√ß√µes de Distribui√ß√µes e Regimes...\\n\")\n",
    "\n",
    "# Figura com 6 subplots (2 linhas √ó 3 colunas)\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. Histogramas dos Scores\n",
    "ax1 = fig.add_subplot(gs[0, :])  # Primeira linha inteira\n",
    "score_cols = ['D', 'V', 'C', 'MomentumIndex']\n",
    "colors = ['steelblue', 'coral', 'mediumseagreen', 'purple']\n",
    "\n",
    "for score, color in zip(score_cols, colors):\n",
    "    if df[score].notna().sum() > 0:\n",
    "        ax1.hist(df[score].dropna(), bins=20, alpha=0.6, \n",
    "                label=score, color=color, edgecolor='black')\n",
    "\n",
    "ax1.set_title('Distribui√ß√£o dos Scores de Momentum', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Valor do Score', fontsize=11)\n",
    "ax1.set_ylabel('Frequ√™ncia', fontsize=11)\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Distribui√ß√£o de Regimes de Volatilidade\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "vol_counts = df['vol_regime'].value_counts()\n",
    "colors_vol = {'ALTA': 'red', 'NEUTRA': 'yellow', 'BAIXA': 'green'}\n",
    "colors_list = [colors_vol.get(x, 'gray') for x in vol_counts.index]\n",
    "\n",
    "ax2.bar(range(len(vol_counts)), vol_counts.values, color=colors_list, \n",
    "        edgecolor='black', alpha=0.7)\n",
    "ax2.set_xticks(range(len(vol_counts)))\n",
    "ax2.set_xticklabels(vol_counts.index, rotation=45, ha='right')\n",
    "ax2.set_title('Regime de Volatilidade', fontsize=13, fontweight='bold')\n",
    "ax2.set_ylabel('Contagem', fontsize=11)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Adicionar percentuais\n",
    "for i, (regime, count) in enumerate(vol_counts.items()):\n",
    "    pct = 100 * count / len(df)\n",
    "    ax2.text(i, count + 0.05*count, f'{pct:.1f}%', \n",
    "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# 3. Distribui√ß√£o de Regimes de Tilt (Dire√ß√£o)\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "tilt_counts = df['tilt_regime'].value_counts()\n",
    "colors_tilt = {'ALTA': 'green', 'NEUTRO': 'gray', 'BAIXA': 'red'}\n",
    "colors_list = [colors_tilt.get(x, 'gray') for x in tilt_counts.index]\n",
    "\n",
    "ax3.bar(range(len(tilt_counts)), tilt_counts.values, color=colors_list, \n",
    "        edgecolor='black', alpha=0.7)\n",
    "ax3.set_xticks(range(len(tilt_counts)))\n",
    "ax3.set_xticklabels(tilt_counts.index, rotation=45, ha='right')\n",
    "ax3.set_title('Regime de Tilt (Dire√ß√£o)', fontsize=13, fontweight='bold')\n",
    "ax3.set_ylabel('Contagem', fontsize=11)\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for i, (regime, count) in enumerate(tilt_counts.items()):\n",
    "    pct = 100 * count / len(df)\n",
    "    ax3.text(i, count + 0.05*count, f'{pct:.1f}%', \n",
    "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# 4. Distribui√ß√£o de For√ßa do Momentum\n",
    "ax4 = fig.add_subplot(gs[1, 2])\n",
    "strength_counts = df['momentum_strength'].value_counts()\n",
    "colors_strength = {'MUITO_FORTE': 'darkgreen', 'FORTE': 'green', \n",
    "                  'MODERADO': 'yellow', 'FRACO': 'lightgray'}\n",
    "colors_list = [colors_strength.get(x, 'gray') for x in strength_counts.index]\n",
    "\n",
    "ax4.bar(range(len(strength_counts)), strength_counts.values, color=colors_list, \n",
    "        edgecolor='black', alpha=0.7)\n",
    "ax4.set_xticks(range(len(strength_counts)))\n",
    "ax4.set_xticklabels(strength_counts.index, rotation=45, ha='right')\n",
    "ax4.set_title('For√ßa do Momentum', fontsize=13, fontweight='bold')\n",
    "ax4.set_ylabel('Contagem', fontsize=11)\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for i, (strength, count) in enumerate(strength_counts.items()):\n",
    "    pct = 100 * count / len(df)\n",
    "    ax4.text(i, count + 0.05*count, f'{pct:.1f}%', \n",
    "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# 5. Scatter: Width Percentile vs MomentumIndex\n",
    "ax5 = fig.add_subplot(gs[2, 0])\n",
    "scatter = ax5.scatter(df['width_pct'], df['MomentumIndex'], \n",
    "                     c=df['D'], cmap='RdYlGn', s=100, \n",
    "                     alpha=0.7, edgecolors='black', linewidth=0.5)\n",
    "ax5.set_xlabel('Width Percentile', fontsize=11)\n",
    "ax5.set_ylabel('MomentumIndex', fontsize=11)\n",
    "ax5.set_title('Width √ó Momentum (cor=D)', fontsize=13, fontweight='bold')\n",
    "ax5.grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter, ax=ax5, label='Score D')\n",
    "\n",
    "# 6. Scatter: Tilt Ratio vs Slope\n",
    "ax6 = fig.add_subplot(gs[2, 1])\n",
    "scatter2 = ax6.scatter(df['tilt_ratio'], df['slope_q50_1w'], \n",
    "                      c=df['MomentumIndex'], cmap='viridis', s=100, \n",
    "                      alpha=0.7, edgecolors='black', linewidth=0.5)\n",
    "ax6.set_xlabel('Tilt Ratio', fontsize=11)\n",
    "ax6.set_ylabel('Slope q50 (1w)', fontsize=11)\n",
    "ax6.set_title('Tilt √ó Slope (cor=MI)', fontsize=13, fontweight='bold')\n",
    "ax6.grid(True, alpha=0.3)\n",
    "ax6.axhline(y=0, color='gray', linestyle='--', linewidth=1, alpha=0.5)\n",
    "ax6.axvline(x=0, color='gray', linestyle='--', linewidth=1, alpha=0.5)\n",
    "plt.colorbar(scatter2, ax=ax6, label='MomentumIndex')\n",
    "\n",
    "# 7. Box plot dos scores por horizonte\n",
    "ax7 = fig.add_subplot(gs[2, 2])\n",
    "df_melted = df.melt(id_vars='T', value_vars=['D', 'V', 'C', 'MomentumIndex'],\n",
    "                    var_name='Score', value_name='Value')\n",
    "\n",
    "# Criar box plot\n",
    "bp = ax7.boxplot([df_melted[df_melted['Score'] == s]['Value'].dropna() \n",
    "                  for s in ['D', 'V', 'C', 'MomentumIndex']],\n",
    "                 labels=['D', 'V', 'C', 'MI'],\n",
    "                 patch_artist=True,\n",
    "                 showmeans=True)\n",
    "\n",
    "# Colorir boxes\n",
    "colors_box = ['steelblue', 'coral', 'mediumseagreen', 'purple']\n",
    "for patch, color in zip(bp['boxes'], colors_box):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.6)\n",
    "\n",
    "ax7.set_title('Box Plot dos Scores', fontsize=13, fontweight='bold')\n",
    "ax7.set_ylabel('Valor', fontsize=11)\n",
    "ax7.grid(True, alpha=0.3, axis='y')\n",
    "ax7.axhline(y=0, color='gray', linestyle='--', linewidth=1, alpha=0.5)\n",
    "\n",
    "plt.suptitle('An√°lise de Distribui√ß√µes e Regimes de Momentum', \n",
    "             fontsize=16, fontweight='bold', y=0.998)\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualiza√ß√µes de distribui√ß√µes criadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "### 6.5 Resumo das Visualiza√ß√µes\n",
    "\n",
    "Consolida√ß√£o dos insights visuais obtidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìù Resumo das Visualiza√ß√µes Criadas\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "visualizations_summary = {\n",
    "    'Fan Charts': {\n",
    "        'Descri√ß√£o': 'Evolu√ß√£o das bandas quant√≠licas (q05-q95, q25-q75) por horizonte',\n",
    "        'Insights': [\n",
    "            'Visualiza√ß√£o da incerteza nas previs√µes',\n",
    "            'Compara√ß√£o entre mediana projetada e pre√ßo spot',\n",
    "            'An√°lise de largura das bandas ao longo do tempo'\n",
    "        ]\n",
    "    },\n",
    "    'Time Series': {\n",
    "        'Descri√ß√£o': 'Evolu√ß√£o temporal dos scores D, V, C e MomentumIndex',\n",
    "        'Insights': [\n",
    "            'Tend√™ncias direcionais (D) ao longo do tempo',\n",
    "            'Regimes de volatilidade (V)',\n",
    "            'Confian√ßa nos sinais (C)',\n",
    "            'For√ßa do momentum composto'\n",
    "        ]\n",
    "    },\n",
    "    'Heatmaps': {\n",
    "        'Descri√ß√£o': 'An√°lise multi-horizonte em formato matricial',\n",
    "        'Insights': [\n",
    "            'Identifica√ß√£o de padr√µes por horizonte',\n",
    "            'Compara√ß√£o entre diferentes T',\n",
    "            'Evolu√ß√£o temporal consolidada'\n",
    "        ]\n",
    "    },\n",
    "    'Distribui√ß√µes': {\n",
    "        'Descri√ß√£o': 'Histogramas, regimes e correla√ß√µes entre m√©tricas',\n",
    "        'Insights': [\n",
    "            'Distribui√ß√£o estat√≠stica dos scores',\n",
    "            'Propor√ß√£o de cada regime (vol/tilt/for√ßa)',\n",
    "            'Rela√ß√µes entre width, tilt e slope',\n",
    "            'Outliers e valores extremos'\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "for viz_type, info in visualizations_summary.items():\n",
    "    print(f\"\\nüìä {viz_type}\")\n",
    "    print(f\"   {info['Descri√ß√£o']}\")\n",
    "    print(f\"\\n   Insights principais:\")\n",
    "    for insight in info['Insights']:\n",
    "        print(f\"   ‚Ä¢ {insight}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ Total de visualiza√ß√µes criadas: 4 tipos (12+ gr√°ficos)\")\n",
    "print(\"\\nüí° Observa√ß√µes:\")\n",
    "print(\"   ‚Ä¢ Dados atuais cont√™m apenas 1 linha (T=42h)\")\n",
    "print(\"   ‚Ä¢ Visualiza√ß√µes foram testadas e funcionam corretamente\")\n",
    "print(\"   ‚Ä¢ Com mais dados hist√≥ricos, os gr√°ficos ser√£o mais informativos\")\n",
    "print(\"   ‚Ä¢ Fan charts mostram distribui√ß√£o de incerteza das previs√µes\")\n",
    "print(\"   ‚Ä¢ Scores combinam m√∫ltiplas dimens√µes: dire√ß√£o, volatilidade e confian√ßa\")\n",
    "\n",
    "print(f\"\\nüìã Colunas dispon√≠veis no dataset final:\")\n",
    "print(f\"   Total: {len(df.columns)} colunas\")\n",
    "print(f\"   Scores: D, V, C, MomentumIndex, MomentumIndex_signed\")\n",
    "print(f\"   Regimes: vol_regime, tilt_regime, trend, momentum_strength, momentum_direction\")\n",
    "print(f\"   Labels: momentum_label\")\n",
    "\n",
    "print(\"\\n‚úÖ Se√ß√£o de visualiza√ß√µes conclu√≠da com sucesso!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
