{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# BTC Momentum Analysis — Module 02 Only\n",
    "\n",
    "**Objetivo:** Produzir sinais de momentum a partir das bandas quantílicas do Módulo 02.\n",
    "\n",
    "**Escopo:** Apenas arquivos `preds_T=*.parquet` com T ∈ {42,48,54,60}.\n",
    "\n",
    "**Saídas:**\n",
    "- Série temporal de scores de momentum (direção/volatilidade/confiança)\n",
    "- Snapshots por horizonte T\n",
    "- Gráficos de análise\n",
    "- Relatório HTML\n",
    "- Pré-checagens para Módulo 03\n",
    "\n",
    "---\n",
    "\n",
    "**Data de execução:** October 2, 2025  \n",
    "**Timezone:** UTC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import yaml\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "from scipy import stats\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuração de plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "# Seed para reprodutibilidade\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"✅ Imports carregados\")\n",
    "print(f\"📅 Data: {datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### 1.1 Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuração do notebook\n",
    "CONFIG = {\n",
    "    'lookback_days': 180,           # janela para análises/percentis\n",
    "    'horizons': [42, 48, 54, 60],   # horizontes de previsão (barras de 4H = 7-10 dias)\n",
    "    'target_T_default': 48,         # horizonte padrão para gráficos\n",
    "    'tilt_strength_hi': 1.2,        # |tilt_ratio| > este valor => direção forte\n",
    "    'vol_hi_pct': 0.80,             # percentil de largura > 0.8 => vol alta\n",
    "    'vol_lo_pct': 0.20,             # percentil de largura < 0.2 => vol baixa\n",
    "    'recency_max_days': 7,          # máximo de dias desde última previsão\n",
    "    \n",
    "    'score_weights': {\n",
    "        'directional': {'tilt': 0.6, 'slope': 0.4},\n",
    "        'volatility': {'width_pct': 0.7, 'rv_delta': 0.3},\n",
    "        'confidence': {'consistency': 0.5, 'stability': 0.5}\n",
    "    },\n",
    "    \n",
    "    'export_paths': {\n",
    "        'ts_table': 'data/processed/momentum/momentum_timeseries.parquet',\n",
    "        'snapshot': 'data/processed/momentum/momentum_snapshot.csv',\n",
    "        'report_html': 'data/processed/momentum/momentum_report.html',\n",
    "        'charts_dir': 'data/processed/momentum/charts/'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Exibir configuração\n",
    "print(\"⚙️  Configuração do Notebook:\")\n",
    "print(\"=\" * 60)\n",
    "for key, value in CONFIG.items():\n",
    "    if key != 'score_weights' and key != 'export_paths':\n",
    "        print(f\"{key:20s}: {value}\")\n",
    "\n",
    "print(\"\\n📊 Pesos dos Scores:\")\n",
    "for score_type, weights in CONFIG['score_weights'].items():\n",
    "    print(f\"  {score_type:15s}: {weights}\")\n",
    "\n",
    "print(\"\\n✅ Configuração carregada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 2. Data Discovery & Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### 2.1 Descoberta de Arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descobrir arquivos de predições\n",
    "data_dir = Path('../data/processed/preds')\n",
    "pred_files = sorted(data_dir.glob('preds_T=*.parquet'))\n",
    "\n",
    "print(f\"🔍 Buscando em: {data_dir}\")\n",
    "print(f\"📁 Arquivos encontrados: {len(pred_files)}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if not pred_files:\n",
    "    # Tentar diretório alternativo (notebooks)\n",
    "    alt_dir = Path('notebooks/data/processed/preds')\n",
    "    pred_files = sorted(alt_dir.glob('preds_T=*.parquet'))\n",
    "    print(f\"🔍 Tentando diretório alternativo: {alt_dir}\")\n",
    "    print(f\"📁 Arquivos encontrados: {len(pred_files)}\")\n",
    "\n",
    "if pred_files:\n",
    "    for f in pred_files:\n",
    "        size_mb = f.stat().st_size / (1024*1024)\n",
    "        print(f\"  ✓ {f.name:30s} ({size_mb:6.2f} MB)\")\n",
    "else:\n",
    "    print(\"❌ ERRO: Nenhum arquivo preds_T=*.parquet encontrado!\")\n",
    "    print(\"\\nVerificando estrutura de diretórios...\")\n",
    "    if data_dir.exists():\n",
    "        all_files = list(data_dir.glob('*'))\n",
    "        print(f\"\\nArquivos em {data_dir}:\")\n",
    "        for f in all_files[:10]:  # Primeiros 10\n",
    "            print(f\"  - {f.name}\")\n",
    "\n",
    "# Buscar metadados opcionais\n",
    "meta_file = data_dir / 'meta_pred.json'\n",
    "qc_file = data_dir / 'qc_oos.json'\n",
    "\n",
    "print(f\"\\n📋 Metadados:\")\n",
    "print(f\"  meta_pred.json: {'✓ Encontrado' if meta_file.exists() else '✗ Não encontrado'}\")\n",
    "print(f\"  qc_oos.json:    {'✓ Encontrado' if qc_file.exists() else '✗ Não encontrado'}\")\n",
    "\n",
    "print(\"\\n✅ Descoberta concluída\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### 2.2 Carregar e Concatenar Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar todos os arquivos de predição\n",
    "if not pred_files:\n",
    "    raise FileNotFoundError(\"❌ Nenhum arquivo de predição encontrado. Verifique o diretório de dados.\")\n",
    "\n",
    "dfs = []\n",
    "for f in pred_files:\n",
    "    try:\n",
    "        df_temp = pd.read_parquet(f)\n",
    "        dfs.append(df_temp)\n",
    "        print(f\"✓ Carregado {f.name}: {len(df_temp):,} linhas\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Erro ao carregar {f.name}: {e}\")\n",
    "\n",
    "# Concatenar\n",
    "if dfs:\n",
    "    df_raw = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"\\n📊 Dataset concatenado: {len(df_raw):,} linhas, {len(df_raw.columns)} colunas\")\n",
    "else:\n",
    "    raise ValueError(\"❌ Nenhum arquivo foi carregado com sucesso\")\n",
    "\n",
    "# Exibir colunas disponíveis\n",
    "print(f\"\\n📋 Colunas disponíveis:\")\n",
    "print(f\"  {', '.join(sorted(df_raw.columns))}\")\n",
    "\n",
    "# Amostra dos dados\n",
    "print(f\"\\n🔍 Primeiras linhas:\")\n",
    "display(df_raw.head())\n",
    "\n",
    "print(\"\\n✅ Dados carregados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### 🔍 DIAGNÓSTICO: Por que temos apenas 1 linha?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔍 DIAGNÓSTICO: Investigando dados carregados\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. Verificar arquivos encontrados\n",
    "print(f\"\\n📁 Arquivos encontrados: {len(pred_files)}\")\n",
    "for f in pred_files:\n",
    "    print(f\"   - {f.name}\")\n",
    "\n",
    "# 2. Verificar dados brutos carregados\n",
    "print(f\"\\n📊 Dataset bruto (df_raw):\")\n",
    "print(f\"   Shape: {df_raw.shape}\")\n",
    "print(f\"   Colunas: {list(df_raw.columns)}\")\n",
    "\n",
    "# 3. Verificar distribuição por horizonte T\n",
    "print(f\"\\n📈 Distribuição por horizonte (df_raw):\")\n",
    "print(df_raw['T'].value_counts().sort_index())\n",
    "\n",
    "# 4. Verificar timestamps únicos\n",
    "print(f\"\\n📅 Timestamps únicos no df_raw: {df_raw['ts0'].nunique()}\")\n",
    "print(f\"   Período: {df_raw['ts0'].min()} até {df_raw['ts0'].max()}\")\n",
    "\n",
    "# 5. Verificar o que aconteceu após filtros\n",
    "print(f\"\\n🔍 Dataset após filtros (df):\")\n",
    "print(f\"   Shape atual: {df.shape}\")\n",
    "print(f\"   Colunas: {len(df.columns)}\")\n",
    "\n",
    "# 6. Verificar se perdemos dados nos filtros\n",
    "print(f\"\\n⚠️  Análise de perda de dados:\")\n",
    "print(f\"   df_raw: {len(df_raw)} linhas\")\n",
    "print(f\"   df: {len(df)} linhas\")\n",
    "print(f\"   Perda: {len(df_raw) - len(df)} linhas ({100*(len(df_raw)-len(df))/len(df_raw):.1f}%)\")\n",
    "\n",
    "# 7. Mostrar amostra do df_raw\n",
    "print(f\"\\n📋 Amostra do df_raw (primeiras 3 linhas):\")\n",
    "display(df_raw.head(3))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"💡 PROBLEMA IDENTIFICADO:\")\n",
    "print(\"   Os arquivos preds_T=*.parquet provavelmente contêm apenas 1 linha cada\")\n",
    "print(\"   ou foram filtrados excessivamente durante a validação.\")\n",
    "print(\"\\n🎯 SOLUÇÃO:\")\n",
    "print(\"   1. Verificar se os modelos foram treinados e geraram predições\")\n",
    "print(\"   2. Checar se há mais dados históricos em data/processed/preds/\")\n",
    "print(\"   3. Possivelmente precisamos rodar o treinamento do Módulo 02\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔎 Investigando diretórios de predições...\\n\")\n",
    "\n",
    "# Verificar diferentes locais possíveis\n",
    "possible_dirs = [\n",
    "    Path('data/processed/preds'),\n",
    "    Path('notebooks/data/processed/preds'),\n",
    "    Path('data/processed/02a_train'),\n",
    "    Path('data/processed/models'),\n",
    "    Path('data/processed/models_super_fast'),\n",
    "]\n",
    "\n",
    "for dir_path in possible_dirs:\n",
    "    if dir_path.exists():\n",
    "        print(f\"\\n📁 {dir_path}:\")\n",
    "        # Listar todos os arquivos\n",
    "        all_files = list(dir_path.glob('*'))\n",
    "        if all_files:\n",
    "            for f in sorted(all_files)[:20]:  # Limitar a 20 arquivos\n",
    "                if f.is_file():\n",
    "                    size = f.stat().st_size / 1024  # KB\n",
    "                    print(f\"   {'📄' if f.suffix else '📁'} {f.name:40s} ({size:8.2f} KB)\")\n",
    "                else:\n",
    "                    print(f\"   📁 {f.name}/\")\n",
    "        else:\n",
    "            print(f\"   (vazio)\")\n",
    "    else:\n",
    "        print(f\"\\n❌ {dir_path}: não existe\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"💡 ANÁLISE:\")\n",
    "print(\"   Parece que só temos 1 arquivo: preds_T=42.parquet com apenas 1 linha\")\n",
    "print(\"   Isso significa que:\")\n",
    "print(\"   1. O modelo do Módulo 02 NÃO foi treinado completamente, OU\")\n",
    "print(\"   2. As predições não foram geradas para série temporal, OU\")\n",
    "print(\"   3. Estamos olhando no diretório errado\")\n",
    "print(\"\\n🎯 PRÓXIMOS PASSOS:\")\n",
    "print(\"   A. Verificar se precisa treinar os modelos do Módulo 02\")\n",
    "print(\"   B. Olhar nos notebooks anteriores (02a) para ver onde estão as predições\")\n",
    "print(\"   C. Usar dados de exemplo/teste para demonstrar o notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### ✅ PROBLEMA IDENTIFICADO E SOLUÇÃO\n",
    "\n",
    "**Causa Raiz:**\n",
    "Os gráficos estão vazios porque temos apenas **1 linha de dados** (1 timestamp).\n",
    "\n",
    "**Análise:**\n",
    "1. ✅ Modelos do Módulo 02 **foram treinados** (models_T42.joblib, etc. existem)\n",
    "2. ❌ **Predições OOS em série temporal NÃO foram geradas**\n",
    "3. ❌ Arquivos `preds_T={42,48,54,60}.parquet` não existem ou têm apenas 1 linha\n",
    "\n",
    "**Por que isso aconteceu:**\n",
    "- O notebook `02a_train_report_gold.ipynb` treina modelos mas gera apenas:\n",
    "  - Métricas de validação cruzada\n",
    "  - Predições de teste pontuais\n",
    "  - **NÃO gera série temporal de predições OOS**\n",
    "\n",
    "**Soluções:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔧 GERANDO PREDIÇÕES OOS PARA SÉRIE TEMPORAL\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Vamos gerar predições usando os modelos treinados\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# 1. Carregar features\n",
    "features_path = Path('data/processed/features/features_4H.parquet')\n",
    "if not features_path.exists():\n",
    "    print(\"❌ Arquivo features_4H.parquet não encontrado!\")\n",
    "else:\n",
    "    print(f\"✓ Carregando features de: {features_path}\")\n",
    "    df_features = pd.read_parquet(features_path)\n",
    "    print(f\"  Shape: {df_features.shape}\")\n",
    "    print(f\"  Período: {df_features['timestamp'].min()} até {df_features['timestamp'].max()}\")\n",
    "    \n",
    "    # 2. Carregar modelos e gerar predições para cada horizonte\n",
    "    horizons_to_predict = [42, 48, 54, 60]\n",
    "    predictions_dict = {}\n",
    "    \n",
    "    for T in horizons_to_predict:\n",
    "        model_path = Path(f'data/processed/preds/models_T{T}.joblib')\n",
    "        \n",
    "        if model_path.exists():\n",
    "            print(f\"\\n📊 Processando T={T}h:\")\n",
    "            \n",
    "            # Carregar modelo\n",
    "            models = joblib.load(model_path)\n",
    "            print(f\"  ✓ Modelo carregado: {type(models)}\")\n",
    "            \n",
    "            # Preparar features (assumindo que os modelos esperam as mesmas features do treino)\n",
    "            # Aqui precisaríamos saber quais features foram usadas no treino\n",
    "            \n",
    "            # Por enquanto, vamos criar predições de exemplo\n",
    "            # Na prática, você precisaria:\n",
    "            # 1. Carregar meta_train.json para saber quais features usar\n",
    "            # 2. Fazer rolling window predictions\n",
    "            # 3. Aplicar calibradores\n",
    "            \n",
    "            print(f\"  ⚠️  Implementação completa requer lógica de predição do Módulo 02\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"\\n❌ T={T}h: modelo não encontrado em {model_path}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"💡 PRÓXIMO PASSO:\")\n",
    "    print(\"   Precisamos implementar a lógica de geração de predições OOS\")\n",
    "    print(\"   Isso envolve:\")\n",
    "    print(\"   1. Carregar modelos treinados\")\n",
    "    print(\"   2. Fazer rolling window predictions no conjunto de features\")\n",
    "    print(\"   3. Aplicar calibração conformal\")\n",
    "    print(\"   4. Salvar como preds_T=*.parquet\")\n",
    "    print(\"\\n🎯 ALTERNATIVA RÁPIDA:\")\n",
    "    print(\"   Podemos criar dados sintéticos para demonstrar o notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📋 RESUMO EXECUTIVO - Por que os gráficos estão vazios?\n",
    "\n",
    "### 🔍 Diagnóstico Completo:\n",
    "\n",
    "**Situação Atual:**\n",
    "- ✅ **Módulo 02 (modelos) FOI treinado com sucesso**\n",
    "  - Existem: `models_T{42,48,54,60}.joblib` (15-18 MB cada)\n",
    "  - Existem: calibradores, métricas CV, feature importance\n",
    "  - Treinamento completo em `data/processed/preds/`\n",
    "\n",
    "- ❌ **Predições OOS em série temporal NÃO foram geradas**\n",
    "  - Arquivos `preds_T=*.parquet` não existem (ou têm apenas 1 linha)\n",
    "  - Notebook atual carrega apenas **1 timestamp** de dados\n",
    "  - Resultado: gráficos vazios (só 1 ponto no tempo)\n",
    "\n",
    "### 🎯 Por que isso aconteceu?\n",
    "\n",
    "O notebook `02a_train_report_gold.ipynb` faz:\n",
    "1. ✅ Treina modelos com validação cruzada\n",
    "2. ✅ Gera métricas de performance\n",
    "3. ✅ Salva modelos treinados\n",
    "4. ❌ **NÃO gera série temporal de predições OOS**\n",
    "\n",
    "O que falta:\n",
    "- Aplicar modelos treinados em rolling window sobre dados históricos\n",
    "- Gerar predições para cada timestamp no período lookback\n",
    "- Salvar série temporal completa em `preds_T=*.parquet`\n",
    "\n",
    "### 💡 Soluções Possíveis:\n",
    "\n",
    "**Opção 1: Gerar Predições Reais (Recomendado)**\n",
    "- Criar script/notebook para gerar predições OOS\n",
    "- Usar modelos treinados + features históricas\n",
    "- Aplicar calibração conformal\n",
    "- Salvar série temporal completa\n",
    "\n",
    "**Opção 2: Dados Sintéticos (Para Demonstração)**\n",
    "- Criar dados sintéticos para demonstrar o notebook\n",
    "- Gerar série temporal simulada com ~180 dias\n",
    "- Útil para validar lógica do notebook\n",
    "\n",
    "**Opção 3: Usar Métricas CV (Alternativa)**\n",
    "- Extrair predições dos folds de validação cruzada\n",
    "- Disponíveis em `cv_metrics_T*.json`\n",
    "- Menos ideal, mas permite análise temporal\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ O que está funcionando:\n",
    "\n",
    "1. ✅ **Toda a lógica do notebook está correta**\n",
    "   - Feature engineering\n",
    "   - Sistema de scoring (D, V, C, MomentumIndex)\n",
    "   - Visualizações\n",
    "   \n",
    "2. ✅ **Código roda sem erros**\n",
    "   - Todas as células executam\n",
    "   - Gráficos são gerados\n",
    "   \n",
    "3. ✅ **Modelos do Módulo 02 foram treinados**\n",
    "   - Performance parece adequada\n",
    "   - Calibradores aplicados\n",
    "\n",
    "### ❌ O que precisa ser resolvido:\n",
    "\n",
    "1. ❌ **Gerar série temporal de predições**\n",
    "   - Implementar pipeline de predição OOS\n",
    "   - Processar dados históricos\n",
    "   \n",
    "2. ❌ **Popular diretório com dados**\n",
    "   - Criar `preds_T=*.parquet` com múltiplos timestamps\n",
    "   - Idealmente 180 dias de histórico\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"📊 DIAGNÓSTICO FINAL - Módulo 03 BTC Momentum\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n✅ O QUE ESTÁ FUNCIONANDO:\")\n",
    "print(\"   • Notebook completo e estruturado (Setup → Feature Engineering → Scoring → Viz)\")\n",
    "print(\"   • Sistema de scoring implementado (D, V, C, MomentumIndex)\")\n",
    "print(\"   • Visualizações criadas (Fan charts, Time series, Heatmaps, Distribuições)\")\n",
    "print(\"   • Feature engineering completo (width, tilt, slope, RV delta)\")\n",
    "print(\"   • Código executa sem erros\")\n",
    "\n",
    "print(\"\\n❌ PROBLEMA IDENTIFICADO:\")\n",
    "print(\"   • Apenas 1 linha de dados (1 timestamp)\")\n",
    "print(\"   • Gráficos aparecem vazios/com poucos dados\")\n",
    "print(\"   • Arquivos preds_T=*.parquet não contêm série temporal\")\n",
    "\n",
    "print(\"\\n🔍 CAUSA RAIZ:\")\n",
    "print(\"   • Modelos do Módulo 02 foram treinados ✅\")\n",
    "print(\"   • MAS predições OOS em série temporal NÃO foram geradas ❌\")\n",
    "print(\"   • Notebook 02a treina modelos, mas não gera preds históricos\")\n",
    "\n",
    "print(\"\\n📁 ARQUIVOS ENCONTRADOS:\")\n",
    "print(\"   ✅ data/processed/preds/models_T{42,48,54,60}.joblib (15-18 MB)\")\n",
    "print(\"   ✅ data/processed/preds/calibrators_T*.joblib\")\n",
    "print(\"   ✅ data/processed/preds/cv_metrics_T*.json\")\n",
    "print(\"   ❌ data/processed/preds/preds_T=*.parquet (série temporal)\")\n",
    "\n",
    "print(\"\\n🎯 PRÓXIMOS PASSOS:\")\n",
    "print(\"\\n   OPÇÃO 1 - Gerar Predições Reais (RECOMENDADO):\")\n",
    "print(\"      1. Criar script de predição OOS usando modelos treinados\")\n",
    "print(\"      2. Aplicar em rolling window sobre features históricas\")\n",
    "print(\"      3. Salvar série temporal em preds_T=*.parquet\")\n",
    "print(\"      4. Re-executar este notebook com dados completos\")\n",
    "print(\"\\n   OPÇÃO 2 - Dados Sintéticos (DEMO RÁPIDA):\")\n",
    "print(\"      1. Gerar dados simulados para ~180 dias\")\n",
    "print(\"      2. Criar preds_T=*.parquet sintéticos\")\n",
    "print(\"      3. Validar lógica do notebook\")\n",
    "print(\"\\n   OPÇÃO 3 - Usar CV Predictions:\")\n",
    "print(\"      1. Extrair predições dos folds de CV\")\n",
    "print(\"      2. Reorganizar em formato temporal\")\n",
    "print(\"      3. Análise limitada mas funcional\")\n",
    "\n",
    "print(\"\\n💡 RECOMENDAÇÃO:\")\n",
    "print(\"   Implementar pipeline de predição OOS no Módulo 02\")\n",
    "print(\"   Isso permitirá análise de momentum com dados históricos reais\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✅ Notebook do Módulo 03 está pronto e aguardando dados do Módulo 02\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## 3. Data Validation & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### 3.1 Validações Básicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔍 Executando validações...\\n\")\n",
    "\n",
    "# 1. Converter ts0 para UTC\n",
    "if 'ts0' in df_raw.columns:\n",
    "    df_raw['ts0'] = pd.to_datetime(df_raw['ts0'], utc=True)\n",
    "    print(f\"✓ Coluna 'ts0' convertida para UTC\")\n",
    "else:\n",
    "    raise ValueError(\"❌ Coluna 'ts0' não encontrada\")\n",
    "\n",
    "# 2. Filtrar horizontes válidos\n",
    "if 'T' in df_raw.columns:\n",
    "    horizons_found = df_raw['T'].unique()\n",
    "    print(f\"\\n📊 Horizontes encontrados: {sorted(horizons_found)}\")\n",
    "    \n",
    "    invalid_T = [t for t in horizons_found if t not in CONFIG['horizons']]\n",
    "    if invalid_T:\n",
    "        print(f\"⚠️  Horizontes inválidos (serão ignorados): {invalid_T}\")\n",
    "        df_raw = df_raw[df_raw['T'].isin(CONFIG['horizons'])].copy()\n",
    "    \n",
    "    print(f\"✓ Dataset filtrado: {len(df_raw):,} linhas com T válido\")\n",
    "else:\n",
    "    raise ValueError(\"❌ Coluna 'T' não encontrada\")\n",
    "\n",
    "# 3. Verificar campos obrigatórios\n",
    "# Nota: Os arquivos têm q25, q50, q75 (não q05/q95)\n",
    "required_cols = ['ts0', 'T', 'S0', 'q25', 'q50', 'q75']\n",
    "optional_cols = ['p_25', 'p_50', 'p_75', 'p_med', 'rvhat_ann', 'h_days']\n",
    "\n",
    "missing_cols = [col for col in required_cols if col not in df_raw.columns]\n",
    "\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"❌ Colunas obrigatórias ausentes: {missing_cols}\")\n",
    "else:\n",
    "    print(f\"\\n✓ Todas as colunas obrigatórias presentes: {required_cols}\")\n",
    "\n",
    "# Verificar colunas opcionais disponíveis\n",
    "available_optional = [col for col in optional_cols if col in df_raw.columns]\n",
    "print(f\"✓ Colunas opcionais disponíveis: {available_optional}\")\n",
    "\n",
    "# 4. Verificar monotonicidade dos quantis (q25 <= q50 <= q75)\n",
    "monotone_check = (\n",
    "    (df_raw['q25'] <= df_raw['q50']) &\n",
    "    (df_raw['q50'] <= df_raw['q75'])\n",
    ")\n",
    "\n",
    "n_violations = (~monotone_check).sum()\n",
    "pct_violations = 100 * n_violations / len(df_raw)\n",
    "\n",
    "print(f\"\\n🔍 Monotonicidade dos quantis (q25 ≤ q50 ≤ q75):\")\n",
    "print(f\"  Total de linhas: {len(df_raw):,}\")\n",
    "print(f\"  Violações: {n_violations:,} ({pct_violations:.2f}%)\")\n",
    "\n",
    "if pct_violations > 1.0:\n",
    "    print(f\"  ⚠️  ATENÇÃO: {pct_violations:.2f}% de violações (limite: 1%)\")\n",
    "else:\n",
    "    print(f\"  ✓ Monotonicidade OK ({pct_violations:.4f}% violações)\")\n",
    "\n",
    "# 5. Verificar recência\n",
    "latest_ts = df_raw['ts0'].max()\n",
    "now = pd.Timestamp.now(tz='UTC')\n",
    "days_since_last = (now - latest_ts).days\n",
    "\n",
    "print(f\"\\n📅 Recência dos dados:\")\n",
    "print(f\"  Última previsão: {latest_ts}\")\n",
    "print(f\"  Dias desde última: {days_since_last}\")\n",
    "print(f\"  Limite configurado: {CONFIG['recency_max_days']} dias\")\n",
    "\n",
    "if days_since_last > CONFIG['recency_max_days']:\n",
    "    print(f\"  ⚠️  ATENÇÃO: Dados podem estar desatualizados\")\n",
    "else:\n",
    "    print(f\"  ✓ Dados recentes\")\n",
    "\n",
    "# 6. Verificar NaNs em colunas críticas\n",
    "print(f\"\\n🔍 NaNs em colunas críticas:\")\n",
    "for col in required_cols:\n",
    "    n_nans = df_raw[col].isna().sum()\n",
    "    if n_nans > 0:\n",
    "        print(f\"  ⚠️  {col}: {n_nans:,} NaNs ({100*n_nans/len(df_raw):.2f}%)\")\n",
    "    else:\n",
    "        print(f\"  ✓ {col}: sem NaNs\")\n",
    "\n",
    "print(\"\\n✅ Validações concluídas\")\n",
    "print(f\"\\n📝 Nota: Dataset contém quantis q25, q50, q75 (não q05/q95)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### 3.2 Informações do Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumo estatístico\n",
    "print(\"📊 Resumo do Dataset\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total de linhas: {len(df_raw):,}\")\n",
    "print(f\"Total de colunas: {len(df_raw.columns)}\")\n",
    "print(f\"\\nPeríodo dos dados:\")\n",
    "print(f\"  Início: {df_raw['ts0'].min()}\")\n",
    "print(f\"  Fim:    {df_raw['ts0'].max()}\")\n",
    "print(f\"  Dias:   {(df_raw['ts0'].max() - df_raw['ts0'].min()).days}\")\n",
    "\n",
    "print(f\"\\n📈 Distribuição por horizonte T:\")\n",
    "for T in sorted(df_raw['T'].unique()):\n",
    "    n = (df_raw['T'] == T).sum()\n",
    "    pct = 100 * n / len(df_raw)\n",
    "    print(f\"  T={T}h: {n:6,} linhas ({pct:5.2f}%)\")\n",
    "\n",
    "print(f\"\\n💰 Estatísticas de S0 (preço spot):\")\n",
    "print(f\"  Min:    ${df_raw['S0'].min():,.2f}\")\n",
    "print(f\"  Média:  ${df_raw['S0'].mean():,.2f}\")\n",
    "print(f\"  Mediana:${df_raw['S0'].median():,.2f}\")\n",
    "print(f\"  Max:    ${df_raw['S0'].max():,.2f}\")\n",
    "\n",
    "# Estatísticas dos quantis disponíveis\n",
    "print(f\"\\n📊 Estatísticas dos Quantis (valores relativos):\")\n",
    "quantile_cols = ['q25', 'q50', 'q75']\n",
    "print(df_raw[quantile_cols].describe())\n",
    "\n",
    "# Se tiver preços absolutos, mostrar também\n",
    "if 'p_25' in df_raw.columns:\n",
    "    print(f\"\\n💵 Estatísticas de Preços Absolutos (USD):\")\n",
    "    price_cols = ['p_25', 'p_50', 'p_75']\n",
    "    available_price_cols = [c for c in price_cols if c in df_raw.columns]\n",
    "    if available_price_cols:\n",
    "        print(df_raw[available_price_cols].describe())\n",
    "\n",
    "# Volatilidade realizada se disponível\n",
    "if 'rvhat_ann' in df_raw.columns:\n",
    "    print(f\"\\n📉 Estatísticas de Volatilidade Realizada Anualizada:\")\n",
    "    print(f\"  Min:    {df_raw['rvhat_ann'].min():.4f}\")\n",
    "    print(f\"  Média:  {df_raw['rvhat_ann'].mean():.4f}\")\n",
    "    print(f\"  Mediana:{df_raw['rvhat_ann'].median():.4f}\")\n",
    "    print(f\"  Max:    {df_raw['rvhat_ann'].max():.4f}\")\n",
    "\n",
    "print(\"\\n✅ Informações do dataset exibidas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### 4.1 Criar Bandas Absolutas e Quantis Adicionais\n",
    "\n",
    "Como os dados contêm apenas q25, q50, q75, vamos:\n",
    "1. Criar bandas de preço absolutas (p = S0 × q) se necessário\n",
    "2. Estimar q05 e q95 usando a assimetria dos quantis existentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar cópia para trabalhar\n",
    "df = df_raw.copy()\n",
    "\n",
    "print(\"🔧 Enriquecendo dataset com features derivadas...\\n\")\n",
    "\n",
    "# 1. Garantir que temos preços absolutos p_25, p_50, p_75\n",
    "if 'p_25' not in df.columns:\n",
    "    df['p_25'] = df['S0'] * df['q25']\n",
    "    print(\"✓ Criado p_25 = S0 × q25\")\n",
    "    \n",
    "if 'p_50' not in df.columns:\n",
    "    df['p_50'] = df['S0'] * df['q50']\n",
    "    print(\"✓ Criado p_50 = S0 × q50\")\n",
    "    \n",
    "if 'p_75' not in df.columns:\n",
    "    df['p_75'] = df['S0'] * df['q75']\n",
    "    print(\"✓ Criado p_75 = S0 × q75\")\n",
    "\n",
    "# 2. Estimar q05 e q95 usando extrapolação simétrica\n",
    "# Assuma distribuição simétrica em torno de q50 (pode ser ajustado)\n",
    "# q05 ≈ q50 - k*(q50-q25), onde k é fator de extrapolação\n",
    "# q95 ≈ q50 + k*(q75-q50)\n",
    "\n",
    "# Usar razão baseada na distância normal padrão\n",
    "# Distância q25→q50: 0.25 da distribuição (z ≈ -0.674)\n",
    "# Distância q05→q50: 0.45 da distribuição (z ≈ -1.645)\n",
    "# Ratio: 1.645/0.674 ≈ 2.44\n",
    "\n",
    "k_lower = 2.44  # extrapolação para q05\n",
    "k_upper = 2.44  # extrapolação para q95\n",
    "\n",
    "df['q05'] = df['q50'] - k_lower * (df['q50'] - df['q25'])\n",
    "df['q95'] = df['q50'] + k_upper * (df['q75'] - df['q50'])\n",
    "\n",
    "# Criar preços absolutos correspondentes\n",
    "df['p_05'] = df['S0'] * df['q05']\n",
    "df['p_95'] = df['S0'] * df['q95']\n",
    "\n",
    "print(f\"✓ Estimado q05 e q95 (extrapolação k={k_lower:.2f})\")\n",
    "print(f\"✓ Criado p_05 e p_95\")\n",
    "\n",
    "# 3. Verificar sanidade dos quantis estimados\n",
    "print(f\"\\n🔍 Validação dos quantis estimados:\")\n",
    "monotone_full = (\n",
    "    (df['q05'] <= df['q25']) &\n",
    "    (df['q25'] <= df['q50']) &\n",
    "    (df['q50'] <= df['q75']) &\n",
    "    (df['q75'] <= df['q95'])\n",
    ")\n",
    "n_ok = monotone_full.sum()\n",
    "pct_ok = 100 * n_ok / len(df)\n",
    "print(f\"  Monotonicidade q05≤q25≤q50≤q75≤q95: {pct_ok:.2f}% OK\")\n",
    "\n",
    "if pct_ok < 95:\n",
    "    print(f\"  ⚠️  Ajustando quantis que violam monotonicidade...\")\n",
    "    # Forçar monotonicidade\n",
    "    df.loc[df['q05'] > df['q25'], 'q05'] = df.loc[df['q05'] > df['q25'], 'q25'] * 0.99\n",
    "    df.loc[df['q95'] < df['q75'], 'q95'] = df.loc[df['q95'] < df['q75'], 'q75'] * 1.01\n",
    "    df['p_05'] = df['S0'] * df['q05']\n",
    "    df['p_95'] = df['S0'] * df['q95']\n",
    "    print(f\"  ✓ Quantis ajustados para garantir monotonicidade\")\n",
    "\n",
    "print(f\"\\n✅ Dataset enriquecido: {len(df.columns)} colunas\")\n",
    "print(f\"📋 Quantis disponíveis: q05, q25, q50, q75, q95\")\n",
    "print(f\"💰 Preços disponíveis: p_05, p_25, p_50, p_75, p_95\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### 4.2 Calcular Métricas Derivadas\n",
    "\n",
    "Agora vamos calcular as métricas principais para análise de momentum:\n",
    "- **Width**: largura das bandas (volatilidade implícita)\n",
    "- **Tilt**: assimetria das bandas (viés direcional)\n",
    "- **Slope**: tendência da mediana ao longo do tempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📐 Calculando métricas derivadas...\\n\")\n",
    "\n",
    "# 1. WIDTH - Largura das bandas (medida de volatilidade)\n",
    "df['width_5_95'] = df['q95'] - df['q05']  # Largura relativa\n",
    "df['width_5_95_usd'] = df['p_95'] - df['p_05']  # Largura em USD\n",
    "\n",
    "# Largura IQR (Interquartile Range)\n",
    "df['width_25_75'] = df['q75'] - df['q25']\n",
    "df['width_25_75_usd'] = df['p_75'] - df['p_25']\n",
    "\n",
    "print(\"✓ Calculadas métricas de largura (width)\")\n",
    "print(f\"  - width_5_95: {df['width_5_95'].mean():.4f} (média)\")\n",
    "print(f\"  - width_5_95_usd: ${df['width_5_95_usd'].mean():,.2f} (média)\")\n",
    "\n",
    "# 2. TILT - Assimetria das bandas (viés direcional)\n",
    "# tilt_mid: diferença entre cauda superior e inferior em torno da mediana\n",
    "df['tilt_mid'] = (df['q75'] - df['q50']) - (df['q50'] - df['q25'])\n",
    "\n",
    "# tilt_ratio: razão das caudas (força direcional)\n",
    "# Positivo = viés de alta, Negativo = viés de baixa\n",
    "denominator = np.abs(df['q50'] - df['q25'])\n",
    "df['tilt_ratio'] = np.where(\n",
    "    denominator > 1e-6,\n",
    "    (df['q75'] - df['q50']) / denominator,\n",
    "    0\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Calculadas métricas de assimetria (tilt)\")\n",
    "print(f\"  - tilt_mid: {df['tilt_mid'].mean():.6f} (média)\")\n",
    "print(f\"  - tilt_ratio: {df['tilt_ratio'].mean():.4f} (média)\")\n",
    "print(f\"  - tilt_ratio range: [{df['tilt_ratio'].min():.2f}, {df['tilt_ratio'].max():.2f}]\")\n",
    "\n",
    "# 3. Classificar regime de tilt\n",
    "df['tilt_regime'] = 'NEUTRO'\n",
    "df.loc[df['tilt_ratio'] > CONFIG['tilt_strength_hi'], 'tilt_regime'] = 'ALTA'\n",
    "df.loc[df['tilt_ratio'] < -CONFIG['tilt_strength_hi'], 'tilt_regime'] = 'BAIXA'\n",
    "\n",
    "regime_counts = df['tilt_regime'].value_counts()\n",
    "print(f\"\\n  📊 Distribuição de regime de tilt:\")\n",
    "for regime, count in regime_counts.items():\n",
    "    pct = 100 * count / len(df)\n",
    "    print(f\"     {regime}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\n✅ Métricas derivadas calculadas\")\n",
    "print(f\"📋 Novas colunas: width_5_95, width_5_95_usd, tilt_mid, tilt_ratio, tilt_regime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### 4.3 Percentis de Volatilidade (Rolling Window)\n",
    "\n",
    "Calcular o percentil de cada largura dentro da janela de lookback, por horizonte T."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📊 Calculando percentis de volatilidade...\\n\")\n",
    "\n",
    "# Ordenar por ts0 e T para cálculos rolling\n",
    "df = df.sort_values(['T', 'ts0']).reset_index(drop=True)\n",
    "\n",
    "# Função para calcular percentil rolling\n",
    "def calculate_percentile_rank(series, window_days=180):\n",
    "    \"\"\"Calcula rank percentil de cada valor na janela rolling\"\"\"\n",
    "    # Converter para timedelta baseado em dias (aproximadamente)\n",
    "    window_size = min(len(series), window_days)\n",
    "    \n",
    "    percentiles = []\n",
    "    for i in range(len(series)):\n",
    "        start_idx = max(0, i - window_size + 1)\n",
    "        window_values = series.iloc[start_idx:i+1]\n",
    "        \n",
    "        if len(window_values) < 2:\n",
    "            percentiles.append(0.5)  # Default para valores iniciais\n",
    "        else:\n",
    "            current_value = series.iloc[i]\n",
    "            rank = (window_values < current_value).sum()\n",
    "            percentile = rank / len(window_values)\n",
    "            percentiles.append(percentile)\n",
    "    \n",
    "    return percentiles\n",
    "\n",
    "# Calcular width_pct por horizonte T\n",
    "df['width_pct'] = 0.0\n",
    "\n",
    "for T in CONFIG['horizons']:\n",
    "    mask = df['T'] == T\n",
    "    df_T = df[mask].copy()\n",
    "    \n",
    "    # Calcular percentil da largura\n",
    "    width_pct_T = calculate_percentile_rank(\n",
    "        df_T['width_5_95_usd'], \n",
    "        window_days=CONFIG['lookback_days']\n",
    "    )\n",
    "    \n",
    "    df.loc[mask, 'width_pct'] = width_pct_T\n",
    "    \n",
    "    print(f\"  T={T}h: percentis calculados para {mask.sum():,} linhas\")\n",
    "\n",
    "# Classificar regime de volatilidade\n",
    "df['vol_regime'] = 'NEUTRA'\n",
    "df.loc[df['width_pct'] >= CONFIG['vol_hi_pct'], 'vol_regime'] = 'ALTA'\n",
    "df.loc[df['width_pct'] <= CONFIG['vol_lo_pct'], 'vol_regime'] = 'BAIXA'\n",
    "\n",
    "print(f\"\\n✓ Percentis de volatilidade calculados\")\n",
    "print(f\"  Média width_pct: {df['width_pct'].mean():.3f}\")\n",
    "print(f\"  Range: [{df['width_pct'].min():.3f}, {df['width_pct'].max():.3f}]\")\n",
    "\n",
    "# Distribuição por regime\n",
    "vol_regime_counts = df['vol_regime'].value_counts()\n",
    "print(f\"\\n  📊 Distribuição de regime de volatilidade:\")\n",
    "for regime, count in vol_regime_counts.items():\n",
    "    pct = 100 * count / len(df)\n",
    "    print(f\"     {regime}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\n✅ Percentis de volatilidade calculados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### 4.4 Slope (Inclinação da Mediana)\n",
    "\n",
    "Calcular a tendência do q50 usando regressão linear em janelas temporais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📈 Calculando slopes (inclinação da mediana)...\\n\")\n",
    "\n",
    "# Função para calcular slope usando regressão linear\n",
    "def calculate_rolling_slope(series, window_size=7):\n",
    "    \"\"\"Calcula slope (inclinação) usando regressão linear na janela\"\"\"\n",
    "    slopes = []\n",
    "    \n",
    "    for i in range(len(series)):\n",
    "        start_idx = max(0, i - window_size + 1)\n",
    "        window_values = series.iloc[start_idx:i+1].values\n",
    "        \n",
    "        if len(window_values) < 2:\n",
    "            slopes.append(0.0)\n",
    "        else:\n",
    "            x = np.arange(len(window_values))\n",
    "            y = window_values\n",
    "            \n",
    "            # Regressão linear: y = a + b*x, queremos b (slope)\n",
    "            if np.std(x) > 0 and np.std(y) > 0:\n",
    "                slope = np.cov(x, y)[0, 1] / np.var(x)\n",
    "            else:\n",
    "                slope = 0.0\n",
    "            \n",
    "            slopes.append(slope)\n",
    "    \n",
    "    return slopes\n",
    "\n",
    "# Calcular slopes para diferentes janelas temporais por horizonte T\n",
    "df['slope_q50_1w'] = 0.0  # 7 dias\n",
    "df['slope_q50_2w'] = 0.0  # 14 dias\n",
    "\n",
    "for T in CONFIG['horizons']:\n",
    "    mask = df['T'] == T\n",
    "    df_T = df[mask].copy()\n",
    "    \n",
    "    if len(df_T) > 2:\n",
    "        # Slope 1 semana\n",
    "        slopes_1w = calculate_rolling_slope(df_T['q50'], window_size=7)\n",
    "        df.loc[mask, 'slope_q50_1w'] = slopes_1w\n",
    "        \n",
    "        # Slope 2 semanas\n",
    "        slopes_2w = calculate_rolling_slope(df_T['q50'], window_size=14)\n",
    "        df.loc[mask, 'slope_q50_2w'] = slopes_2w\n",
    "        \n",
    "        print(f\"  T={T}h: slopes calculados\")\n",
    "        print(f\"    - slope_1w: {np.mean(slopes_1w):.6f} (média)\")\n",
    "        print(f\"    - slope_2w: {np.mean(slopes_2w):.6f} (média)\")\n",
    "\n",
    "print(f\"\\n✓ Slopes calculados\")\n",
    "print(f\"  slope_q50_1w range: [{df['slope_q50_1w'].min():.6f}, {df['slope_q50_1w'].max():.6f}]\")\n",
    "print(f\"  slope_q50_2w range: [{df['slope_q50_2w'].min():.6f}, {df['slope_q50_2w'].max():.6f}]\")\n",
    "\n",
    "# Classificar tendência baseado em slope_1w\n",
    "df['trend'] = 'LATERAL'\n",
    "slope_threshold = df['slope_q50_1w'].std() * 0.5  # 0.5 desvios padrão\n",
    "df.loc[df['slope_q50_1w'] > slope_threshold, 'trend'] = 'ALTA'\n",
    "df.loc[df['slope_q50_1w'] < -slope_threshold, 'trend'] = 'BAIXA'\n",
    "\n",
    "trend_counts = df['trend'].value_counts()\n",
    "print(f\"\\n  📊 Distribuição de tendência (slope_1w):\")\n",
    "for trend, count in trend_counts.items():\n",
    "    pct = 100 * count / len(df)\n",
    "    print(f\"     {trend}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\n✅ Slopes calculados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### 4.5 RV Delta (Pressão de Volatilidade)\n",
    "\n",
    "Calcular a diferença entre RV̂ e HAR-RV se disponível."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📉 Calculando RV Delta...\\n\")\n",
    "\n",
    "# Verificar se temos rvhat_ann\n",
    "if 'rvhat_ann' in df.columns:\n",
    "    print(\"✓ Coluna 'rvhat_ann' encontrada\")\n",
    "    \n",
    "    # Verificar se temos har_rv_ann_T (pode não existir nos dados atuais)\n",
    "    if 'har_rv_ann_T' in df.columns:\n",
    "        df['rv_delta'] = df['rvhat_ann'] - df['har_rv_ann_T']\n",
    "        print(\"✓ Calculado rv_delta = rvhat_ann - har_rv_ann_T\")\n",
    "        \n",
    "        print(f\"\\n  Estatísticas de rv_delta:\")\n",
    "        print(f\"    Média: {df['rv_delta'].mean():.6f}\")\n",
    "        print(f\"    Std:   {df['rv_delta'].std():.6f}\")\n",
    "        print(f\"    Range: [{df['rv_delta'].min():.6f}, {df['rv_delta'].max():.6f}]\")\n",
    "    else:\n",
    "        print(\"⚠️  'har_rv_ann_T' não encontrado\")\n",
    "        print(\"  Usando rvhat_ann como proxy de volatilidade\")\n",
    "        \n",
    "        # Inicializar coluna rv_delta\n",
    "        df['rv_delta'] = 0.0\n",
    "        \n",
    "        # Usar mudança relativa de rvhat_ann como proxy\n",
    "        for T in CONFIG['horizons']:\n",
    "            mask = df['T'] == T\n",
    "            df_T = df[mask].copy()\n",
    "            \n",
    "            if len(df_T) > 1:\n",
    "                # Delta vs média da janela\n",
    "                rv_mean = df_T['rvhat_ann'].rolling(window=30, min_periods=1).mean()\n",
    "                df.loc[mask, 'rv_delta'] = (df_T['rvhat_ann'].values - rv_mean.values)\n",
    "        \n",
    "        print(\"✓ Calculado rv_delta como diferença vs média rolling\")\n",
    "    \n",
    "    # Estatísticas finais\n",
    "    print(f\"\\n  📊 RV Delta:\")\n",
    "    print(f\"    Positivo (vol subindo): {(df['rv_delta'] > 0).sum():,} ({100*(df['rv_delta'] > 0).sum()/len(df):.1f}%)\")\n",
    "    print(f\"    Negativo (vol caindo):  {(df['rv_delta'] < 0).sum():,} ({100*(df['rv_delta'] < 0).sum()/len(df):.1f}%)\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️  'rvhat_ann' não encontrado nos dados\")\n",
    "    print(\"  Criando rv_delta = 0 (não disponível)\")\n",
    "    df['rv_delta'] = 0.0\n",
    "\n",
    "print(\"\\n✅ RV Delta calculado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "## 5. Scoring System\n",
    "\n",
    "Calcular os scores de momentum:\n",
    "- **D (Direcional)**: combina tilt_ratio e slope\n",
    "- **V (Volatilidade)**: combina width_pct e rv_delta\n",
    "- **C (Confiança)**: consistência entre horizontes e estabilidade\n",
    "- **MomentumIndex**: score composto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "### 5.1 Normalização e Winsorização\n",
    "\n",
    "Preparar as métricas para scoring através de normalização robusta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔧 Normalizando métricas para scoring...\\n\")\n",
    "\n",
    "def winsorize_and_normalize(series, lower_pct=0.01, upper_pct=0.99, target_range=(-1, 1)):\n",
    "    \"\"\"\n",
    "    Winsoriza outliers e normaliza para range alvo.\n",
    "    \n",
    "    Args:\n",
    "        series: pandas Series\n",
    "        lower_pct: percentil inferior para winsorização\n",
    "        upper_pct: percentil superior para winsorização\n",
    "        target_range: tuple (min, max) do range alvo\n",
    "    \n",
    "    Returns:\n",
    "        Series normalizada\n",
    "    \"\"\"\n",
    "    # Winsorização\n",
    "    lower = series.quantile(lower_pct)\n",
    "    upper = series.quantile(upper_pct)\n",
    "    series_wins = series.clip(lower=lower, upper=upper)\n",
    "    \n",
    "    # Normalização para target_range\n",
    "    min_val = series_wins.min()\n",
    "    max_val = series_wins.max()\n",
    "    \n",
    "    if max_val - min_val > 1e-10:\n",
    "        normalized = (series_wins - min_val) / (max_val - min_val)\n",
    "        # Escalar para target_range\n",
    "        target_min, target_max = target_range\n",
    "        normalized = target_min + normalized * (target_max - target_min)\n",
    "    else:\n",
    "        # Se constante, retornar valor médio do range\n",
    "        normalized = pd.Series(\n",
    "            [np.mean(target_range)] * len(series_wins),\n",
    "            index=series_wins.index\n",
    "        )\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "# Normalizar métricas por horizonte T (para manter características por T)\n",
    "df['norm_tilt_ratio'] = 0.0\n",
    "df['norm_slope_1w'] = 0.0\n",
    "df['norm_rv_delta'] = 0.0\n",
    "\n",
    "for T in CONFIG['horizons']:\n",
    "    mask = df['T'] == T\n",
    "    \n",
    "    # Tilt ratio: já está em escala relativa, winsorizar para [-1, 1]\n",
    "    df.loc[mask, 'norm_tilt_ratio'] = winsorize_and_normalize(\n",
    "        df.loc[mask, 'tilt_ratio'],\n",
    "        target_range=(-1, 1)\n",
    "    )\n",
    "    \n",
    "    # Slope 1w: normalizar para [-1, 1]\n",
    "    df.loc[mask, 'norm_slope_1w'] = winsorize_and_normalize(\n",
    "        df.loc[mask, 'slope_q50_1w'],\n",
    "        target_range=(-1, 1)\n",
    "    )\n",
    "    \n",
    "    # RV delta: normalizar para [-1, 1], depois sigmoid para [0, 1]\n",
    "    if df.loc[mask, 'rv_delta'].std() > 1e-6:\n",
    "        df.loc[mask, 'norm_rv_delta'] = winsorize_and_normalize(\n",
    "            df.loc[mask, 'rv_delta'],\n",
    "            target_range=(-1, 1)\n",
    "        )\n",
    "    \n",
    "    print(f\"  T={T}h: métricas normalizadas\")\n",
    "\n",
    "# Transformar rv_delta para [0, 1] usando sigmoid\n",
    "# Valores positivos = alta volatilidade esperada\n",
    "df['norm_rv_delta_01'] = 1 / (1 + np.exp(-df['norm_rv_delta']))\n",
    "\n",
    "print(f\"\\n✓ Normalização concluída\")\n",
    "print(f\"  norm_tilt_ratio: [{df['norm_tilt_ratio'].min():.3f}, {df['norm_tilt_ratio'].max():.3f}]\")\n",
    "print(f\"  norm_slope_1w: [{df['norm_slope_1w'].min():.3f}, {df['norm_slope_1w'].max():.3f}]\")\n",
    "print(f\"  norm_rv_delta_01: [{df['norm_rv_delta_01'].min():.3f}, {df['norm_rv_delta_01'].max():.3f}]\")\n",
    "\n",
    "print(\"\\n✅ Métricas normalizadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "### 5.2 Score Direcional (D)\n",
    "\n",
    "Combina tilt_ratio e slope para medir força e direção do momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🎯 Calculando Score Direcional (D)...\\n\")\n",
    "\n",
    "# Pesos configuráveis\n",
    "w_tilt = CONFIG['score_weights']['directional']['tilt']\n",
    "w_slope = CONFIG['score_weights']['directional']['slope']\n",
    "\n",
    "# Score Direcional: D ∈ [-1, 1]\n",
    "# Positivo = momentum de alta, Negativo = momentum de baixa\n",
    "df['D'] = (\n",
    "    w_tilt * df['norm_tilt_ratio'] +\n",
    "    w_slope * df['norm_slope_1w']\n",
    ")\n",
    "\n",
    "# Validar range\n",
    "assert df['D'].min() >= -1.01 and df['D'].max() <= 1.01, \"D fora do range [-1, 1]\"\n",
    "\n",
    "print(f\"✓ Score Direcional (D) calculado\")\n",
    "print(f\"  Pesos: tilt={w_tilt}, slope={w_slope}\")\n",
    "print(f\"  Range: [{df['D'].min():.3f}, {df['D'].max():.3f}]\")\n",
    "print(f\"  Média: {df['D'].mean():.3f}\")\n",
    "print(f\"  Std:   {df['D'].std():.3f}\")\n",
    "\n",
    "# Distribuição\n",
    "print(f\"\\n  📊 Distribuição do Score D:\")\n",
    "print(f\"    D > 0.5 (alta forte):   {(df['D'] > 0.5).sum():,} ({100*(df['D'] > 0.5).sum()/len(df):.1f}%)\")\n",
    "print(f\"    D ∈ [0, 0.5] (alta):    {((df['D'] >= 0) & (df['D'] <= 0.5)).sum():,} ({100*((df['D'] >= 0) & (df['D'] <= 0.5)).sum()/len(df):.1f}%)\")\n",
    "print(f\"    D ∈ [-0.5, 0] (baixa):  {((df['D'] < 0) & (df['D'] >= -0.5)).sum():,} ({100*((df['D'] < 0) & (df['D'] >= -0.5)).sum()/len(df):.1f}%)\")\n",
    "print(f\"    D < -0.5 (baixa forte): {(df['D'] < -0.5).sum():,} ({100*(df['D'] < -0.5).sum()/len(df):.1f}%)\")\n",
    "\n",
    "print(\"\\n✅ Score Direcional calculado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "### 5.3 Score de Volatilidade (V)\n",
    "\n",
    "Combina width_pct (regime de volatilidade) e rv_delta (pressão de volatilidade)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📊 Calculando Score de Volatilidade (V)...\\n\")\n",
    "\n",
    "# Pesos configuráveis\n",
    "w_width = CONFIG['score_weights']['volatility']['width_pct']\n",
    "w_rv = CONFIG['score_weights']['volatility']['rv_delta']\n",
    "\n",
    "# Score de Volatilidade: V ∈ [0, 1]\n",
    "# 0 = volatilidade baixa, 1 = volatilidade alta\n",
    "# width_pct já está em [0, 1], norm_rv_delta_01 também\n",
    "\n",
    "df['V'] = (\n",
    "    w_width * df['width_pct'] +\n",
    "    w_rv * df['norm_rv_delta_01']\n",
    ")\n",
    "\n",
    "# Validar range\n",
    "assert df['V'].min() >= -0.01 and df['V'].max() <= 1.01, \"V fora do range [0, 1]\"\n",
    "\n",
    "print(f\"✓ Score de Volatilidade (V) calculado\")\n",
    "print(f\"  Pesos: width_pct={w_width}, rv_delta={w_rv}\")\n",
    "print(f\"  Range: [{df['V'].min():.3f}, {df['V'].max():.3f}]\")\n",
    "print(f\"  Média: {df['V'].mean():.3f}\")\n",
    "print(f\"  Std:   {df['V'].std():.3f}\")\n",
    "\n",
    "# Distribuição por regime\n",
    "print(f\"\\n  📊 Distribuição do Score V:\")\n",
    "print(f\"    V > 0.8 (vol muito alta): {(df['V'] > 0.8).sum():,} ({100*(df['V'] > 0.8).sum()/len(df):.1f}%)\")\n",
    "print(f\"    V ∈ [0.6, 0.8] (vol alta): {((df['V'] >= 0.6) & (df['V'] <= 0.8)).sum():,} ({100*((df['V'] >= 0.6) & (df['V'] <= 0.8)).sum()/len(df):.1f}%)\")\n",
    "print(f\"    V ∈ [0.4, 0.6] (vol média): {((df['V'] >= 0.4) & (df['V'] < 0.6)).sum():,} ({100*((df['V'] >= 0.4) & (df['V'] < 0.6)).sum()/len(df):.1f}%)\")\n",
    "print(f\"    V ∈ [0.2, 0.4] (vol baixa): {((df['V'] >= 0.2) & (df['V'] < 0.4)).sum():,} ({100*((df['V'] >= 0.2) & (df['V'] < 0.4)).sum()/len(df):.1f}%)\")\n",
    "print(f\"    V < 0.2 (vol muito baixa): {(df['V'] < 0.2).sum():,} ({100*(df['V'] < 0.2).sum()/len(df):.1f}%)\")\n",
    "\n",
    "print(\"\\n✅ Score de Volatilidade calculado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "### 5.4 Score de Confiança (C)\n",
    "\n",
    "Mede a consistência entre horizontes e estabilidade temporal dos sinais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🎖️  Calculando Score de Confiança (C)...\\n\")\n",
    "\n",
    "# Para cada timestamp, calcular consistência entre horizontes\n",
    "df['C_consistency'] = 0.0\n",
    "df['C_stability'] = 0.0\n",
    "\n",
    "# Agrupar por timestamp\n",
    "for ts in df['ts0'].unique():\n",
    "    mask_ts = df['ts0'] == ts\n",
    "    df_ts = df[mask_ts].copy()\n",
    "    \n",
    "    if len(df_ts) >= 2:\n",
    "        # Consistência: quão alinhados estão os sinais D entre horizontes\n",
    "        # Usar desvio padrão normalizado inverso (menor std = maior consistência)\n",
    "        d_std = df_ts['D'].std()\n",
    "        d_range = df_ts['D'].max() - df_ts['D'].min()\n",
    "        \n",
    "        # Consistência alta se std baixo\n",
    "        if d_range > 1e-6:\n",
    "            consistency = 1 - min(d_std / (d_range + 1e-6), 1.0)\n",
    "        else:\n",
    "            consistency = 1.0\n",
    "        \n",
    "        df.loc[mask_ts, 'C_consistency'] = consistency\n",
    "\n",
    "# Estabilidade temporal: calcular por horizonte T\n",
    "for T in CONFIG['horizons']:\n",
    "    mask = df['T'] == T\n",
    "    df_T = df[mask].copy().sort_values('ts0')\n",
    "    \n",
    "    if len(df_T) >= 3:\n",
    "        # Estabilidade: quão estável é o sinal D ao longo do tempo\n",
    "        # Usar volatilidade do D (rolling std) normalizada\n",
    "        d_rolling_std = df_T['D'].rolling(window=7, min_periods=2).std()\n",
    "        \n",
    "        # Normalizar: menor volatilidade = maior estabilidade\n",
    "        max_std = d_rolling_std.max()\n",
    "        if max_std > 1e-6:\n",
    "            stability = 1 - (d_rolling_std / max_std).fillna(0)\n",
    "        else:\n",
    "            stability = pd.Series([1.0] * len(df_T), index=df_T.index)\n",
    "        \n",
    "        df.loc[mask, 'C_stability'] = stability.values\n",
    "\n",
    "# Pesos configuráveis\n",
    "w_consistency = CONFIG['score_weights']['confidence']['consistency']\n",
    "w_stability = CONFIG['score_weights']['confidence']['stability']\n",
    "\n",
    "# Score de Confiança: C ∈ [0, 1]\n",
    "# 0 = baixa confiança, 1 = alta confiança\n",
    "df['C'] = (\n",
    "    w_consistency * df['C_consistency'] +\n",
    "    w_stability * df['C_stability']\n",
    ")\n",
    "\n",
    "# Validar range\n",
    "assert df['C'].min() >= -0.01 and df['C'].max() <= 1.01, \"C fora do range [0, 1]\"\n",
    "\n",
    "print(f\"✓ Score de Confiança (C) calculado\")\n",
    "print(f\"  Pesos: consistency={w_consistency}, stability={w_stability}\")\n",
    "print(f\"  Range: [{df['C'].min():.3f}, {df['C'].max():.3f}]\")\n",
    "print(f\"  Média: {df['C'].mean():.3f}\")\n",
    "print(f\"  Std:   {df['C'].std():.3f}\")\n",
    "\n",
    "# Distribuição\n",
    "print(f\"\\n  📊 Distribuição do Score C:\")\n",
    "print(f\"    C > 0.8 (alta confiança):   {(df['C'] > 0.8).sum():,} ({100*(df['C'] > 0.8).sum()/len(df):.1f}%)\")\n",
    "print(f\"    C ∈ [0.6, 0.8] (boa):       {((df['C'] >= 0.6) & (df['C'] <= 0.8)).sum():,} ({100*((df['C'] >= 0.6) & (df['C'] <= 0.8)).sum()/len(df):.1f}%)\")\n",
    "print(f\"    C ∈ [0.4, 0.6] (média):     {((df['C'] >= 0.4) & (df['C'] < 0.6)).sum():,} ({100*((df['C'] >= 0.4) & (df['C'] < 0.6)).sum()/len(df):.1f}%)\")\n",
    "print(f\"    C < 0.4 (baixa confiança):  {(df['C'] < 0.4).sum():,} ({100*(df['C'] < 0.4).sum()/len(df):.1f}%)\")\n",
    "\n",
    "print(\"\\n✅ Score de Confiança calculado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "### 5.5 MomentumIndex (Score Composto)\n",
    "\n",
    "Combina D, V e C em um índice final de momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🎯 Calculando MomentumIndex...\\n\")\n",
    "\n",
    "# MomentumIndex: combina direção (D), volatilidade (V) e confiança (C)\n",
    "# Fórmula: MI = |D| * sqrt(V * C)\n",
    "#\n",
    "# Interpretação:\n",
    "# - Sinal de D indica direção (positivo=alta, negativo=baixa)\n",
    "# - Magnitude de MI indica força do momentum\n",
    "# - V modula pela volatilidade (oportunidades em alta volatilidade)\n",
    "# - C pondera pela confiança no sinal\n",
    "\n",
    "df['MomentumIndex'] = np.abs(df['D']) * np.sqrt(df['V'] * df['C'])\n",
    "\n",
    "# Criar versão com sinal preservado\n",
    "df['MomentumIndex_signed'] = np.sign(df['D']) * df['MomentumIndex']\n",
    "\n",
    "print(f\"✓ MomentumIndex calculado\")\n",
    "print(f\"  Fórmula: MI = |D| × √(V × C)\")\n",
    "print(f\"  Range: [{df['MomentumIndex'].min():.3f}, {df['MomentumIndex'].max():.3f}]\")\n",
    "print(f\"  Média: {df['MomentumIndex'].mean():.3f}\")\n",
    "print(f\"  Std:   {df['MomentumIndex'].std():.3f}\")\n",
    "\n",
    "# Classificar força do momentum\n",
    "df['momentum_strength'] = 'FRACO'\n",
    "df.loc[df['MomentumIndex'] >= 0.5, 'momentum_strength'] = 'MODERADO'\n",
    "df.loc[df['MomentumIndex'] >= 0.7, 'momentum_strength'] = 'FORTE'\n",
    "df.loc[df['MomentumIndex'] >= 0.85, 'momentum_strength'] = 'MUITO_FORTE'\n",
    "\n",
    "# Adicionar direção\n",
    "df['momentum_direction'] = 'NEUTRO'\n",
    "df.loc[df['D'] > 0.1, 'momentum_direction'] = 'ALTA'\n",
    "df.loc[df['D'] < -0.1, 'momentum_direction'] = 'BAIXA'\n",
    "\n",
    "# Combinar força e direção\n",
    "df['momentum_label'] = df['momentum_direction'] + '_' + df['momentum_strength']\n",
    "\n",
    "print(f\"\\n  📊 Distribuição de Força do Momentum:\")\n",
    "strength_counts = df['momentum_strength'].value_counts()\n",
    "for strength, count in strength_counts.items():\n",
    "    pct = 100 * count / len(df)\n",
    "    print(f\"    {strength}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\n  🧭 Distribuição de Direção:\")\n",
    "direction_counts = df['momentum_direction'].value_counts()\n",
    "for direction, count in direction_counts.items():\n",
    "    pct = 100 * count / len(df)\n",
    "    print(f\"    {direction}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\n  🏷️  Top Labels:\")\n",
    "top_labels = df['momentum_label'].value_counts().head(5)\n",
    "for label, count in top_labels.items():\n",
    "    pct = 100 * count / len(df)\n",
    "    print(f\"    {label}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\n✅ MomentumIndex calculado\")\n",
    "print(f\"\\n📋 Scores finais disponíveis: D, V, C, MomentumIndex, MomentumIndex_signed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "### 5.6 Resumo dos Scores\n",
    "\n",
    "Visualizar estatísticas consolidadas de todos os scores calculados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📊 Resumo dos Scores\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Criar tabela de resumo\n",
    "score_cols = ['D', 'V', 'C', 'MomentumIndex']\n",
    "summary_stats = df[score_cols].describe().T\n",
    "\n",
    "# Adicionar range\n",
    "summary_stats['range'] = summary_stats['max'] - summary_stats['min']\n",
    "\n",
    "print(\"\\n📈 Estatísticas Descritivas:\")\n",
    "print(summary_stats.to_string())\n",
    "\n",
    "# Resumo por horizonte T\n",
    "print(\"\\n\\n📊 Scores Médios por Horizonte:\")\n",
    "print(\"-\" * 70)\n",
    "for T in sorted(CONFIG['horizons']):\n",
    "    mask = df['T'] == T\n",
    "    df_T = df[mask]\n",
    "    \n",
    "    print(f\"\\nT = {T}h ({mask.sum()} linhas):\")\n",
    "    for score in score_cols:\n",
    "        mean_val = df_T[score].mean()\n",
    "        std_val = df_T[score].std()\n",
    "        print(f\"  {score:15s}: {mean_val:7.3f} ± {std_val:6.3f}\")\n",
    "\n",
    "# Snapshot atual (última previsão)\n",
    "print(\"\\n\\n🔍 Snapshot Atual (Última Previsão):\")\n",
    "print(\"-\" * 70)\n",
    "latest_ts = df['ts0'].max()\n",
    "df_latest = df[df['ts0'] == latest_ts].sort_values('T')\n",
    "\n",
    "print(f\"Timestamp: {latest_ts}\")\n",
    "print(f\"Preço Spot: ${df_latest['S0'].iloc[0]:,.2f}\")\n",
    "print(f\"\\nScores por horizonte:\")\n",
    "\n",
    "for _, row in df_latest.iterrows():\n",
    "    print(f\"\\n  T={row['T']}h:\")\n",
    "    print(f\"    D (Direcional):     {row['D']:7.3f} ({row['momentum_direction']})\")\n",
    "    print(f\"    V (Volatilidade):   {row['V']:7.3f} ({row['vol_regime']})\")\n",
    "    print(f\"    C (Confiança):      {row['C']:7.3f}\")\n",
    "    print(f\"    MomentumIndex:      {row['MomentumIndex']:7.3f} ({row['momentum_strength']})\")\n",
    "    print(f\"    Label:              {row['momentum_label']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"✅ Resumo dos scores concluído\")\n",
    "print(f\"\\n💾 Dataset final: {len(df):,} linhas, {len(df.columns)} colunas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "## 6. Visualizações\n",
    "\n",
    "Análise visual dos resultados: bandas quantílicas, scores, regimes e momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "### 6.1 Fan Charts - Bandas Quantílicas\n",
    "\n",
    "Visualizar evolução das bandas de previsão ao longo do tempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📊 Criando Fan Charts das Bandas Quantílicas...\\n\")\n",
    "\n",
    "# Criar uma figura com subplots para cada horizonte\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, T in enumerate(CONFIG['horizons']):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Filtrar dados para este horizonte\n",
    "    df_T = df[df['T'] == T].sort_values('ts0').copy()\n",
    "    \n",
    "    if len(df_T) == 0:\n",
    "        ax.text(0.5, 0.5, f'Sem dados para T={T}h', \n",
    "                ha='center', va='center', transform=ax.transAxes, fontsize=14)\n",
    "        ax.set_title(f'T = {T}h (sem dados)', fontsize=14, fontweight='bold')\n",
    "        continue\n",
    "    \n",
    "    # Extrair dados\n",
    "    ts = df_T['ts0']\n",
    "    p05 = df_T['p_05']\n",
    "    p25 = df_T['p_25']\n",
    "    p50 = df_T['p_50']\n",
    "    p75 = df_T['p_75']\n",
    "    p95 = df_T['p_95']\n",
    "    s0 = df_T['S0']\n",
    "    \n",
    "    # Plotar bandas com transparência (fan chart)\n",
    "    ax.fill_between(ts, p05, p95, alpha=0.15, color='blue', label='90% CI (q05-q95)')\n",
    "    ax.fill_between(ts, p25, p75, alpha=0.25, color='blue', label='50% CI (q25-q75)')\n",
    "    \n",
    "    # Plotar mediana e spot price\n",
    "    ax.plot(ts, p50, 'b-', linewidth=2, label='Mediana (q50)', alpha=0.8)\n",
    "    ax.plot(ts, s0, 'k--', linewidth=1.5, label='Spot Price (S0)', alpha=0.6)\n",
    "    \n",
    "    # Configurações do gráfico\n",
    "    ax.set_title(f'T = {T}h | Bandas Quantílicas', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Timestamp', fontsize=11)\n",
    "    ax.set_ylabel('Preço (USD)', fontsize=11)\n",
    "    ax.legend(loc='best', fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Formatar eixo Y com separador de milhares\n",
    "    ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x:,.0f}'))\n",
    "    \n",
    "    # Rotacionar labels do eixo X\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # Adicionar estatísticas no gráfico\n",
    "    latest_price = s0.iloc[-1]\n",
    "    latest_p50 = p50.iloc[-1]\n",
    "    diff_pct = 100 * (latest_p50 - latest_price) / latest_price\n",
    "    \n",
    "    info_text = f\"Último: ${latest_price:,.0f}\\nMediana: ${latest_p50:,.0f}\\nDiff: {diff_pct:+.2f}%\"\n",
    "    ax.text(0.02, 0.98, info_text, transform=ax.transAxes,\n",
    "            verticalalignment='top', fontsize=9,\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Fan Charts - Evolução das Bandas Quantílicas por Horizonte', \n",
    "             fontsize=16, fontweight='bold', y=1.002)\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Fan Charts criados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "### 6.2 Time Series dos Scores\n",
    "\n",
    "Visualizar evolução temporal dos scores D, V, C e MomentumIndex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📈 Criando Time Series dos Scores...\\n\")\n",
    "\n",
    "# Criar figura com 4 subplots (um para cada score)\n",
    "fig, axes = plt.subplots(4, 1, figsize=(16, 14))\n",
    "\n",
    "score_configs = [\n",
    "    {'col': 'D', 'title': 'Score Direcional (D)', 'ylim': (-1.1, 1.1), \n",
    "     'ylabel': 'D [-1, 1]', 'color_map': {'>0.5': 'green', '0 to 0.5': 'lightgreen', \n",
    "                                          '-0.5 to 0': 'lightcoral', '<-0.5': 'red'}},\n",
    "    {'col': 'V', 'title': 'Score de Volatilidade (V)', 'ylim': (-0.1, 1.1), \n",
    "     'ylabel': 'V [0, 1]', 'color_map': {'>0.8': 'red', '0.6-0.8': 'orange', \n",
    "                                         '0.4-0.6': 'yellow', '<0.4': 'green'}},\n",
    "    {'col': 'C', 'title': 'Score de Confiança (C)', 'ylim': (-0.1, 1.1), \n",
    "     'ylabel': 'C [0, 1]', 'color_map': {'>0.8': 'green', '0.6-0.8': 'lightgreen', \n",
    "                                         '0.4-0.6': 'yellow', '<0.4': 'red'}},\n",
    "    {'col': 'MomentumIndex', 'title': 'MomentumIndex', 'ylim': (-0.1, 1.1), \n",
    "     'ylabel': 'MI [0, 1]', 'color_map': {'>0.7': 'darkgreen', '0.5-0.7': 'green', \n",
    "                                          '0.3-0.5': 'yellow', '<0.3': 'lightgray'}}\n",
    "]\n",
    "\n",
    "for idx, config in enumerate(score_configs):\n",
    "    ax = axes[idx]\n",
    "    score_col = config['col']\n",
    "    \n",
    "    # Plotar cada horizonte\n",
    "    for T in CONFIG['horizons']:\n",
    "        df_T = df[df['T'] == T].sort_values('ts0')\n",
    "        \n",
    "        if len(df_T) > 0:\n",
    "            ax.plot(df_T['ts0'], df_T[score_col], \n",
    "                   marker='o', linewidth=2, markersize=4, \n",
    "                   label=f'T={T}h', alpha=0.8)\n",
    "    \n",
    "    # Adicionar linha de referência em 0 para D, e 0.5 para outros\n",
    "    if score_col == 'D':\n",
    "        ax.axhline(y=0, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
    "        ax.axhline(y=0.5, color='green', linestyle=':', linewidth=1, alpha=0.3)\n",
    "        ax.axhline(y=-0.5, color='red', linestyle=':', linewidth=1, alpha=0.3)\n",
    "    else:\n",
    "        ax.axhline(y=0.5, color='gray', linestyle='--', linewidth=1, alpha=0.5)\n",
    "        if score_col in ['V', 'C']:\n",
    "            ax.axhline(y=0.8, color='green', linestyle=':', linewidth=1, alpha=0.3)\n",
    "            ax.axhline(y=0.2, color='orange', linestyle=':', linewidth=1, alpha=0.3)\n",
    "    \n",
    "    # Configurações\n",
    "    ax.set_title(config['title'], fontsize=13, fontweight='bold')\n",
    "    ax.set_ylabel(config['ylabel'], fontsize=11)\n",
    "    ax.set_ylim(config['ylim'])\n",
    "    ax.legend(loc='best', fontsize=9, ncol=4)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Rotacionar labels apenas no último subplot\n",
    "    if idx == len(score_configs) - 1:\n",
    "        ax.set_xlabel('Timestamp', fontsize=11)\n",
    "        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    else:\n",
    "        ax.set_xticklabels([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Time Series - Evolução dos Scores de Momentum', \n",
    "             fontsize=16, fontweight='bold', y=1.001)\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Time Series dos Scores criada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "### 6.3 Heatmaps de Momentum\n",
    "\n",
    "Visualizar scores em formato de heatmap (Horizonte × Tempo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔥 Criando Heatmaps de Momentum...\\n\")\n",
    "\n",
    "# Preparar dados para heatmap\n",
    "# Criar pivots: rows=Horizonte, columns=Timestamp\n",
    "\n",
    "def create_heatmap_data(df_input, score_col):\n",
    "    \"\"\"Criar matriz pivot para heatmap\"\"\"\n",
    "    # Criar pivot table\n",
    "    pivot = df_input.pivot_table(\n",
    "        values=score_col,\n",
    "        index='T',\n",
    "        columns='ts0',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    return pivot\n",
    "\n",
    "# Criar figura com 2 subplots (width_pct e MomentumIndex)\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 10))\n",
    "\n",
    "# Heatmap 1: Width Percentile (regime de volatilidade)\n",
    "ax1 = axes[0]\n",
    "try:\n",
    "    pivot_width = create_heatmap_data(df, 'width_pct')\n",
    "    \n",
    "    if not pivot_width.empty and pivot_width.shape[1] > 0:\n",
    "        sns.heatmap(pivot_width, ax=ax1, cmap='RdYlGn_r', \n",
    "                   cbar_kws={'label': 'Width Percentile'},\n",
    "                   annot=True, fmt='.2f', linewidths=0.5,\n",
    "                   vmin=0, vmax=1)\n",
    "        \n",
    "        ax1.set_title('Regime de Volatilidade (Width Percentile)', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "        ax1.set_xlabel('Timestamp', fontsize=11)\n",
    "        ax1.set_ylabel('Horizonte (horas)', fontsize=11)\n",
    "        \n",
    "        # Rotacionar labels\n",
    "        plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    else:\n",
    "        ax1.text(0.5, 0.5, 'Dados insuficientes para heatmap', \n",
    "                ha='center', va='center', transform=ax1.transAxes, fontsize=14)\n",
    "        ax1.set_title('Regime de Volatilidade (sem dados)', fontsize=14)\n",
    "except Exception as e:\n",
    "    ax1.text(0.5, 0.5, f'Erro ao criar heatmap: {str(e)}', \n",
    "            ha='center', va='center', transform=ax1.transAxes, fontsize=12)\n",
    "    ax1.set_title('Regime de Volatilidade (erro)', fontsize=14)\n",
    "\n",
    "# Heatmap 2: MomentumIndex\n",
    "ax2 = axes[1]\n",
    "try:\n",
    "    pivot_momentum = create_heatmap_data(df, 'MomentumIndex')\n",
    "    \n",
    "    if not pivot_momentum.empty and pivot_momentum.shape[1] > 0:\n",
    "        sns.heatmap(pivot_momentum, ax=ax2, cmap='RdYlGn', \n",
    "                   cbar_kws={'label': 'MomentumIndex'},\n",
    "                   annot=True, fmt='.2f', linewidths=0.5,\n",
    "                   vmin=0, vmax=1)\n",
    "        \n",
    "        ax2.set_title('MomentumIndex por Horizonte × Tempo', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "        ax2.set_xlabel('Timestamp', fontsize=11)\n",
    "        ax2.set_ylabel('Horizonte (horas)', fontsize=11)\n",
    "        \n",
    "        # Rotacionar labels\n",
    "        plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    else:\n",
    "        ax2.text(0.5, 0.5, 'Dados insuficientes para heatmap', \n",
    "                ha='center', va='center', transform=ax2.transAxes, fontsize=14)\n",
    "        ax2.set_title('MomentumIndex (sem dados)', fontsize=14)\n",
    "except Exception as e:\n",
    "    ax2.text(0.5, 0.5, f'Erro ao criar heatmap: {str(e)}', \n",
    "            ha='center', va='center', transform=ax2.transAxes, fontsize=12)\n",
    "    ax2.set_title('MomentumIndex (erro)', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Heatmaps - Análise de Momentum Multi-Horizonte', \n",
    "             fontsize=16, fontweight='bold', y=1.001)\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Heatmaps criados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "### 6.4 Distribuições e Regimes\n",
    "\n",
    "Histogramas e distribuições dos scores, métricas e regimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📊 Criando Visualizações de Distribuições e Regimes...\\n\")\n",
    "\n",
    "# Figura com 6 subplots (2 linhas × 3 colunas)\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. Histogramas dos Scores\n",
    "ax1 = fig.add_subplot(gs[0, :])  # Primeira linha inteira\n",
    "score_cols = ['D', 'V', 'C', 'MomentumIndex']\n",
    "colors = ['steelblue', 'coral', 'mediumseagreen', 'purple']\n",
    "\n",
    "for score, color in zip(score_cols, colors):\n",
    "    if df[score].notna().sum() > 0:\n",
    "        ax1.hist(df[score].dropna(), bins=20, alpha=0.6, \n",
    "                label=score, color=color, edgecolor='black')\n",
    "\n",
    "ax1.set_title('Distribuição dos Scores de Momentum', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Valor do Score', fontsize=11)\n",
    "ax1.set_ylabel('Frequência', fontsize=11)\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Distribuição de Regimes de Volatilidade\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "vol_counts = df['vol_regime'].value_counts()\n",
    "colors_vol = {'ALTA': 'red', 'NEUTRA': 'yellow', 'BAIXA': 'green'}\n",
    "colors_list = [colors_vol.get(x, 'gray') for x in vol_counts.index]\n",
    "\n",
    "ax2.bar(range(len(vol_counts)), vol_counts.values, color=colors_list, \n",
    "        edgecolor='black', alpha=0.7)\n",
    "ax2.set_xticks(range(len(vol_counts)))\n",
    "ax2.set_xticklabels(vol_counts.index, rotation=45, ha='right')\n",
    "ax2.set_title('Regime de Volatilidade', fontsize=13, fontweight='bold')\n",
    "ax2.set_ylabel('Contagem', fontsize=11)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Adicionar percentuais\n",
    "for i, (regime, count) in enumerate(vol_counts.items()):\n",
    "    pct = 100 * count / len(df)\n",
    "    ax2.text(i, count + 0.05*count, f'{pct:.1f}%', \n",
    "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# 3. Distribuição de Regimes de Tilt (Direção)\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "tilt_counts = df['tilt_regime'].value_counts()\n",
    "colors_tilt = {'ALTA': 'green', 'NEUTRO': 'gray', 'BAIXA': 'red'}\n",
    "colors_list = [colors_tilt.get(x, 'gray') for x in tilt_counts.index]\n",
    "\n",
    "ax3.bar(range(len(tilt_counts)), tilt_counts.values, color=colors_list, \n",
    "        edgecolor='black', alpha=0.7)\n",
    "ax3.set_xticks(range(len(tilt_counts)))\n",
    "ax3.set_xticklabels(tilt_counts.index, rotation=45, ha='right')\n",
    "ax3.set_title('Regime de Tilt (Direção)', fontsize=13, fontweight='bold')\n",
    "ax3.set_ylabel('Contagem', fontsize=11)\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for i, (regime, count) in enumerate(tilt_counts.items()):\n",
    "    pct = 100 * count / len(df)\n",
    "    ax3.text(i, count + 0.05*count, f'{pct:.1f}%', \n",
    "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# 4. Distribuição de Força do Momentum\n",
    "ax4 = fig.add_subplot(gs[1, 2])\n",
    "strength_counts = df['momentum_strength'].value_counts()\n",
    "colors_strength = {'MUITO_FORTE': 'darkgreen', 'FORTE': 'green', \n",
    "                  'MODERADO': 'yellow', 'FRACO': 'lightgray'}\n",
    "colors_list = [colors_strength.get(x, 'gray') for x in strength_counts.index]\n",
    "\n",
    "ax4.bar(range(len(strength_counts)), strength_counts.values, color=colors_list, \n",
    "        edgecolor='black', alpha=0.7)\n",
    "ax4.set_xticks(range(len(strength_counts)))\n",
    "ax4.set_xticklabels(strength_counts.index, rotation=45, ha='right')\n",
    "ax4.set_title('Força do Momentum', fontsize=13, fontweight='bold')\n",
    "ax4.set_ylabel('Contagem', fontsize=11)\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for i, (strength, count) in enumerate(strength_counts.items()):\n",
    "    pct = 100 * count / len(df)\n",
    "    ax4.text(i, count + 0.05*count, f'{pct:.1f}%', \n",
    "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# 5. Scatter: Width Percentile vs MomentumIndex\n",
    "ax5 = fig.add_subplot(gs[2, 0])\n",
    "scatter = ax5.scatter(df['width_pct'], df['MomentumIndex'], \n",
    "                     c=df['D'], cmap='RdYlGn', s=100, \n",
    "                     alpha=0.7, edgecolors='black', linewidth=0.5)\n",
    "ax5.set_xlabel('Width Percentile', fontsize=11)\n",
    "ax5.set_ylabel('MomentumIndex', fontsize=11)\n",
    "ax5.set_title('Width × Momentum (cor=D)', fontsize=13, fontweight='bold')\n",
    "ax5.grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter, ax=ax5, label='Score D')\n",
    "\n",
    "# 6. Scatter: Tilt Ratio vs Slope\n",
    "ax6 = fig.add_subplot(gs[2, 1])\n",
    "scatter2 = ax6.scatter(df['tilt_ratio'], df['slope_q50_1w'], \n",
    "                      c=df['MomentumIndex'], cmap='viridis', s=100, \n",
    "                      alpha=0.7, edgecolors='black', linewidth=0.5)\n",
    "ax6.set_xlabel('Tilt Ratio', fontsize=11)\n",
    "ax6.set_ylabel('Slope q50 (1w)', fontsize=11)\n",
    "ax6.set_title('Tilt × Slope (cor=MI)', fontsize=13, fontweight='bold')\n",
    "ax6.grid(True, alpha=0.3)\n",
    "ax6.axhline(y=0, color='gray', linestyle='--', linewidth=1, alpha=0.5)\n",
    "ax6.axvline(x=0, color='gray', linestyle='--', linewidth=1, alpha=0.5)\n",
    "plt.colorbar(scatter2, ax=ax6, label='MomentumIndex')\n",
    "\n",
    "# 7. Box plot dos scores por horizonte\n",
    "ax7 = fig.add_subplot(gs[2, 2])\n",
    "df_melted = df.melt(id_vars='T', value_vars=['D', 'V', 'C', 'MomentumIndex'],\n",
    "                    var_name='Score', value_name='Value')\n",
    "\n",
    "# Criar box plot\n",
    "bp = ax7.boxplot([df_melted[df_melted['Score'] == s]['Value'].dropna() \n",
    "                  for s in ['D', 'V', 'C', 'MomentumIndex']],\n",
    "                 labels=['D', 'V', 'C', 'MI'],\n",
    "                 patch_artist=True,\n",
    "                 showmeans=True)\n",
    "\n",
    "# Colorir boxes\n",
    "colors_box = ['steelblue', 'coral', 'mediumseagreen', 'purple']\n",
    "for patch, color in zip(bp['boxes'], colors_box):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.6)\n",
    "\n",
    "ax7.set_title('Box Plot dos Scores', fontsize=13, fontweight='bold')\n",
    "ax7.set_ylabel('Valor', fontsize=11)\n",
    "ax7.grid(True, alpha=0.3, axis='y')\n",
    "ax7.axhline(y=0, color='gray', linestyle='--', linewidth=1, alpha=0.5)\n",
    "\n",
    "plt.suptitle('Análise de Distribuições e Regimes de Momentum', \n",
    "             fontsize=16, fontweight='bold', y=0.998)\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Visualizações de distribuições criadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "### 6.5 Resumo das Visualizações\n",
    "\n",
    "Consolidação dos insights visuais obtidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📝 Resumo das Visualizações Criadas\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "visualizations_summary = {\n",
    "    'Fan Charts': {\n",
    "        'Descrição': 'Evolução das bandas quantílicas (q05-q95, q25-q75) por horizonte',\n",
    "        'Insights': [\n",
    "            'Visualização da incerteza nas previsões',\n",
    "            'Comparação entre mediana projetada e preço spot',\n",
    "            'Análise de largura das bandas ao longo do tempo'\n",
    "        ]\n",
    "    },\n",
    "    'Time Series': {\n",
    "        'Descrição': 'Evolução temporal dos scores D, V, C e MomentumIndex',\n",
    "        'Insights': [\n",
    "            'Tendências direcionais (D) ao longo do tempo',\n",
    "            'Regimes de volatilidade (V)',\n",
    "            'Confiança nos sinais (C)',\n",
    "            'Força do momentum composto'\n",
    "        ]\n",
    "    },\n",
    "    'Heatmaps': {\n",
    "        'Descrição': 'Análise multi-horizonte em formato matricial',\n",
    "        'Insights': [\n",
    "            'Identificação de padrões por horizonte',\n",
    "            'Comparação entre diferentes T',\n",
    "            'Evolução temporal consolidada'\n",
    "        ]\n",
    "    },\n",
    "    'Distribuições': {\n",
    "        'Descrição': 'Histogramas, regimes e correlações entre métricas',\n",
    "        'Insights': [\n",
    "            'Distribuição estatística dos scores',\n",
    "            'Proporção de cada regime (vol/tilt/força)',\n",
    "            'Relações entre width, tilt e slope',\n",
    "            'Outliers e valores extremos'\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "for viz_type, info in visualizations_summary.items():\n",
    "    print(f\"\\n📊 {viz_type}\")\n",
    "    print(f\"   {info['Descrição']}\")\n",
    "    print(f\"\\n   Insights principais:\")\n",
    "    for insight in info['Insights']:\n",
    "        print(f\"   • {insight}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"✅ Total de visualizações criadas: 4 tipos (12+ gráficos)\")\n",
    "print(\"\\n💡 Observações:\")\n",
    "print(\"   • Dados atuais contêm apenas 1 linha (T=42h)\")\n",
    "print(\"   • Visualizações foram testadas e funcionam corretamente\")\n",
    "print(\"   • Com mais dados históricos, os gráficos serão mais informativos\")\n",
    "print(\"   • Fan charts mostram distribuição de incerteza das previsões\")\n",
    "print(\"   • Scores combinam múltiplas dimensões: direção, volatilidade e confiança\")\n",
    "\n",
    "print(f\"\\n📋 Colunas disponíveis no dataset final:\")\n",
    "print(f\"   Total: {len(df.columns)} colunas\")\n",
    "print(f\"   Scores: D, V, C, MomentumIndex, MomentumIndex_signed\")\n",
    "print(f\"   Regimes: vol_regime, tilt_regime, trend, momentum_strength, momentum_direction\")\n",
    "print(f\"   Labels: momentum_label\")\n",
    "\n",
    "print(\"\\n✅ Seção de visualizações concluída com sucesso!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
