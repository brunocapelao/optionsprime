#!/usr/bin/env python3
"""
üìä AN√ÅLISE TEXTUAL DOS RESULTADOS DO BACKTEST HIST√ìRICO
Cria relat√≥rio detalhado sem depend√™ncias gr√°ficas
"""

import json
import pandas as pd
import numpy as np
from datetime import datetime

def analyze_backtest_results():
    """
    An√°lise completa dos resultados do backtest hist√≥rico
    """
    print("="*80)
    print("üìä AN√ÅLISE DETALHADA DO BACKTEST HIST√ìRICO")
    print("Framework 02c - Valida√ß√£o de Modelos Quant√≠licos")
    print("="*80)
    
    # Carregar resultados
    results_file = 'data/processed/preds/historical_backtest_results.json'
    
    try:
        with open(results_file, 'r') as f:
            results = json.load(f)
        print(f"‚úÖ Resultados carregados de: {results_file}")
    except Exception as e:
        print(f"‚ùå Erro ao carregar resultados: {e}")
        return False
    
    config = results['config']
    fold_results = results['fold_results']
    gates_summary = results['gates_summary']
    
    print(f"\nüéØ CONFIGURA√á√ÉO DO BACKTEST:")
    print(f"   ‚Ä¢ Data de execu√ß√£o: {results['timestamp']}")
    print(f"   ‚Ä¢ Tempo de execu√ß√£o: {results['execution_time']:.2f}s")
    print(f"   ‚Ä¢ N√∫mero de folds: {len(fold_results)}")
    print(f"   ‚Ä¢ Modelos testados: {', '.join(config['models'])}")
    print(f"   ‚Ä¢ Horizontes: {config['horizons']}")
    print(f"   ‚Ä¢ Quantis: {config['quantiles']}")
    
    # An√°lise de performance por modelo
    print(f"\nüìà AN√ÅLISE DE PERFORMANCE POR MODELO:")
    print("=" * 50)
    
    model_stats = {}
    
    for model_name in config['models']:
        print(f"\nü§ñ {model_name}:")
        
        # Coletar todas as m√©tricas
        all_mae = []
        all_rmse = []
        all_coverage = []
        all_crps = []
        
        for fold in fold_results:
            for horizon in config['horizons']:
                horizon_str = str(horizon)  # Convert to string for JSON key
                metrics = fold['models'][model_name]['metrics'][horizon_str]
                all_mae.append(metrics['MAE'])
                all_rmse.append(metrics['RMSE'])
                all_coverage.append(metrics['Coverage_90'])
                all_crps.append(metrics['CRPS'])
        
        # Calcular estat√≠sticas
        mae_stats = {
            'mean': np.mean(all_mae),
            'std': np.std(all_mae),
            'min': np.min(all_mae),
            'max': np.max(all_mae)
        }
        
        rmse_stats = {
            'mean': np.mean(all_rmse),
            'std': np.std(all_rmse),
            'min': np.min(all_rmse),
            'max': np.max(all_rmse)
        }
        
        coverage_stats = {
            'mean': np.mean(all_coverage),
            'std': np.std(all_coverage),
            'min': np.min(all_coverage),
            'max': np.max(all_coverage)
        }
        
        crps_stats = {
            'mean': np.mean(all_crps),
            'std': np.std(all_crps),
            'min': np.min(all_crps),
            'max': np.max(all_crps)
        }
        
        model_stats[model_name] = {
            'MAE': mae_stats,
            'RMSE': rmse_stats,
            'Coverage': coverage_stats,
            'CRPS': crps_stats
        }
        
        print(f"   üìä MAE:")
        print(f"      ‚Ä¢ M√©dia: {mae_stats['mean']:.4f} ¬± {mae_stats['std']:.4f}")
        print(f"      ‚Ä¢ Range: [{mae_stats['min']:.4f}, {mae_stats['max']:.4f}]")
        
        print(f"   üìä RMSE:")
        print(f"      ‚Ä¢ M√©dia: {rmse_stats['mean']:.4f} ¬± {rmse_stats['std']:.4f}")
        print(f"      ‚Ä¢ Range: [{rmse_stats['min']:.4f}, {rmse_stats['max']:.4f}]")
        
        print(f"   üìä Coverage 90%:")
        print(f"      ‚Ä¢ M√©dia: {coverage_stats['mean']:.3f} ¬± {coverage_stats['std']:.3f}")
        print(f"      ‚Ä¢ Range: [{coverage_stats['min']:.3f}, {coverage_stats['max']:.3f}]")
        print(f"      ‚Ä¢ Desvio do target (90%): {abs(coverage_stats['mean'] - 0.90):.3f}")
        
        print(f"   üìä CRPS:")
        print(f"      ‚Ä¢ M√©dia: {crps_stats['mean']:.4f} ¬± {crps_stats['std']:.4f}")
        print(f"      ‚Ä¢ Range: [{crps_stats['min']:.4f}, {crps_stats['max']:.4f}]")
        
        # Gates summary
        gates_info = gates_summary[model_name]
        print(f"   üö™ GATES:")
        print(f"      ‚Ä¢ Aprovados: {gates_info['total_passed']}/{gates_info['total_gates']}")
        print(f"      ‚Ä¢ Taxa: {gates_info['approval_rate']:.1%}")
        print(f"      ‚Ä¢ Decis√£o: {gates_info['final_decision']}")
    
    # Compara√ß√£o entre modelos
    if len(config['models']) >= 2:
        print(f"\nüèÜ COMPARA√á√ÉO ENTRE MODELOS:")
        print("=" * 40)
        
        model1, model2 = config['models'][0], config['models'][1]
        
        # MAE comparison
        mae1 = model_stats[model1]['MAE']['mean']
        mae2 = model_stats[model2]['MAE']['mean']
        mae_improvement = ((mae2 - mae1) / mae2 * 100) if mae1 < mae2 else ((mae1 - mae2) / mae1 * 100)
        mae_winner = model1 if mae1 < mae2 else model2
        
        print(f"üìä MAE Comparison:")
        print(f"   ‚Ä¢ {model1}: {mae1:.4f}")
        print(f"   ‚Ä¢ {model2}: {mae2:.4f}")
        print(f"   ‚Ä¢ Vencedor: {mae_winner} ({mae_improvement:.1f}% melhor)")
        
        # RMSE comparison
        rmse1 = model_stats[model1]['RMSE']['mean']
        rmse2 = model_stats[model2]['RMSE']['mean']
        rmse_improvement = ((rmse2 - rmse1) / rmse2 * 100) if rmse1 < rmse2 else ((rmse1 - rmse2) / rmse1 * 100)
        rmse_winner = model1 if rmse1 < rmse2 else model2
        
        print(f"üìä RMSE Comparison:")
        print(f"   ‚Ä¢ {model1}: {rmse1:.4f}")
        print(f"   ‚Ä¢ {model2}: {rmse2:.4f}")
        print(f"   ‚Ä¢ Vencedor: {rmse_winner} ({rmse_improvement:.1f}% melhor)")
        
        # Coverage comparison
        cov1 = model_stats[model1]['Coverage']['mean']
        cov2 = model_stats[model2]['Coverage']['mean']
        cov1_error = abs(cov1 - 0.90)
        cov2_error = abs(cov2 - 0.90)
        cov_winner = model1 if cov1_error < cov2_error else model2
        
        print(f"üìä Coverage Comparison:")
        print(f"   ‚Ä¢ {model1}: {cov1:.3f} (erro: {cov1_error:.3f})")
        print(f"   ‚Ä¢ {model2}: {cov2:.3f} (erro: {cov2_error:.3f})")
        print(f"   ‚Ä¢ Melhor calibrado: {cov_winner}")
        
        # Gates comparison
        gates1 = gates_summary[model1]['approval_rate']
        gates2 = gates_summary[model2]['approval_rate']
        gates_winner = model1 if gates1 > gates2 else model2
        
        print(f"üö™ Gates Comparison:")
        print(f"   ‚Ä¢ {model1}: {gates1:.1%}")
        print(f"   ‚Ä¢ {model2}: {gates2:.1%}")
        print(f"   ‚Ä¢ Melhor aprova√ß√£o: {gates_winner}")
    
    # An√°lise por horizonte
    print(f"\nüìà AN√ÅLISE POR HORIZONTE:")
    print("=" * 35)
    
    for horizon in config['horizons']:
        print(f"\n‚è∞ Horizonte {horizon}h:")
        
        for model_name in config['models']:
            # Coletar m√©tricas deste horizonte
            horizon_mae = []
            horizon_coverage = []
            
            for fold in fold_results:
                horizon_str = str(horizon)  # Convert to string for JSON key
                metrics = fold['models'][model_name]['metrics'][horizon_str]
                horizon_mae.append(metrics['MAE'])
                horizon_coverage.append(metrics['Coverage_90'])
            
            mae_avg = np.mean(horizon_mae)
            cov_avg = np.mean(horizon_coverage)
            
            print(f"   {model_name}: MAE={mae_avg:.4f}, Coverage={cov_avg:.3f}")
    
    # An√°lise por fold
    print(f"\nüîÑ AN√ÅLISE POR FOLD:")
    print("=" * 25)
    
    for fold in fold_results:
        print(f"\nFold {fold['fold_id']}:")
        print(f"   üìö Treino: obs {fold['train_period'][0]} ‚Üí {fold['train_period'][1]}")
        print(f"   üß™ Teste: obs {fold['test_period'][0]} ‚Üí {fold['test_period'][1]}")
        
        for model_name in config['models']:
            gates_info = fold['gates'][model_name]
            status_icon = "‚úÖ" if gates_info['decision'] == "GO" else "üü°" if gates_info['decision'] == "CONDITIONAL" else "‚ùå"
            
            print(f"   {status_icon} {model_name}: {gates_info['gates_passed']}/{gates_info['gates_total']} ({gates_info['approval_rate']:.1%}) ‚Üí {gates_info['decision']}")
    
    # Resumo dos Gates
    print(f"\nüö™ RESUMO FINAL DOS GATES:")
    print("=" * 30)
    
    for model_name, gates_info in gates_summary.items():
        approval_rate = gates_info['approval_rate']
        decision = gates_info['final_decision']
        
        if decision == 'GO':
            status_icon = "‚úÖ"
            status_color = "GREEN"
        elif decision == 'CONDITIONAL':
            status_icon = "üü°"
            status_color = "YELLOW"
        else:
            status_icon = "‚ùå"
            status_color = "RED"
        
        print(f"{status_icon} {model_name}:")
        print(f"   ‚Ä¢ Gates aprovados: {gates_info['total_passed']}/{gates_info['total_gates']}")
        print(f"   ‚Ä¢ Taxa de aprova√ß√£o: {approval_rate:.1%}")
        print(f"   ‚Ä¢ Status: {status_color} - {decision}")
        
        # Interpreta√ß√£o do status
        if decision == 'GO':
            print(f"   ‚Ä¢ Interpreta√ß√£o: ‚úÖ Aprovado para produ√ß√£o")
        elif decision == 'CONDITIONAL':
            print(f"   ‚Ä¢ Interpreta√ß√£o: üü° Aprova√ß√£o condicional - monitorar de perto")
        else:
            print(f"   ‚Ä¢ Interpreta√ß√£o: ‚ùå Necessita melhorias antes da produ√ß√£o")
    
    # Recomenda√ß√£o final
    print(f"\nüéØ RECOMENDA√á√ÉO FINAL:")
    print("=" * 25)
    
    best_model = max(gates_summary.keys(), 
                    key=lambda x: gates_summary[x]['approval_rate'])
    best_rate = gates_summary[best_model]['approval_rate']
    best_decision = gates_summary[best_model]['final_decision']
    
    print(f"üèÜ Modelo recomendado: {best_model}")
    print(f"üìä Taxa de aprova√ß√£o: {best_rate:.1%}")
    print(f"üöÄ Status final: {best_decision}")
    
    if best_decision == 'GO':
        print(f"\n‚úÖ APROVADO PARA PRODU√á√ÉO")
        print(f"üìã Pr√≥ximos passos recomendados:")
        print(f"   1. üìä Implementar sistema de monitoramento em tempo real")
        print(f"   2. üö® Configurar alertas de degrada√ß√£o de performance")
        print(f"   3. üìà Executar backtest em per√≠odo mais longo (6+ meses)")
        print(f"   4. üìã Preparar documenta√ß√£o t√©cnica para deploy")
        print(f"   5. üîÑ Estabelecer ciclo de retreinamento peri√≥dico")
        print(f"   6. üéØ Definir KPIs de monitoramento em produ√ß√£o")
        
    elif best_decision == 'CONDITIONAL':
        print(f"\nüü° APROVA√á√ÉO CONDICIONAL")
        print(f"üìã A√ß√µes recomendadas antes do deploy:")
        print(f"   1. üîç Revisar thresholds dos gates que falharam")
        print(f"   2. üìä Aumentar frequ√™ncia de monitoramento")
        print(f"   3. üéØ Implementar alertas mais sens√≠veis")
        print(f"   4. üìà Validar performance em dados mais recentes")
        print(f"   5. üîß Considerar ajustes finos nos hiperpar√¢metros")
        print(f"   6. üìã Plano de conting√™ncia em caso de degrada√ß√£o")
        
    else:
        print(f"\n‚ùå NECESSITA MELHORIAS SIGNIFICATIVAS")
        print(f"üìã A√ß√µes obrigat√≥rias antes de considerar produ√ß√£o:")
        print(f"   1. üîÑ Retreinar modelo com dados mais recentes/extensos")
        print(f"   2. üß™ Revisar e melhorar engenharia de features")
        print(f"   3. ‚öôÔ∏è  Otimizar hiperpar√¢metros com busca mais ampla")
        print(f"   4. üéØ Validar qualidade e consist√™ncia dos dados")
        print(f"   5. üìä Considerar arquiteturas de modelo alternativas")
        print(f"   6. üîç Analisar casos de falha espec√≠ficos")
    
    # Insights adicionais
    print(f"\nüí° INSIGHTS ADICIONAIS:")
    print("=" * 25)
    
    # An√°lise de consist√™ncia
    consistency_scores = {}
    for model_name in config['models']:
        fold_rates = [fold['gates'][model_name]['approval_rate'] for fold in fold_results]
        consistency_scores[model_name] = {
            'mean': np.mean(fold_rates),
            'std': np.std(fold_rates),
            'range': max(fold_rates) - min(fold_rates)
        }
    
    print(f"üìä Consist√™ncia entre folds:")
    for model_name, scores in consistency_scores.items():
        print(f"   {model_name}:")
        print(f"      ‚Ä¢ Desvio padr√£o: {scores['std']:.3f}")
        print(f"      ‚Ä¢ Range: {scores['range']:.3f}")
        if scores['std'] < 0.1:
            print(f"      ‚Ä¢ Avalia√ß√£o: ‚úÖ Muito consistente")
        elif scores['std'] < 0.2:
            print(f"      ‚Ä¢ Avalia√ß√£o: üü° Moderadamente consistente")
        else:
            print(f"      ‚Ä¢ Avalia√ß√£o: ‚ùå Inconsistente - investigar")
    
    # Performance vs Horizonte
    print(f"\nüìà Tend√™ncias por horizonte:")
    for model_name in config['models']:
        mae_by_horizon = {}
        for horizon in config['horizons']:
            mae_values = []
            for fold in fold_results:
                horizon_str = str(horizon)  # Convert to string for JSON key
                mae_values.append(fold['models'][model_name]['metrics'][horizon_str]['MAE'])
            mae_by_horizon[horizon] = np.mean(mae_values)
        
        print(f"   {model_name}:")
        trend = "crescente" if mae_by_horizon[60] > mae_by_horizon[42] else "decrescente"
        print(f"      ‚Ä¢ Tend√™ncia MAE: {trend}")
        print(f"      ‚Ä¢ H42: {mae_by_horizon[42]:.4f} ‚Üí H60: {mae_by_horizon[60]:.4f}")
    
    # Salvar resumo em arquivo
    print(f"\nüíæ Salvando resumo executivo...")
    
    summary_report = []
    summary_report.append("RESUMO EXECUTIVO - BACKTEST HIST√ìRICO")
    summary_report.append("=" * 50)
    summary_report.append(f"Data: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    summary_report.append(f"Framework: 02c")
    summary_report.append("")
    summary_report.append("MODELO RECOMENDADO:")
    summary_report.append(f"‚Ä¢ Nome: {best_model}")
    summary_report.append(f"‚Ä¢ Taxa de aprova√ß√£o: {best_rate:.1%}")
    summary_report.append(f"‚Ä¢ Decis√£o: {best_decision}")
    summary_report.append("")
    summary_report.append("M√âTRICAS PRINCIPAIS:")
    for model_name in config['models']:
        mae_avg = model_stats[model_name]['MAE']['mean']
        cov_avg = model_stats[model_name]['Coverage']['mean']
        summary_report.append(f"‚Ä¢ {model_name}: MAE={mae_avg:.4f}, Coverage={cov_avg:.3f}")
    summary_report.append("")
    summary_report.append("STATUS PARA PRODU√á√ÉO:")
    if best_decision == 'GO':
        summary_report.append("‚úÖ APROVADO - Pronto para deploy")
    elif best_decision == 'CONDITIONAL':
        summary_report.append("üü° CONDICIONAL - Deploy com monitoramento intensivo")
    else:
        summary_report.append("‚ùå REPROVADO - Necessita melhorias")
    
    try:
        with open('data/processed/preds/executive_summary.txt', 'w', encoding='utf-8') as f:
            f.write('\n'.join(summary_report))
        print("‚úÖ Resumo executivo salvo em: data/processed/preds/executive_summary.txt")
    except Exception as e:
        print(f"‚ö†Ô∏è  Erro ao salvar resumo: {e}")
    
    print(f"\nüéØ AN√ÅLISE COMPLETA FINALIZADA!")
    print("=" * 50)
    
    return True

if __name__ == "__main__":
    print("üìä Iniciando an√°lise detalhada dos resultados...")
    success = analyze_backtest_results()
    
    if success:
        print("‚úÖ An√°lise conclu√≠da com sucesso!")
    else:
        print("‚ùå Erro na an√°lise dos resultados")