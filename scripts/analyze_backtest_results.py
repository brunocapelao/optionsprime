#!/usr/bin/env python3
"""
ğŸ“Š ANÃLISE TEXTUAL DOS RESULTADOS DO BACKTEST HISTÃ“RICO
Cria relatÃ³rio detalhado sem dependÃªncias grÃ¡ficas
"""

import json
import pandas as pd
import numpy as np
from datetime import datetime

def analyze_backtest_results():
    """
    AnÃ¡lise completa dos resultados do backtest histÃ³rico
    """
    print("="*80)
    print("ğŸ“Š ANÃLISE DETALHADA DO BACKTEST HISTÃ“RICO")
    print("Framework 02c - ValidaÃ§Ã£o de Modelos QuantÃ­licos")
    print("="*80)
    
    # Carregar resultados
    results_file = 'data/processed/preds/historical_backtest_results.json'
    
    try:
        with open(results_file, 'r') as f:
            results = json.load(f)
        print(f"âœ… Resultados carregados de: {results_file}")
    except Exception as e:
        print(f"âŒ Erro ao carregar resultados: {e}")
        return False
    
    config = results['config']
    fold_results = results['fold_results']
    gates_summary = results['gates_summary']
    
    print(f"\nğŸ¯ CONFIGURAÃ‡ÃƒO DO BACKTEST:")
    print(f"   â€¢ Data de execuÃ§Ã£o: {results['timestamp']}")
    print(f"   â€¢ Tempo de execuÃ§Ã£o: {results['execution_time']:.2f}s")
    print(f"   â€¢ NÃºmero de folds: {len(fold_results)}")
    print(f"   â€¢ Modelos testados: {', '.join(config['models'])}")
    print(f"   â€¢ Horizontes: {config['horizons']}")
    print(f"   â€¢ Quantis: {config['quantiles']}")
    
    # AnÃ¡lise de performance por modelo
    print(f"\nğŸ“ˆ ANÃLISE DE PERFORMANCE POR MODELO:")
    print("=" * 50)
    
    model_stats = {}
    
    for model_name in config['models']:
        print(f"\nğŸ¤– {model_name}:")
        
        # Coletar todas as mÃ©tricas
        all_mae = []
        all_rmse = []
        all_coverage = []
        all_crps = []
        
        for fold in fold_results:
            for horizon in config['horizons']:
                horizon_str = str(horizon)  # Convert to string for JSON key
                metrics = fold['models'][model_name]['metrics'][horizon_str]
                all_mae.append(metrics['MAE'])
                all_rmse.append(metrics['RMSE'])
                all_coverage.append(metrics['Coverage_90'])
                all_crps.append(metrics['CRPS'])
        
        # Calcular estatÃ­sticas
        mae_stats = {
            'mean': np.mean(all_mae),
            'std': np.std(all_mae),
            'min': np.min(all_mae),
            'max': np.max(all_mae)
        }
        
        rmse_stats = {
            'mean': np.mean(all_rmse),
            'std': np.std(all_rmse),
            'min': np.min(all_rmse),
            'max': np.max(all_rmse)
        }
        
        coverage_stats = {
            'mean': np.mean(all_coverage),
            'std': np.std(all_coverage),
            'min': np.min(all_coverage),
            'max': np.max(all_coverage)
        }
        
        crps_stats = {
            'mean': np.mean(all_crps),
            'std': np.std(all_crps),
            'min': np.min(all_crps),
            'max': np.max(all_crps)
        }
        
        model_stats[model_name] = {
            'MAE': mae_stats,
            'RMSE': rmse_stats,
            'Coverage': coverage_stats,
            'CRPS': crps_stats
        }
        
        print(f"   ğŸ“Š MAE:")
        print(f"      â€¢ MÃ©dia: {mae_stats['mean']:.4f} Â± {mae_stats['std']:.4f}")
        print(f"      â€¢ Range: [{mae_stats['min']:.4f}, {mae_stats['max']:.4f}]")
        
        print(f"   ğŸ“Š RMSE:")
        print(f"      â€¢ MÃ©dia: {rmse_stats['mean']:.4f} Â± {rmse_stats['std']:.4f}")
        print(f"      â€¢ Range: [{rmse_stats['min']:.4f}, {rmse_stats['max']:.4f}]")
        
        print(f"   ğŸ“Š Coverage 90%:")
        print(f"      â€¢ MÃ©dia: {coverage_stats['mean']:.3f} Â± {coverage_stats['std']:.3f}")
        print(f"      â€¢ Range: [{coverage_stats['min']:.3f}, {coverage_stats['max']:.3f}]")
        print(f"      â€¢ Desvio do target (90%): {abs(coverage_stats['mean'] - 0.90):.3f}")
        
        print(f"   ğŸ“Š CRPS:")
        print(f"      â€¢ MÃ©dia: {crps_stats['mean']:.4f} Â± {crps_stats['std']:.4f}")
        print(f"      â€¢ Range: [{crps_stats['min']:.4f}, {crps_stats['max']:.4f}]")
        
        # Gates summary
        gates_info = gates_summary[model_name]
        print(f"   ğŸšª GATES:")
        print(f"      â€¢ Aprovados: {gates_info['total_passed']}/{gates_info['total_gates']}")
        print(f"      â€¢ Taxa: {gates_info['approval_rate']:.1%}")
        print(f"      â€¢ DecisÃ£o: {gates_info['final_decision']}")
    
    # ComparaÃ§Ã£o entre modelos
    if len(config['models']) >= 2:
        print(f"\nğŸ† COMPARAÃ‡ÃƒO ENTRE MODELOS:")
        print("=" * 40)
        
        model1, model2 = config['models'][0], config['models'][1]
        
        # MAE comparison
        mae1 = model_stats[model1]['MAE']['mean']
        mae2 = model_stats[model2]['MAE']['mean']
        mae_improvement = ((mae2 - mae1) / mae2 * 100) if mae1 < mae2 else ((mae1 - mae2) / mae1 * 100)
        mae_winner = model1 if mae1 < mae2 else model2
        
        print(f"ğŸ“Š MAE Comparison:")
        print(f"   â€¢ {model1}: {mae1:.4f}")
        print(f"   â€¢ {model2}: {mae2:.4f}")
        print(f"   â€¢ Vencedor: {mae_winner} ({mae_improvement:.1f}% melhor)")
        
        # RMSE comparison
        rmse1 = model_stats[model1]['RMSE']['mean']
        rmse2 = model_stats[model2]['RMSE']['mean']
        rmse_improvement = ((rmse2 - rmse1) / rmse2 * 100) if rmse1 < rmse2 else ((rmse1 - rmse2) / rmse1 * 100)
        rmse_winner = model1 if rmse1 < rmse2 else model2
        
        print(f"ğŸ“Š RMSE Comparison:")
        print(f"   â€¢ {model1}: {rmse1:.4f}")
        print(f"   â€¢ {model2}: {rmse2:.4f}")
        print(f"   â€¢ Vencedor: {rmse_winner} ({rmse_improvement:.1f}% melhor)")
        
        # Coverage comparison
        cov1 = model_stats[model1]['Coverage']['mean']
        cov2 = model_stats[model2]['Coverage']['mean']
        cov1_error = abs(cov1 - 0.90)
        cov2_error = abs(cov2 - 0.90)
        cov_winner = model1 if cov1_error < cov2_error else model2
        
        print(f"ğŸ“Š Coverage Comparison:")
        print(f"   â€¢ {model1}: {cov1:.3f} (erro: {cov1_error:.3f})")
        print(f"   â€¢ {model2}: {cov2:.3f} (erro: {cov2_error:.3f})")
        print(f"   â€¢ Melhor calibrado: {cov_winner}")
        
        # Gates comparison
        gates1 = gates_summary[model1]['approval_rate']
        gates2 = gates_summary[model2]['approval_rate']
        gates_winner = model1 if gates1 > gates2 else model2
        
        print(f"ğŸšª Gates Comparison:")
        print(f"   â€¢ {model1}: {gates1:.1%}")
        print(f"   â€¢ {model2}: {gates2:.1%}")
        print(f"   â€¢ Melhor aprovaÃ§Ã£o: {gates_winner}")
    
    # AnÃ¡lise por horizonte
    print(f"\nğŸ“ˆ ANÃLISE POR HORIZONTE:")
    print("=" * 35)
    
    for horizon in config['horizons']:
        print(f"\nâ° Horizonte {horizon}h:")
        
        for model_name in config['models']:
            # Coletar mÃ©tricas deste horizonte
            horizon_mae = []
            horizon_coverage = []
            
            for fold in fold_results:
                horizon_str = str(horizon)  # Convert to string for JSON key
                metrics = fold['models'][model_name]['metrics'][horizon_str]
                horizon_mae.append(metrics['MAE'])
                horizon_coverage.append(metrics['Coverage_90'])
            
            mae_avg = np.mean(horizon_mae)
            cov_avg = np.mean(horizon_coverage)
            
            print(f"   {model_name}: MAE={mae_avg:.4f}, Coverage={cov_avg:.3f}")
    
    # AnÃ¡lise por fold
    print(f"\nğŸ”„ ANÃLISE POR FOLD:")
    print("=" * 25)
    
    for fold in fold_results:
        print(f"\nFold {fold['fold_id']}:")
        print(f"   ğŸ“š Treino: obs {fold['train_period'][0]} â†’ {fold['train_period'][1]}")
        print(f"   ğŸ§ª Teste: obs {fold['test_period'][0]} â†’ {fold['test_period'][1]}")
        
        for model_name in config['models']:
            gates_info = fold['gates'][model_name]
            status_icon = "âœ…" if gates_info['decision'] == "GO" else "ğŸŸ¡" if gates_info['decision'] == "CONDITIONAL" else "âŒ"
            
            print(f"   {status_icon} {model_name}: {gates_info['gates_passed']}/{gates_info['gates_total']} ({gates_info['approval_rate']:.1%}) â†’ {gates_info['decision']}")
    
    # Resumo dos Gates
    print(f"\nğŸšª RESUMO FINAL DOS GATES:")
    print("=" * 30)
    
    for model_name, gates_info in gates_summary.items():
        approval_rate = gates_info['approval_rate']
        decision = gates_info['final_decision']
        
        if decision == 'GO':
            status_icon = "âœ…"
            status_color = "GREEN"
        elif decision == 'CONDITIONAL':
            status_icon = "ğŸŸ¡"
            status_color = "YELLOW"
        else:
            status_icon = "âŒ"
            status_color = "RED"
        
        print(f"{status_icon} {model_name}:")
        print(f"   â€¢ Gates aprovados: {gates_info['total_passed']}/{gates_info['total_gates']}")
        print(f"   â€¢ Taxa de aprovaÃ§Ã£o: {approval_rate:.1%}")
        print(f"   â€¢ Status: {status_color} - {decision}")
        
        # InterpretaÃ§Ã£o do status
        if decision == 'GO':
            print(f"   â€¢ InterpretaÃ§Ã£o: âœ… Aprovado para produÃ§Ã£o")
        elif decision == 'CONDITIONAL':
            print(f"   â€¢ InterpretaÃ§Ã£o: ğŸŸ¡ AprovaÃ§Ã£o condicional - monitorar de perto")
        else:
            print(f"   â€¢ InterpretaÃ§Ã£o: âŒ Necessita melhorias antes da produÃ§Ã£o")
    
    # RecomendaÃ§Ã£o final
    print(f"\nğŸ¯ RECOMENDAÃ‡ÃƒO FINAL:")
    print("=" * 25)
    
    best_model = max(gates_summary.keys(), 
                    key=lambda x: gates_summary[x]['approval_rate'])
    best_rate = gates_summary[best_model]['approval_rate']
    best_decision = gates_summary[best_model]['final_decision']
    
    print(f"ğŸ† Modelo recomendado: {best_model}")
    print(f"ğŸ“Š Taxa de aprovaÃ§Ã£o: {best_rate:.1%}")
    print(f"ğŸš€ Status final: {best_decision}")
    
    if best_decision == 'GO':
        print(f"\nâœ… APROVADO PARA PRODUÃ‡ÃƒO")
        print(f"ğŸ“‹ PrÃ³ximos passos recomendados:")
        print(f"   1. ğŸ“Š Implementar sistema de monitoramento em tempo real")
        print(f"   2. ğŸš¨ Configurar alertas de degradaÃ§Ã£o de performance")
        print(f"   3. ğŸ“ˆ Executar backtest em perÃ­odo mais longo (6+ meses)")
        print(f"   4. ğŸ“‹ Preparar documentaÃ§Ã£o tÃ©cnica para deploy")
        print(f"   5. ğŸ”„ Estabelecer ciclo de retreinamento periÃ³dico")
        print(f"   6. ğŸ¯ Definir KPIs de monitoramento em produÃ§Ã£o")
        
    elif best_decision == 'CONDITIONAL':
        print(f"\nğŸŸ¡ APROVAÃ‡ÃƒO CONDICIONAL")
        print(f"ğŸ“‹ AÃ§Ãµes recomendadas antes do deploy:")
        print(f"   1. ğŸ” Revisar thresholds dos gates que falharam")
        print(f"   2. ğŸ“Š Aumentar frequÃªncia de monitoramento")
        print(f"   3. ğŸ¯ Implementar alertas mais sensÃ­veis")
        print(f"   4. ğŸ“ˆ Validar performance em dados mais recentes")
        print(f"   5. ğŸ”§ Considerar ajustes finos nos hiperparÃ¢metros")
        print(f"   6. ğŸ“‹ Plano de contingÃªncia em caso de degradaÃ§Ã£o")
        
    else:
        print(f"\nâŒ NECESSITA MELHORIAS SIGNIFICATIVAS")
        print(f"ğŸ“‹ AÃ§Ãµes obrigatÃ³rias antes de considerar produÃ§Ã£o:")
        print(f"   1. ğŸ”„ Retreinar modelo com dados mais recentes/extensos")
        print(f"   2. ğŸ§ª Revisar e melhorar engenharia de features")
        print(f"   3. âš™ï¸  Otimizar hiperparÃ¢metros com busca mais ampla")
        print(f"   4. ğŸ¯ Validar qualidade e consistÃªncia dos dados")
        print(f"   5. ğŸ“Š Considerar arquiteturas de modelo alternativas")
        print(f"   6. ğŸ” Analisar casos de falha especÃ­ficos")
    
    # Insights adicionais
    print(f"\nğŸ’¡ INSIGHTS ADICIONAIS:")
    print("=" * 25)
    
    # AnÃ¡lise de consistÃªncia
    consistency_scores = {}
    for model_name in config['models']:
        fold_rates = [fold['gates'][model_name]['approval_rate'] for fold in fold_results]
        consistency_scores[model_name] = {
            'mean': np.mean(fold_rates),
            'std': np.std(fold_rates),
            'range': max(fold_rates) - min(fold_rates)
        }
    
    print(f"ğŸ“Š ConsistÃªncia entre folds:")
    for model_name, scores in consistency_scores.items():
        print(f"   {model_name}:")
        print(f"      â€¢ Desvio padrÃ£o: {scores['std']:.3f}")
        print(f"      â€¢ Range: {scores['range']:.3f}")
        if scores['std'] < 0.1:
            print(f"      â€¢ AvaliaÃ§Ã£o: âœ… Muito consistente")
        elif scores['std'] < 0.2:
            print(f"      â€¢ AvaliaÃ§Ã£o: ğŸŸ¡ Moderadamente consistente")
        else:
            print(f"      â€¢ AvaliaÃ§Ã£o: âŒ Inconsistente - investigar")
    
    # Performance vs Horizonte
    print(f"\nğŸ“ˆ TendÃªncias por horizonte:")
    for model_name in config['models']:
        mae_by_horizon = {}
        for horizon in config['horizons']:
            mae_values = []
            for fold in fold_results:
                horizon_str = str(horizon)  # Convert to string for JSON key
                mae_values.append(fold['models'][model_name]['metrics'][horizon_str]['MAE'])
            mae_by_horizon[horizon] = np.mean(mae_values)
        
        print(f"   {model_name}:")
        trend = "crescente" if mae_by_horizon[60] > mae_by_horizon[42] else "decrescente"
        print(f"      â€¢ TendÃªncia MAE: {trend}")
        print(f"      â€¢ H42: {mae_by_horizon[42]:.4f} â†’ H60: {mae_by_horizon[60]:.4f}")
    
    # Salvar resumo em arquivo
    print(f"\nğŸ’¾ Salvando resumo executivo...")
    
    summary_report = []
    summary_report.append("RESUMO EXECUTIVO - BACKTEST HISTÃ“RICO")
    summary_report.append("=" * 50)
    summary_report.append(f"Data: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    summary_report.append(f"Framework: 02c")
    summary_report.append("")
    summary_report.append("MODELO RECOMENDADO:")
    summary_report.append(f"â€¢ Nome: {best_model}")
    summary_report.append(f"â€¢ Taxa de aprovaÃ§Ã£o: {best_rate:.1%}")
    summary_report.append(f"â€¢ DecisÃ£o: {best_decision}")
    summary_report.append("")
    summary_report.append("MÃ‰TRICAS PRINCIPAIS:")
    for model_name in config['models']:
        mae_avg = model_stats[model_name]['MAE']['mean']
        cov_avg = model_stats[model_name]['Coverage']['mean']
        summary_report.append(f"â€¢ {model_name}: MAE={mae_avg:.4f}, Coverage={cov_avg:.3f}")
    summary_report.append("")
    summary_report.append("STATUS PARA PRODUÃ‡ÃƒO:")
    if best_decision == 'GO':
        summary_report.append("âœ… APROVADO - Pronto para deploy")
    elif best_decision == 'CONDITIONAL':
        summary_report.append("ğŸŸ¡ CONDICIONAL - Deploy com monitoramento intensivo")
    else:
        summary_report.append("âŒ REPROVADO - Necessita melhorias")
    
    try:
        with open('data/processed/preds/executive_summary.txt', 'w', encoding='utf-8') as f:
            f.write('\n'.join(summary_report))
        print("âœ… Resumo executivo salvo em: data/processed/preds/executive_summary.txt")
    except Exception as e:
        print(f"âš ï¸  Erro ao salvar resumo: {e}")
    
    print(f"\nğŸ¯ ANÃLISE COMPLETA FINALIZADA!")
    print("=" * 50)
    
    return True

if __name__ == "__main__":
    print("ğŸ“Š Iniciando anÃ¡lise detalhada dos resultados...")
    success = analyze_backtest_results()
    
    if success:
        print("âœ… AnÃ¡lise concluÃ­da com sucesso!")
    else:
        print("âŒ Erro na anÃ¡lise dos resultados")