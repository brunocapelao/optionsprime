#!/usr/bin/env python3
"""
ğŸ“Š ANÃLISE VISUAL DOS RESULTADOS DO BACKTEST HISTÃ“RICO
Cria grÃ¡ficos e dashboard dos resultados do framework 02c
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import json
from pathlib import Path

def create_backtest_dashboard():
    """
    Cria dashboard visual completo dos resultados do backtest
    """
    print("="*60)
    print("ğŸ“Š CRIANDO DASHBOARD VISUAL DO BACKTEST")
    print("="*60)
    
    # Carregar resultados
    results_file = 'data/processed/preds/historical_backtest_results.json'
    
    try:
        with open(results_file, 'r') as f:
            results = json.load(f)
        print(f"âœ… Resultados carregados de: {results_file}")
    except Exception as e:
        print(f"âŒ Erro ao carregar resultados: {e}")
        return False
    
    # Extrair dados para visualizaÃ§Ã£o
    config = results['config']
    fold_results = results['fold_results']
    gates_summary = results['gates_summary']
    
    # Configurar estilo dos grÃ¡ficos
    plt.style.use('seaborn-v0_8-darkgrid')
    sns.set_palette("husl")
    
    # Criar figura com subplots
    fig = plt.figure(figsize=(20, 15))
    fig.suptitle('ğŸ¯ Backtest HistÃ³rico - Dashboard Executivo\nFramework 02c', 
                 fontsize=20, fontweight='bold', y=0.98)
    
    # 1. Performance MAE por Fold e Modelo
    ax1 = plt.subplot(3, 3, 1)
    
    fold_numbers = []
    mae_data = {model: [] for model in config['models']}
    
    for fold in fold_results:
        fold_numbers.append(fold['fold_id'])
        for model_name in config['models']:
            # Calcular MAE mÃ©dio do fold
            mae_values = [fold['models'][model_name]['metrics'][h]['MAE'] 
                         for h in config['horizons']]
            mae_data[model_name].append(np.mean(mae_values))
    
    for model_name, mae_vals in mae_data.items():
        ax1.plot(fold_numbers, mae_vals, marker='o', linewidth=3, 
                markersize=8, label=model_name)
    
    ax1.set_title('ğŸ“ˆ MAE por Fold', fontsize=14, fontweight='bold')
    ax1.set_xlabel('Fold')
    ax1.set_ylabel('MAE')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. Coverage por Horizonte
    ax2 = plt.subplot(3, 3, 2)
    
    horizons = config['horizons']
    coverage_data = {model: [] for model in config['models']}
    
    for model_name in config['models']:
        for horizon in horizons:
            coverage_values = [fold['models'][model_name]['metrics'][horizon]['Coverage_90'] 
                             for fold in fold_results]
            coverage_data[model_name].append(np.mean(coverage_values))
    
    x_pos = np.arange(len(horizons))
    width = 0.35
    
    for i, (model_name, cov_vals) in enumerate(coverage_data.items()):
        ax2.bar(x_pos + i*width, cov_vals, width, label=model_name, alpha=0.8)
    
    ax2.axhline(y=0.9, color='red', linestyle='--', alpha=0.7, label='Target (90%)')
    ax2.set_title('ğŸ“Š Coverage 90% por Horizonte', fontsize=14, fontweight='bold')
    ax2.set_xlabel('Horizonte (horas)')
    ax2.set_ylabel('Coverage')
    ax2.set_xticks(x_pos + width/2)
    ax2.set_xticklabels(horizons)
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    ax2.set_ylim(0.7, 1.0)
    
    # 3. Gates Approval Rate
    ax3 = plt.subplot(3, 3, 3)
    
    models = list(gates_summary.keys())
    approval_rates = [gates_summary[model]['approval_rate'] for model in models]
    
    # Cores baseadas na aprovaÃ§Ã£o
    colors = ['green' if rate >= 0.75 else 'orange' if rate >= 0.60 else 'red' 
              for rate in approval_rates]
    
    bars = ax3.bar(models, approval_rates, color=colors, alpha=0.7)
    ax3.axhline(y=0.75, color='green', linestyle='--', alpha=0.7, label='GO (75%)')
    ax3.axhline(y=0.60, color='orange', linestyle='--', alpha=0.7, label='CONDITIONAL (60%)')
    
    # Adicionar valores nas barras
    for bar, rate in zip(bars, approval_rates):
        height = bar.get_height()
        ax3.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{rate:.1%}', ha='center', va='bottom', fontweight='bold')
    
    ax3.set_title('ğŸšª Taxa de AprovaÃ§Ã£o nos Gates', fontsize=14, fontweight='bold')
    ax3.set_ylabel('Taxa de AprovaÃ§Ã£o')
    ax3.set_ylim(0, 1)
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # 4. RMSE Comparison
    ax4 = plt.subplot(3, 3, 4)
    
    rmse_data = {model: [] for model in config['models']}
    
    for model_name in config['models']:
        rmse_values = []
        for fold in fold_results:
            for horizon in config['horizons']:
                rmse_values.append(fold['models'][model_name]['metrics'][horizon]['RMSE'])
        rmse_data[model_name] = rmse_values
    
    # Box plot para RMSE
    data_for_boxplot = [rmse_data[model] for model in config['models']]
    box_plot = ax4.boxplot(data_for_boxplot, labels=config['models'], patch_artist=True)
    
    # Colorir as caixas
    colors = ['lightblue', 'lightcoral']
    for patch, color in zip(box_plot['boxes'], colors):
        patch.set_facecolor(color)
        patch.set_alpha(0.7)
    
    ax4.set_title('ğŸ“Š DistribuiÃ§Ã£o do RMSE', fontsize=14, fontweight='bold')
    ax4.set_ylabel('RMSE')
    ax4.grid(True, alpha=0.3)
    
    # 5. Performance por Horizonte (Heatmap)
    ax5 = plt.subplot(3, 3, 5)
    
    # Criar matriz de performance (MAE)
    perf_matrix = []
    for model_name in config['models']:
        model_row = []
        for horizon in config['horizons']:
            mae_values = [fold['models'][model_name]['metrics'][horizon]['MAE'] 
                         for fold in fold_results]
            model_row.append(np.mean(mae_values))
        perf_matrix.append(model_row)
    
    perf_df = pd.DataFrame(perf_matrix, 
                          index=config['models'], 
                          columns=[f'H{h}' for h in config['horizons']])
    
    sns.heatmap(perf_df, annot=True, fmt='.4f', cmap='RdYlGn_r', 
                ax=ax5, cbar_kws={'label': 'MAE'})
    ax5.set_title('ğŸ¯ MAE por Modelo e Horizonte', fontsize=14, fontweight='bold')
    
    # 6. Gates Passed por Fold
    ax6 = plt.subplot(3, 3, 6)
    
    fold_gates = {model: [] for model in config['models']}
    
    for fold in fold_results:
        for model_name in config['models']:
            gates_info = fold['gates'][model_name]
            approval_rate = gates_info['approval_rate']
            fold_gates[model_name].append(approval_rate)
    
    for model_name, rates in fold_gates.items():
        ax6.plot(fold_numbers, rates, marker='s', linewidth=3, 
                markersize=8, label=model_name)
    
    ax6.axhline(y=0.75, color='green', linestyle='--', alpha=0.7, label='GO Threshold')
    ax6.set_title('ğŸšª Gates por Fold', fontsize=14, fontweight='bold')
    ax6.set_xlabel('Fold')
    ax6.set_ylabel('Taxa de AprovaÃ§Ã£o')
    ax6.legend()
    ax6.grid(True, alpha=0.3)
    ax6.set_ylim(0, 1)
    
    # 7. Coverage Distribution
    ax7 = plt.subplot(3, 3, 7)
    
    coverage_data_full = {model: [] for model in config['models']}
    
    for model_name in config['models']:
        for fold in fold_results:
            for horizon in config['horizons']:
                coverage_data_full[model_name].append(
                    fold['models'][model_name]['metrics'][horizon]['Coverage_90'])
    
    # Histograma de coverage
    for model_name, cov_vals in coverage_data_full.items():
        ax7.hist(cov_vals, alpha=0.6, label=model_name, bins=10, density=True)
    
    ax7.axvline(x=0.9, color='red', linestyle='--', alpha=0.7, label='Target (90%)')
    ax7.set_title('ğŸ“Š DistribuiÃ§Ã£o do Coverage', fontsize=14, fontweight='bold')
    ax7.set_xlabel('Coverage 90%')
    ax7.set_ylabel('Densidade')
    ax7.legend()
    ax7.grid(True, alpha=0.3)
    
    # 8. Performance Summary (Radar Chart)
    ax8 = plt.subplot(3, 3, 8, projection='polar')
    
    # MÃ©tricas para radar chart
    metrics = ['MAE', 'RMSE', 'Coverage', 'Gates']
    
    for model_name in config['models']:
        # Calcular valores mÃ©dios (normalizar para 0-1)
        mae_avg = np.mean([np.mean([fold['models'][model_name]['metrics'][h]['MAE'] 
                                  for h in config['horizons']]) for fold in fold_results])
        rmse_avg = np.mean([np.mean([fold['models'][model_name]['metrics'][h]['RMSE'] 
                                   for h in config['horizons']]) for fold in fold_results])
        cov_avg = np.mean([np.mean([fold['models'][model_name]['metrics'][h]['Coverage_90'] 
                                  for h in config['horizons']]) for fold in fold_results])
        gates_avg = gates_summary[model_name]['approval_rate']
        
        # Normalizar (inverter MAE e RMSE - menor Ã© melhor)
        mae_norm = max(0, 1 - mae_avg / 0.05)  # Normalizar para 0-1
        rmse_norm = max(0, 1 - rmse_avg / 0.08)
        cov_norm = cov_avg  # JÃ¡ estÃ¡ em 0-1
        gates_norm = gates_avg  # JÃ¡ estÃ¡ em 0-1
        
        values = [mae_norm, rmse_norm, cov_norm, gates_norm]
        values += values[:1]  # Completar o cÃ­rculo
        
        angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()
        angles += angles[:1]
        
        ax8.plot(angles, values, 'o-', linewidth=2, label=model_name)
        ax8.fill(angles, values, alpha=0.25)
    
    ax8.set_xticks(angles[:-1])
    ax8.set_xticklabels(metrics)
    ax8.set_ylim(0, 1)
    ax8.set_title('ğŸ¯ Performance Radar', fontsize=14, fontweight='bold')
    ax8.legend()
    ax8.grid(True)
    
    # 9. Executive Summary (Text)
    ax9 = plt.subplot(3, 3, 9)
    ax9.axis('off')
    
    # Preparar texto do resumo
    best_model = max(gates_summary.keys(), 
                    key=lambda x: gates_summary[x]['approval_rate'])
    best_rate = gates_summary[best_model]['approval_rate']
    best_decision = gates_summary[best_model]['final_decision']
    
    # Calcular improvement
    if len(config['models']) >= 2:
        model1, model2 = config['models'][0], config['models'][1]
        rate1 = gates_summary[model1]['approval_rate']
        rate2 = gates_summary[model2]['approval_rate']
        
        if rate1 > rate2:
            improvement = ((rate1 - rate2) / rate2) * 100 if rate2 > 0 else 0
            winner = model1
        else:
            improvement = ((rate2 - rate1) / rate1) * 100 if rate1 > 0 else 0
            winner = model2
    
    summary_text = f"""
ğŸ¯ RESUMO EXECUTIVO

ğŸ“Š Backtest HistÃ³rico Executado:
   â€¢ {len(fold_results)} folds analisados
   â€¢ {len(config['models'])} modelos comparados
   â€¢ {len(config['horizons'])} horizontes testados
   â€¢ Tempo total: {results['execution_time']:.2f}s

ğŸ† Modelo Recomendado:
   â€¢ {best_model}
   â€¢ Taxa de aprovaÃ§Ã£o: {best_rate:.1%}
   â€¢ Status: {best_decision}

ğŸ“ˆ Performance:
   â€¢ Melhoria: {improvement:.1f}%
   â€¢ Melhor modelo: {winner}

ğŸš€ Status: {'âœ… APROVADO' if best_decision == 'GO' else 'ğŸŸ¡ CONDICIONAL' if best_decision == 'CONDITIONAL' else 'âŒ REPROVADO'}
    """
    
    ax9.text(0.05, 0.95, summary_text, transform=ax9.transAxes, 
            fontsize=11, verticalalignment='top', fontfamily='monospace',
            bbox=dict(boxstyle="round,pad=0.5", facecolor="lightgray", alpha=0.8))
    
    # Ajustar layout
    plt.tight_layout()
    
    # Salvar grÃ¡fico
    output_file = 'data/processed/preds/backtest_dashboard.png'
    plt.savefig(output_file, dpi=300, bbox_inches='tight', 
                facecolor='white', edgecolor='none')
    
    print(f"ğŸ’¾ Dashboard salvo em: {output_file}")
    
    # Mostrar grÃ¡fico
    plt.show()
    
    return True

def create_performance_report():
    """
    Cria relatÃ³rio detalhado de performance
    """
    print("\nğŸ“‹ CRIANDO RELATÃ“RIO DE PERFORMANCE...")
    
    # Carregar resultados
    results_file = 'data/processed/preds/historical_backtest_results.json'
    
    try:
        with open(results_file, 'r') as f:
            results = json.load(f)
    except Exception as e:
        print(f"âŒ Erro ao carregar resultados: {e}")
        return False
    
    config = results['config']
    fold_results = results['fold_results']
    gates_summary = results['gates_summary']
    
    # Criar relatÃ³rio
    report = []
    report.append("="*80)
    report.append("ğŸ“Š RELATÃ“RIO DETALHADO DO BACKTEST HISTÃ“RICO")
    report.append("Framework 02c - ValidaÃ§Ã£o de Modelos QuantÃ­licos")
    report.append("="*80)
    report.append("")
    
    # ConfiguraÃ§Ã£o
    report.append("ğŸ¯ CONFIGURAÃ‡ÃƒO DO BACKTEST:")
    report.append(f"   â€¢ Data de execuÃ§Ã£o: {results['timestamp']}")
    report.append(f"   â€¢ Tempo de execuÃ§Ã£o: {results['execution_time']:.2f}s")
    report.append(f"   â€¢ NÃºmero de folds: {len(fold_results)}")
    report.append(f"   â€¢ Modelos testados: {', '.join(config['models'])}")
    report.append(f"   â€¢ Horizontes: {config['horizons']}")
    report.append(f"   â€¢ Quantis: {config['quantiles']}")
    report.append("")
    
    # Performance por modelo
    report.append("ğŸ“ˆ PERFORMANCE POR MODELO:")
    report.append("")
    
    for model_name in config['models']:
        report.append(f"ğŸ¤– {model_name}:")
        
        # Coletar mÃ©tricas
        all_mae = []
        all_rmse = []
        all_coverage = []
        
        for fold in fold_results:
            for horizon in config['horizons']:
                metrics = fold['models'][model_name]['metrics'][horizon]
                all_mae.append(metrics['MAE'])
                all_rmse.append(metrics['RMSE'])
                all_coverage.append(metrics['Coverage_90'])
        
        # EstatÃ­sticas
        report.append(f"   ğŸ“Š MAE: {np.mean(all_mae):.4f} Â± {np.std(all_mae):.4f}")
        report.append(f"   ğŸ“Š RMSE: {np.mean(all_rmse):.4f} Â± {np.std(all_rmse):.4f}")
        report.append(f"   ğŸ“Š Coverage: {np.mean(all_coverage):.3f} Â± {np.std(all_coverage):.3f}")
        
        # Gates
        gates_info = gates_summary[model_name]
        report.append(f"   ğŸšª Gates: {gates_info['total_passed']}/{gates_info['total_gates']} ({gates_info['approval_rate']:.1%})")
        report.append(f"   ğŸ¯ DecisÃ£o: {gates_info['final_decision']}")
        report.append("")
    
    # AnÃ¡lise por fold
    report.append("ğŸ”„ ANÃLISE POR FOLD:")
    report.append("")
    
    for fold in fold_results:
        report.append(f"Fold {fold['fold_id']}:")
        report.append(f"   ğŸ“š PerÃ­odo treino: {fold['train_period'][0]} â†’ {fold['train_period'][1]}")
        report.append(f"   ğŸ§ª PerÃ­odo teste: {fold['test_period'][0]} â†’ {fold['test_period'][1]}")
        
        for model_name in config['models']:
            gates_info = fold['gates'][model_name]
            report.append(f"   {model_name}: {gates_info['gates_passed']}/{gates_info['gates_total']} ({gates_info['approval_rate']:.1%}) â†’ {gates_info['decision']}")
        report.append("")
    
    # RecomendaÃ§Ãµes
    best_model = max(gates_summary.keys(), 
                    key=lambda x: gates_summary[x]['approval_rate'])
    best_rate = gates_summary[best_model]['approval_rate']
    best_decision = gates_summary[best_model]['final_decision']
    
    report.append("ğŸ¯ RECOMENDAÃ‡Ã•ES:")
    report.append(f"   ğŸ† Melhor modelo: {best_model}")
    report.append(f"   ğŸ“Š Taxa de aprovaÃ§Ã£o: {best_rate:.1%}")
    report.append(f"   ğŸš€ Status: {best_decision}")
    report.append("")
    
    if best_decision == 'GO':
        report.append("   âœ… APROVADO PARA PRODUÃ‡ÃƒO")
        report.append("   ğŸ“‹ PrÃ³ximos passos:")
        report.append("      â€¢ Implementar monitoramento em tempo real")
        report.append("      â€¢ Configurar alertas de performance")
        report.append("      â€¢ Executar backtest em perÃ­odo mais longo")
        report.append("      â€¢ Preparar documentaÃ§Ã£o para deploy")
    elif best_decision == 'CONDITIONAL':
        report.append("   ğŸŸ¡ APROVAÃ‡ÃƒO CONDICIONAL")
        report.append("   ğŸ“‹ AÃ§Ãµes recomendadas:")
        report.append("      â€¢ Revisar thresholds dos gates crÃ­ticos")
        report.append("      â€¢ Aumentar frequÃªncia de monitoramento")
        report.append("      â€¢ Considerar ajustes finos no modelo")
        report.append("      â€¢ Validar em dados mais recentes")
    else:
        report.append("   âŒ NECESSITA MELHORIAS")
        report.append("   ğŸ“‹ AÃ§Ãµes obrigatÃ³rias:")
        report.append("      â€¢ Retreinar modelo com dados adicionais")
        report.append("      â€¢ Revisar engenharia de features")
        report.append("      â€¢ Ajustar hiperparÃ¢metros")
        report.append("      â€¢ Validar qualidade dos dados")
    
    report.append("")
    report.append("="*80)
    
    # Salvar relatÃ³rio
    report_text = "\n".join(report)
    
    output_file = 'data/processed/preds/backtest_report.txt'
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(report_text)
        print(f"ğŸ“‹ RelatÃ³rio salvo em: {output_file}")
    except Exception as e:
        print(f"âŒ Erro ao salvar relatÃ³rio: {e}")
    
    # Imprimir resumo
    print("\n" + report_text)
    
    return True

if __name__ == "__main__":
    print("ğŸ“Š Criando anÃ¡lise visual dos resultados do backtest...")
    
    # Criar dashboard
    dashboard_success = create_backtest_dashboard()
    
    if dashboard_success:
        print("âœ… Dashboard criado com sucesso!")
        
        # Criar relatÃ³rio
        report_success = create_performance_report()
        
        if report_success:
            print("âœ… RelatÃ³rio de performance criado!")
            print("\nğŸ¯ ANÃLISE COMPLETA FINALIZADA!")
        else:
            print("âš ï¸  Erro na criaÃ§Ã£o do relatÃ³rio")
    else:
        print("âŒ Erro na criaÃ§Ã£o do dashboard")